from __future__ import annotations

import json
import os
import re
from typing import Any, Dict, List, Optional, Union

from log import log
# [FIX 2026-01-11] å¯¼å…¥ gemini_fix çš„æ¸…ç†å‡½æ•° - ä¸Šæ¸¸åŒæ­¥
from src.converters.gemini_fix import clean_contents, ALLOWED_PART_KEYS
# [FIX 2026-01-16] å¯¼å…¥ thoughtSignature_fix çš„ç­¾åå¤„ç†å‡½æ•°
from src.converters.thoughtSignature_fix import (
    decode_tool_id_and_signature,
    has_valid_thoughtsignature,
    sanitize_thinking_block,
    remove_trailing_unsigned_thinking,
    filter_invalid_thinking_blocks,
    MIN_SIGNATURE_LENGTH,
    SKIP_SIGNATURE_VALIDATOR,
)


# [FIX 2026-01-09] åŒå‘é™åˆ¶ç­–ç•¥å¸¸é‡å®šä¹‰
# æ ¸å¿ƒæ€è·¯ï¼šæ—¢è¦ä¿è¯è¶³å¤Ÿçš„è¾“å‡ºç©ºé—´ï¼Œåˆä¸èƒ½è®© max_tokens è¿‡å¤§è§¦å‘ 429
# [UPDATE 2026-01-09] ç»æµ‹è¯•ç¡®è®¤ 32000 å®Œå…¨å¤Ÿç”¨ï¼Œä¸Šè°ƒè‡³ 65535 æä¾›æ›´å¤§çš„ thinking ç©ºé—´
# [FIX 2026-01-11] æé«˜ MIN_OUTPUT_TOKENS ä»¥æ”¯æŒé•¿æ–‡æ¡£è¾“å‡ºï¼ˆMDæ–‡æ¡£å¯èƒ½éœ€è¦ 10K-30K tokensï¼‰
MAX_ALLOWED_TOKENS = 65535   # max_tokens çš„ç»å¯¹ä¸Šé™ï¼ˆClaude æœ€å¤§å€¼ï¼‰
MIN_OUTPUT_TOKENS = 16384    # å®é™…è¾“å‡ºçš„æœ€å°ä¿éšœç©ºé—´ï¼ˆ4096 -> 16384ï¼Œæ”¯æŒé•¿æ–‡æ¡£ï¼‰

DEFAULT_THINKING_BUDGET = 1024
DEFAULT_TEMPERATURE = 0.4


def _anthropic_debug_enabled() -> bool:
    return str(os.getenv("ANTHROPIC_DEBUG", "")).strip().lower() in {"1", "true", "yes", "on"}


def _is_non_whitespace_text(value: Any) -> bool:
    """
    åˆ¤æ–­æ–‡æœ¬æ˜¯å¦åŒ…å«"éç©ºç™½"å†…å®¹ã€‚

    è¯´æ˜ï¼šä¸‹æ¸¸ï¼ˆAntigravity/Claude å…¼å®¹å±‚ï¼‰ä¼šå¯¹çº¯ text å†…å®¹å—åšæ ¡éªŒï¼š
    - text ä¸èƒ½ä¸ºç©ºå­—ç¬¦ä¸²
    - text ä¸èƒ½ä»…ç”±ç©ºç™½å­—ç¬¦ï¼ˆç©ºæ ¼/æ¢è¡Œ/åˆ¶è¡¨ç­‰ï¼‰ç»„æˆ

    å› æ­¤è¿™é‡Œç»Ÿä¸€æŠŠä»…ç©ºç™½çš„ text part è¿‡æ»¤æ‰ï¼Œä»¥é¿å… 400ï¼š
    `messages: text content blocks must contain non-whitespace text`ã€‚
    """
    if value is None:
        return False
    try:
        return bool(str(value).strip())
    except Exception:
        return False


def _is_thinking_disabled(thinking_value: Any) -> bool:
    """åˆ¤æ–­ thinking æ˜¯å¦è¢«æ˜¾å¼ç¦ç”¨"""
    if thinking_value is None:
        return False
    if isinstance(thinking_value, bool):
        return not thinking_value
    if isinstance(thinking_value, dict):
        return thinking_value.get("type") == "disabled"
    return False


def _should_strip_thinking_blocks(payload: Dict[str, Any]) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦åº”è¯¥åœ¨è¯·æ±‚è½¬æ¢æ—¶æ¸…ç† thinking blocksã€‚
    æ¸…ç†æ¡ä»¶ï¼ˆæ»¡è¶³ä»»ä¸€å³æ¸…ç†ï¼‰ï¼š
    1. thinking è¢«æ˜¾å¼ç¦ç”¨ï¼ˆtype: disabled æˆ– thinking: falseï¼‰
    2. thinking=nullï¼ˆä¸ä¸‹å‘ thinkingConfigï¼Œä¸‹æ¸¸è§†ä¸ºç¦ç”¨ï¼‰
    3. æ²¡æœ‰ thinking å­—æ®µï¼ˆä¸ä¸‹å‘ thinkingConfigï¼Œä¸‹æ¸¸è§†ä¸ºç¦ç”¨ï¼‰
    4. thinking å¯ç”¨ä½†å†å²æ¶ˆæ¯ä¸æ»¡è¶³çº¦æŸï¼ˆä¸ä¸‹å‘ thinkingConfigï¼Œä¸‹æ¸¸è§†ä¸ºç¦ç”¨ï¼‰

    æ ¸å¿ƒåŸåˆ™ï¼šåªè¦ä¸ä¼šä¸‹å‘ thinkingConfigï¼Œå°±åº”è¯¥æ¸…ç† thinking blocksï¼Œ
    é¿å…ä¸‹æ¸¸æŠ¥é”™ "When thinking is disabled, an assistant message cannot contain thinking"
    """
    # æ²¡æœ‰ thinking å­—æ®µ â†’ ä¸ä¸‹å‘ thinkingConfig â†’ éœ€è¦æ¸…ç†
    if "thinking" not in payload:
        return True

    thinking_value = payload.get("thinking")

    # thinking=null â†’ ä¸ä¸‹å‘ thinkingConfig â†’ éœ€è¦æ¸…ç†
    if thinking_value is None:
        return True

    # thinking è¢«æ˜¾å¼ç¦ç”¨ â†’ éœ€è¦æ¸…ç†
    if _is_thinking_disabled(thinking_value):
        return True

    # thinking å¯ç”¨ï¼Œæ£€æŸ¥æ˜¯å¦ä¼šå®é™…ä¸‹å‘ thinkingConfig
    # å¦‚æœå†å²æ¶ˆæ¯ä¸æ»¡è¶³çº¦æŸï¼ŒthinkingConfig ä¸ä¼šè¢«ä¸‹å‘
    thinking_config = get_thinking_config(thinking_value)
    include_thoughts = bool(thinking_config.get("includeThoughts", False))

    if not include_thoughts:
        # includeThoughts=False â†’ éœ€è¦æ¸…ç†
        return True

    # æ£€æŸ¥æœ€åä¸€æ¡ assistant æ¶ˆæ¯çš„ç¬¬ä¸€ä¸ª block ç±»å‹
    messages = payload.get("messages") or []
    last_assistant_first_block_type = None
    for msg in reversed(messages):
        if not isinstance(msg, dict):
            continue
        if msg.get("role") != "assistant":
            continue
        content = msg.get("content")
        if not isinstance(content, list) or not content:
            continue
        first_block = content[0]
        if isinstance(first_block, dict):
            last_assistant_first_block_type = first_block.get("type")
        else:
            last_assistant_first_block_type = None
        break

    # å¦‚æœæœ€åä¸€æ¡ assistant æ¶ˆæ¯çš„ç¬¬ä¸€ä¸ª block ä¸æ˜¯ thinking/redacted_thinkingï¼Œ
    # åˆ™ thinkingConfig ä¸ä¼šè¢«ä¸‹å‘ â†’ éœ€è¦æ¸…ç†
    if last_assistant_first_block_type not in {None, "thinking", "redacted_thinking"}:
        return True

    # æ£€æŸ¥ budget æ˜¯å¦ä¼šå¯¼è‡´ thinkingConfig ä¸ä¸‹å‘
    max_tokens = payload.get("max_tokens")
    if isinstance(max_tokens, int):
        budget = thinking_config.get("thinkingBudget")
        if isinstance(budget, int) and budget >= max_tokens:
            adjusted_budget = max(0, max_tokens - 1)
            if adjusted_budget <= 0:
                # budget æ— æ³•è°ƒæ•´ â†’ thinkingConfig ä¸ä¸‹å‘ â†’ éœ€è¦æ¸…ç†
                return True

    # å…¶ä»–æƒ…å†µï¼šthinkingConfig ä¼šè¢«ä¸‹å‘ï¼Œä¸éœ€è¦æ¸…ç†
    return False


def _strip_thinking_blocks_from_messages(messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ä»æ¶ˆæ¯åˆ—è¡¨ä¸­ç§»é™¤æ‰€æœ‰ thinking/redacted_thinking blocksã€‚
    å½“ thinking è¢«ç¦ç”¨æ—¶ï¼Œå†å²æ¶ˆæ¯ä¸­çš„ thinking blocks ä¼šå¯¼è‡´ 400 é”™è¯¯ï¼š
    "When thinking is disabled, an `assistant` message..."

    æ­¤å‡½æ•°ä¼šï¼š
    1. éå†æ‰€æœ‰æ¶ˆæ¯
    2. å¯¹äº assistant æ¶ˆæ¯ï¼Œç§»é™¤ content ä¸­çš„ thinking/redacted_thinking blocks
    3. ä¿ç•™å…¶ä»–æ‰€æœ‰å†…å®¹ï¼ˆtext, tool_use, tool_result ç­‰ï¼‰

    æ³¨æ„ï¼šthinking blocks åªæ˜¯æ¨¡å‹çš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ï¼Œç§»é™¤å®ƒä»¬ä¸ä¼šå½±å“å¯¹è¯çš„æ ¸å¿ƒå†…å®¹ã€‚
    """
    if not messages:
        return messages

    cleaned_messages = []
    for msg in messages:
        if not isinstance(msg, dict):
            cleaned_messages.append(msg)
            continue

        role = msg.get("role")
        content = msg.get("content")

        # åªå¤„ç† assistant æ¶ˆæ¯çš„ content
        if role != "assistant" or not isinstance(content, list):
            cleaned_messages.append(msg)
            continue

        # è¿‡æ»¤æ‰ thinking å’Œ redacted_thinking blocks
        cleaned_content = []
        for item in content:
            if isinstance(item, dict):
                item_type = item.get("type")
                if item_type in ("thinking", "redacted_thinking"):
                    # è·³è¿‡ thinking blocks
                    continue
            cleaned_content.append(item)

        # å¦‚æœæ¸…ç†å content ä¸ºç©ºï¼Œæ·»åŠ ä¸€ä¸ªç©ºæ–‡æœ¬å—é¿å…æ ¼å¼é”™è¯¯
        if not cleaned_content:
            cleaned_content = [{"type": "text", "text": "..."}]

        # åˆ›å»ºæ–°çš„æ¶ˆæ¯å¯¹è±¡
        cleaned_msg = msg.copy()
        cleaned_msg["content"] = cleaned_content
        cleaned_messages.append(cleaned_msg)

    return cleaned_messages


def get_thinking_config(thinking: Optional[Union[bool, Dict[str, Any]]]) -> Dict[str, Any]:
    """
    æ ¹æ® Anthropic/Claude è¯·æ±‚çš„ thinking å‚æ•°ç”Ÿæˆä¸‹æ¸¸ thinkingConfigã€‚

    è¯¥é€»è¾‘ä»¥æ ¹ç›®å½• `converter.py` çš„è¯­ä¹‰ä¸ºå‡†ï¼š
    - thinking=Noneï¼šé»˜è®¤å¯ç”¨ includeThoughtsï¼Œå¹¶ä½¿ç”¨é»˜è®¤ budget
    - thinking=boolï¼šTrue å¯ç”¨ / False ç¦ç”¨
    - thinking=dictï¼š{'type':'enabled'|'disabled', 'budget_tokens': int}
    """
    if thinking is None:
        return {"includeThoughts": True, "thinkingBudget": DEFAULT_THINKING_BUDGET}

    if isinstance(thinking, bool):
        if thinking:
            return {"includeThoughts": True, "thinkingBudget": DEFAULT_THINKING_BUDGET}
        return {"includeThoughts": False}

    if isinstance(thinking, dict):
        thinking_type = thinking.get("type", "enabled")
        is_enabled = thinking_type == "enabled"
        if not is_enabled:
            return {"includeThoughts": False}

        budget = thinking.get("budget_tokens", DEFAULT_THINKING_BUDGET)
        return {"includeThoughts": True, "thinkingBudget": budget}

    return {"includeThoughts": True, "thinkingBudget": DEFAULT_THINKING_BUDGET}


def map_claude_model_to_gemini(claude_model: str) -> str:
    """
    å°† Claude æ¨¡å‹åæ˜ å°„ä¸ºä¸‹æ¸¸æ¨¡å‹åï¼ˆå«â€œæ”¯æŒåˆ—è¡¨é€ä¼ â€ä¸å›ºå®šæ˜ å°„ï¼‰ã€‚

    è¯¥é€»è¾‘ä»¥æ ¹ç›®å½• `converter.py` ä¸ºå‡†ã€‚
    """
    claude_model = str(claude_model or "").strip()
    if not claude_model:
        return "claude-sonnet-4-5"

    # claude-cli å¸¸è§çš„ç‰ˆæœ¬åŒ–æ¨¡å‹åï¼Œä¾‹å¦‚ï¼š
    # - claude-opus-4-5-20251101
    # - claude-haiku-4-5-20251001
    # è¿™ç±»åç§°ä¸åœ¨ converter.py çš„å›ºå®šæ˜ å°„ä¸­ï¼Œä¼šè½å…¥é»˜è®¤å€¼ï¼Œä»è€Œå¯¼è‡´â€œçœ‹èµ·æ¥åƒè¢«å¼ºåˆ¶ç”¨ sonnetâ€ã€‚
    # è¿™é‡Œåšä¸€æ¬¡è§„èŒƒåŒ–ï¼Œä½¿å…¶æ›´è´´è¿‘ç”¨æˆ·é¢„æœŸã€‚
    m = re.match(r"^(claude-(?:opus|sonnet|haiku)-4-5)-\d{8}$", claude_model)
    if m:
        claude_model = m.group(1)

    # å¯¹ claude 4.5 ç³»åˆ—åšæ›´åˆç†çš„è½åœ°æ˜ å°„ï¼ˆä¿æŒä¸‹æ¸¸å¯ç”¨æ€§ä¼˜å…ˆï¼‰
    if claude_model == "claude-opus-4-5":
        return "claude-opus-4-5-thinking"
    if claude_model == "claude-sonnet-4-5":
        return "claude-sonnet-4-5"
    if claude_model == "claude-haiku-4-5":
        # ä½¿ç”¨ fallback_manager ä¸­å®šä¹‰çš„ Haiku é™çº§ç›®æ ‡ï¼Œä¿æŒä¸€è‡´æ€§
        from src.fallback_manager import HAIKU_FALLBACK_TARGET
        return HAIKU_FALLBACK_TARGET  # gemini-3-flash

    supported_models = {
        # Gemini ç³»åˆ—
        "gemini-2.5-flash",
        "gemini-2.5-flash-thinking",
        "gemini-2.5-flash-lite",
        "gemini-2.5-flash-image",
        "gemini-2.5-pro",
        "gemini-3-flash",
        "gemini-3-pro-low",
        "gemini-3-pro-high",
        "gemini-3-pro-image",
        # Claude ç³»åˆ—
        "claude-sonnet-4-5",
        "claude-sonnet-4-5-thinking",
        "claude-opus-4-5-thinking",
        # GPT ç³»åˆ—
        "gpt-oss-120b-medium",
        # å†…éƒ¨æµ‹è¯•æ¨¡å‹ï¼ˆé¢„ç•™ï¼‰
        "rev19-uic3-1p",
        "chat_20706",
        "chat_23310",
    }

    if claude_model in supported_models:
        return claude_model

    # Haiku æ¨¡å‹ç»Ÿä¸€ä½¿ç”¨ HAIKU_FALLBACK_TARGET
    from src.fallback_manager import HAIKU_FALLBACK_TARGET, is_haiku_model
    if is_haiku_model(claude_model):
        return HAIKU_FALLBACK_TARGET  # gemini-3-flash

    model_mapping = {
        "claude-sonnet-4.5": "claude-sonnet-4-5",
        "claude-3-5-sonnet-20241022": "claude-sonnet-4-5",
        "claude-3-5-sonnet-20240620": "claude-sonnet-4-5",
        "claude-opus-4": "gemini-3-pro-high",
    }

    return model_mapping.get(claude_model, "claude-sonnet-4-5")


def clean_json_schema(schema: Any) -> Any:
    """
    æ¸…ç† JSON Schemaï¼Œç§»é™¤ä¸‹æ¸¸ä¸æ”¯æŒçš„å­—æ®µï¼Œå¹¶æŠŠéªŒè¯è¦æ±‚è¿½åŠ åˆ° descriptionã€‚

    è¯¥é€»è¾‘ä»¥æ ¹ç›®å½• `converter.py` çš„è¯­ä¹‰ä¸ºå‡†ã€‚
    """
    if not isinstance(schema, dict):
        return schema

    # ä¸‹æ¸¸ï¼ˆAntigravity/Vertex/Geminiï¼‰å¯¹ tool parameters çš„ JSON Schema æ”¯æŒèŒƒå›´å¾ˆçª„ï¼Œ
    # ä¸€äº›æ ‡å‡†å­—æ®µä¼šç›´æ¥è§¦å‘ 400ï¼ˆä¾‹å¦‚ $ref / exclusiveMinimumï¼‰ã€‚
    #
    # è¿™é‡Œå‚è€ƒ `src/openai_transfer.py::_clean_schema_for_gemini` çš„åå•ï¼Œåšä¸€æ¬¡ç»Ÿä¸€å‰”é™¤ï¼Œ
    # ä»¥ä¿è¯ Anthropic tools -> ä¸‹æ¸¸ functionDeclarations çš„å…¼å®¹æ€§ã€‚
    unsupported_keys = {
        "$schema",
        "$id",
        "$ref",
        "$defs",
        "definitions",
        "title",
        "example",
        "examples",
        "readOnly",
        "writeOnly",
        "default",
        "exclusiveMaximum",
        "exclusiveMinimum",
        "oneOf",
        "anyOf",
        "allOf",
        "const",
        "additionalItems",
        "contains",
        "patternProperties",
        "dependencies",
        "propertyNames",
        "if",
        "then",
        "else",
        "contentEncoding",
        "contentMediaType",
    }

    validation_fields = {
        "minLength": "minLength",
        "maxLength": "maxLength",
        "minimum": "minimum",
        "maximum": "maximum",
        "minItems": "minItems",
        "maxItems": "maxItems",
    }
    fields_to_remove = {"additionalProperties"}

    validations: List[str] = []
    for field, label in validation_fields.items():
        if field in schema:
            validations.append(f"{label}: {schema[field]}")

    cleaned: Dict[str, Any] = {}
    for key, value in schema.items():
        if key in unsupported_keys or key in fields_to_remove or key in validation_fields:
            continue

        if key == "type" and isinstance(value, list):
            # Roo/Anthropic SDK å¸¸è§å†™æ³•ï¼štype: ["string", "null"]
            # ä¸‹æ¸¸ï¼ˆProto é£æ ¼ Schemaï¼‰é€šå¸¸è¦æ±‚ type ä¸ºå•å€¼å­—æ®µï¼Œå¹¶ä½¿ç”¨ nullable è¡¨è¾¾å¯ç©ºã€‚
            has_null = any(
                isinstance(t, str) and t.strip() and t.strip().lower() == "null" for t in value
            )
            non_null_types = [
                t.strip()
                for t in value
                if isinstance(t, str) and t.strip() and t.strip().lower() != "null"
            ]

            cleaned[key] = non_null_types[0] if non_null_types else "string"
            if has_null:
                cleaned["nullable"] = True
            continue

        if key == "description" and validations:
            cleaned[key] = f"{value} ({', '.join(validations)})"
        elif key == "properties":
            # ç‰¹æ®Šå¤„ç† propertiesï¼šç¡®ä¿æ¯ä¸ªå±æ€§çš„å€¼éƒ½æ˜¯å®Œæ•´çš„ Schema å¯¹è±¡
            if isinstance(value, dict):
                cleaned_properties: Dict[str, Any] = {}
                for prop_name, prop_schema in value.items():
                    if isinstance(prop_schema, dict):
                        # é€’å½’æ¸…ç†å±æ€§ Schema
                        cleaned_prop = clean_json_schema(prop_schema)
                        # å¦‚æœå±æ€§ç±»å‹æ˜¯ objectï¼Œç¡®ä¿å®ƒæ˜¯ä¸€ä¸ªå®Œæ•´çš„ Schema å¯¹è±¡
                        if cleaned_prop.get("type") == "object":
                            # ç¡®ä¿ object ç±»å‹æœ‰å®Œæ•´çš„ Schema ç»“æ„
                            if "properties" not in cleaned_prop:
                                cleaned_prop["properties"] = {}
                            # ç¡®ä¿æœ‰ type å­—æ®µ
                            if "type" not in cleaned_prop:
                                cleaned_prop["type"] = "object"
                        cleaned_properties[prop_name] = cleaned_prop
                    elif isinstance(prop_schema, str) and prop_schema == "object":
                        # å¦‚æœå€¼æ˜¯å­—ç¬¦ä¸² "object"ï¼Œè½¬æ¢ä¸ºå®Œæ•´çš„ Schema å¯¹è±¡
                        cleaned_properties[prop_name] = {"type": "object", "properties": {}}
                    else:
                        # å…¶ä»–æƒ…å†µç›´æ¥ä½¿ç”¨åŸå€¼ï¼ˆä½†åº”è¯¥ä¸ä¼šå‘ç”Ÿï¼‰
                        cleaned_properties[prop_name] = prop_schema
                cleaned[key] = cleaned_properties
            else:
                cleaned[key] = value
        elif isinstance(value, dict):
            cleaned[key] = clean_json_schema(value)
        elif isinstance(value, list):
            cleaned[key] = [clean_json_schema(item) if isinstance(item, dict) else item for item in value]
        else:
            cleaned[key] = value

    if validations and "description" not in cleaned:
        cleaned["description"] = f"Validation: {', '.join(validations)}"

    # ä¸ `src/openai_transfer.py::_clean_schema_for_gemini` ä¿æŒä¸€è‡´ï¼š
    # å¦‚æœæœ‰ properties ä½†æ²¡æœ‰æ˜¾å¼ typeï¼Œåˆ™è¡¥é½ä¸º objectï¼Œé¿å…ä¸‹æ¸¸æ ¡éªŒå¤±è´¥ã€‚
    if "properties" in cleaned and "type" not in cleaned:
        cleaned["type"] = "object"

    # ä¿® Cursor å…¼å®¹æ€§ï¼šç¡®ä¿ input_schema å§‹ç»ˆæœ‰ type å­—æ®µ
    # é”™è¯¯ "tools.0.custom.input_schema.type: Field required" è¡¨æ˜ä¸‹æ¸¸è¦æ±‚ type å¿…å¡«
    # å¦‚æœ cleaned éç©ºä½†æ²¡æœ‰ typeï¼Œé»˜è®¤è¡¥é½ä¸º "object"
    if cleaned and "type" not in cleaned:
        cleaned["type"] = "object"

    return cleaned


# âœ… [FIX 2026-01-17] ç»Ÿä¸€ä½¿ç”¨æ–°ç‰ˆæœ¬çš„ç­¾åæ¢å¤å‡½æ•°
# æ—§ç‰ˆæœ¬å·²åºŸå¼ƒï¼Œæ”¹ç”¨ converters.signature_recovery ä¸­çš„æ–°ç‰ˆæœ¬
# æ–°ç‰ˆæœ¬è¿”å› RecoveryResultï¼Œæä¾›æ›´è¯¦ç»†çš„æ¢å¤ä¿¡æ¯
def recover_signature_for_tool_use(
    tool_id: str,
    encoded_tool_id: str,
    signature: Optional[str],
    last_thought_signature: Optional[str],
    session_id: Optional[str] = None
) -> Optional[str]:
    """
    å¤šå±‚ç­¾åæ¢å¤ç­–ç•¥ï¼ˆç”¨äºå·¥å…·è°ƒç”¨ï¼‰- 6å±‚å®Œæ•´å®ç°
    
    âš ï¸ æ³¨æ„ï¼šæ­¤å‡½æ•°æ˜¯æ—§ç‰ˆæœ¬çš„å…¼å®¹åŒ…è£…ï¼Œå†…éƒ¨è°ƒç”¨æ–°ç‰ˆæœ¬çš„ recover_signature_for_tool_use
    æ–°ç‰ˆæœ¬ä½äº converters.signature_recoveryï¼Œè¿”å› RecoveryResult æä¾›æ›´è¯¦ç»†ä¿¡æ¯
    
    ä¼˜å…ˆçº§ï¼š
    1. å®¢æˆ·ç«¯æä¾›çš„ç­¾å
    2. ä¸Šä¸‹æ–‡ä¸­çš„ç­¾å
    3. ä»ç¼–ç çš„å·¥å…·IDä¸­è§£ç ï¼ˆè‡ªç ”ç‰ˆç‰¹æœ‰ï¼‰
    4. ä¼šè¯çº§ç¼“å­˜
    5. å·¥å…·IDç¼“å­˜
    6. æœ€è¿‘ç­¾åï¼ˆfallbackï¼‰
    """
    from src.converters.signature_recovery import recover_signature_for_tool_use as recover_tool_sig
    
    # è°ƒç”¨æ–°ç‰ˆæœ¬çš„æ¢å¤å‡½æ•°
    result = recover_tool_sig(
        tool_id=tool_id,
        encoded_tool_id=encoded_tool_id,
        client_signature=signature,
        context_signature=last_thought_signature,
        session_id=session_id,
        use_placeholder_fallback=True  # å…è®¸ä½¿ç”¨å ä½ç¬¦ä½œä¸º fallback
    )
    
    # è¿”å›ç­¾åï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬æ¥å£ï¼‰
    return result.signature


def convert_tools(anthropic_tools: Optional[List[Dict[str, Any]]]) -> Optional[List[Dict[str, Any]]]:
    """
    å°† Anthropic tools[] è½¬æ¢ä¸ºä¸‹æ¸¸ toolsï¼ˆfunctionDeclarationsï¼‰ç»“æ„ã€‚
    """
    if not anthropic_tools:
        return None

    gemini_tools: List[Dict[str, Any]] = []
    for tool in anthropic_tools:
        name = tool.get("name")
        if not name:
            continue
        description = tool.get("description", "")
        input_schema = tool.get("input_schema", {}) or {}
        parameters = clean_json_schema(input_schema)

        gemini_tools.append(
            {
                "functionDeclarations": [
                    {
                        "name": name,
                        "description": description,
                        "parameters": parameters,
                    }
                ]
            }
        )

    return gemini_tools or None


def _extract_tool_result_output(content: Any) -> str:
    """
    ä» tool_result.content ä¸­æå–è¾“å‡ºå­—ç¬¦ä¸²ï¼ˆæŒ‰ converter.py çš„æœ€å°è¯­ä¹‰ï¼‰ã€‚
    """
    if isinstance(content, list):
        if not content:
            return ""
        first = content[0]
        if isinstance(first, dict) and first.get("type") == "text":
            return str(first.get("text", ""))
        return str(first)
    if content is None:
        return ""
    return str(content)


def convert_messages_to_contents(messages: List[Dict[str, Any]], *, include_thinking: bool = True) -> List[Dict[str, Any]]:
    """
    å°† Anthropic messages[] è½¬æ¢ä¸ºä¸‹æ¸¸ contents[]ï¼ˆrole: user/model, parts: []ï¼‰ã€‚

    Args:
        messages: Anthropic æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
        include_thinking: æ˜¯å¦åŒ…å« thinking å—ï¼ˆå½“è¯·æ±‚æœªå¯ç”¨ thinking æ—¶åº”è®¾ä¸º Falseï¼‰
    """
    contents: List[Dict[str, Any]] = []

    # [FIX 2026-01-17] ç”Ÿæˆä¼šè¯æŒ‡çº¹ç”¨äº Session Cache
    session_id: Optional[str] = None
    try:
        from src.signature_cache import generate_session_fingerprint
        session_id = generate_session_fingerprint(messages)
        if session_id:
            log.debug(f"[ANTHROPIC CONVERTER] Generated session_id: {session_id[:16]}...")
    except ImportError:
        log.debug("[ANTHROPIC CONVERTER] Session fingerprint generation not available")
    except Exception as e:
        log.warning(f"[ANTHROPIC CONVERTER] Failed to generate session_id: {e}")

    # ç¬¬ä¸€éï¼šå»ºç«‹ tool_use_id -> name çš„æ˜ å°„
    # Anthropic çš„ tool_result æ¶ˆæ¯ä¸åŒ…å« name å­—æ®µï¼Œä½† Gemini çš„ functionResponse éœ€è¦ name
    # [FIX 2026-01-16] ä½¿ç”¨è§£ç åçš„åŸå§‹IDä½œä¸ºkeyï¼Œç¡®ä¿ä¸tool_resultåŒ¹é…
    tool_use_id_to_name: Dict[str, str] = {}
    for msg in messages:
        raw_content = msg.get("content", "")
        if isinstance(raw_content, list):
            for item in raw_content:
                if isinstance(item, dict) and item.get("type") == "tool_use":
                    encoded_tool_id = item.get("id") or ""
                    tool_name = item.get("name")
                    # [FIX 2026-01-16] è§£ç è·å–åŸå§‹ID
                    original_tool_id, _ = decode_tool_id_and_signature(encoded_tool_id)
                    if original_tool_id and tool_name:
                        tool_use_id_to_name[str(original_tool_id)] = str(tool_name)

    # [FIX 2026-01-17] è·Ÿè¸ªæœ€è¿‘çš„ thinking ç­¾åï¼Œç”¨äºå·¥å…·è°ƒç”¨æ¢å¤
    # å…³é”®ä¿®å¤ï¼šClaude çš„ thinking å—å’Œ tool_use å—åœ¨åŒä¸€ä¸ª assistant æ¶ˆæ¯ä¸­
    # thinking å—çš„ç­¾ååº”è¯¥ä¼ é€’ç»™åŒä¸€æ¶ˆæ¯ä¸­çš„ tool_use å—
    last_thinking_signature: Optional[str] = None

    for msg in messages:
        role = msg.get("role", "user")
        gemini_role = "model" if role == "assistant" else "user"
        raw_content = msg.get("content", "")

        # [FIX 2026-01-17] æ¯ä¸ªæ¶ˆæ¯å¼€å§‹æ—¶é‡ç½®å½“å‰æ¶ˆæ¯çš„ thinking ç­¾å
        # è¿™æ ·å¯ä»¥ç¡®ä¿ tool_use å—ä½¿ç”¨çš„æ˜¯åŒä¸€æ¶ˆæ¯ä¸­çš„ thinking ç­¾å
        current_msg_thinking_signature: Optional[str] = None

        parts: List[Dict[str, Any]] = []
        if isinstance(raw_content, str):
            if _is_non_whitespace_text(raw_content):
                parts = [{"text": str(raw_content)}]
        elif isinstance(raw_content, list):
            for item in raw_content:
                if not isinstance(item, dict):
                    if _is_non_whitespace_text(item):
                        parts.append({"text": str(item)})
                    continue

                item_type = item.get("type")
                if item_type == "thinking":
                    # å¦‚æœè¯·æ±‚æœªå¯ç”¨ thinkingï¼Œåˆ™è·³è¿‡å†å² thinking å—
                    if not include_thinking:
                        continue

                    thinking_text = item.get("thinking", "")
                    if thinking_text is None:
                        thinking_text = ""
                    message_signature = item.get("signature", "")

                    # [FIX 2026-01-16] å¤šå±‚ç­¾åæ¢å¤ç­–ç•¥
                    # ä¼˜å…ˆçº§: ç¼“å­˜ç­¾å -> æ¶ˆæ¯ç­¾å(å¦‚æœæœ‰æ•ˆ) -> æœ€è¿‘ç­¾å(fallback) -> è·³è¿‡
                    from src.signature_cache import get_cached_signature, get_last_signature

                    if thinking_text:
                        final_signature = None
                        recovery_source = None

                        # ä¼˜å…ˆçº§ 1: ä»ç¼“å­˜æ¢å¤
                        cached_signature = get_cached_signature(thinking_text)
                        if cached_signature:
                            final_signature = cached_signature
                            recovery_source = "cache"

                        # ä¼˜å…ˆçº§ 2: å¦‚æœç¼“å­˜æœªå‘½ä¸­ï¼Œæ£€æŸ¥æ¶ˆæ¯æä¾›çš„ç­¾åæ˜¯å¦æœ‰æ•ˆ
                        if not final_signature and message_signature and len(message_signature) >= MIN_SIGNATURE_LENGTH:
                            # æ¶ˆæ¯ç­¾åçœ‹èµ·æ¥æœ‰æ•ˆï¼Œä½†æˆ‘ä»¬ä¸èƒ½å®Œå…¨ä¿¡ä»»å®ƒ
                            # åªæœ‰åœ¨ç¼“å­˜å®Œå…¨æœªå‘½ä¸­æ—¶æ‰è€ƒè™‘ä½¿ç”¨
                            final_signature = message_signature
                            recovery_source = "message"
                            log.info(f"[ANTHROPIC CONVERTER] ç¼“å­˜æœªå‘½ä¸­ï¼Œä½¿ç”¨æ¶ˆæ¯ç­¾åï¼ˆå¯èƒ½æœ‰é£é™©ï¼‰: thinking_len={len(thinking_text)}")

                        # ä¼˜å…ˆçº§ 3: ä½¿ç”¨æœ€è¿‘ç¼“å­˜çš„ç­¾åå’Œé…å¯¹æ–‡æœ¬ï¼ˆfallbackï¼‰
                        # [FIX 2026-01-17] å…³é”®ä¿®å¤ï¼šå¿…é¡»åŒæ—¶è·å– signature å’Œé…å¯¹çš„ thinking_text
                        # å¦åˆ™ä¼šå¯¼è‡´ signature ä¸ thinking_text ä¸åŒ¹é…ï¼Œè§¦å‘ 400 é”™è¯¯
                        if not final_signature:
                            from src.signature_cache import get_last_signature_with_text
                            last_result = get_last_signature_with_text()
                            if last_result:
                                final_signature, cached_thinking_text = last_result
                                original_thinking_len = len(thinking_text) if thinking_text else 0
                                # å…³é”®ï¼šä½¿ç”¨ç¼“å­˜çš„ thinking_text æ›¿æ¢å½“å‰çš„ï¼Œç¡®ä¿ä¸ signature åŒ¹é…
                                thinking_text = cached_thinking_text
                                recovery_source = "last_cached_with_text"
                                log.info(f"[ANTHROPIC CONVERTER] ä½¿ç”¨æœ€è¿‘ç¼“å­˜çš„ç­¾åå’Œé…å¯¹æ–‡æœ¬ï¼ˆfallbackï¼‰: "
                                        f"original_len={original_thinking_len}, cached_len={len(thinking_text)}")

                        # å¦‚æœæˆåŠŸæ¢å¤ç­¾åï¼Œæ·»åŠ  thinking block
                        if final_signature:
                            part: Dict[str, Any] = {
                                "text": str(thinking_text),
                                "thought": True,
                                "thoughtSignature": final_signature,
                            }
                            parts.append(part)

                            # [FIX 2026-01-17] ç¼“å­˜æ€ç»´å—ç­¾å (P0 Critical Fix)
                            # åªç¼“å­˜æ¥è‡ªæ¶ˆæ¯çš„ç­¾åï¼Œä¸ç¼“å­˜fallbackç­¾åï¼ˆé¿å…æ±¡æŸ“ï¼‰
                            if recovery_source == "message":
                                from src.signature_cache import cache_signature, cache_session_signature
                                try:
                                    cache_signature(thinking_text, final_signature)
                                    # [FIX 2026-01-17] åŒæ—¶ç¼“å­˜åˆ° Session Cache
                                    if session_id:
                                        cache_session_signature(session_id, final_signature, thinking_text)
                                    log.debug(f"[ANTHROPIC CONVERTER] Cached thinking signature from message: thinking_len={len(thinking_text)}")
                                except Exception as e:
                                    log.warning(f"[SIGNATURE_CACHE] æ€ç»´å—ç­¾åç¼“å­˜å¤±è´¥: {e}")

                            # [FIX 2026-01-17] ä¿å­˜å½“å‰æ¶ˆæ¯çš„ thinking ç­¾åï¼Œç”¨äºåŒä¸€æ¶ˆæ¯ä¸­çš„ tool_use å—
                            # è¿™æ˜¯å…³é”®ä¿®å¤ï¼šClaude çš„ thinking å—å’Œ tool_use å—åœ¨åŒä¸€ä¸ª assistant æ¶ˆæ¯ä¸­
                            current_msg_thinking_signature = final_signature
                            last_thinking_signature = final_signature
                            log.debug(f"[ANTHROPIC CONVERTER] Saved thinking signature for tool_use recovery: sig_len={len(final_signature)}")

                            if recovery_source == "cache":
                                if message_signature and message_signature != cached_signature:
                                    log.info(f"[ANTHROPIC CONVERTER] ä½¿ç”¨ç¼“å­˜ signature æ›¿ä»£æ¶ˆæ¯ signature: thinking_len={len(thinking_text)}")
                                else:
                                    log.debug(f"[ANTHROPIC CONVERTER] ä½¿ç”¨ç¼“å­˜ signature: thinking_len={len(thinking_text)}")
                        else:
                            # [FIX 2026-01-17] æ‰€æœ‰æ¢å¤ç­–ç•¥éƒ½å¤±è´¥ï¼Œé™çº§ä¸º text å—è€Œä¸æ˜¯è·³è¿‡
                            # å‚è€ƒ Antigravity-Manager v3.3.35 çš„å®ç°
                            if thinking_text and thinking_text.strip():
                                parts.append({"text": f"[Previous thinking: {thinking_text}]"})
                                log.info(f"[ANTHROPIC CONVERTER] Thinking block ç­¾åæ¢å¤å¤±è´¥ï¼Œé™çº§ä¸º text: "
                                         f"thinking_len={len(thinking_text)}")
                            else:
                                log.warning(f"[ANTHROPIC CONVERTER] Thinking block æ‰€æœ‰æ¢å¤ç­–ç•¥å¤±è´¥ä¸”å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡æ­¤ block")
                elif item_type == "redacted_thinking":
                    # å¦‚æœè¯·æ±‚æœªå¯ç”¨ thinkingï¼Œåˆ™è·³è¿‡å†å² redacted_thinking å—
                    if not include_thinking:
                        continue

                    # redacted_thinking çš„å…·ä½“å­—æ®µåœ¨ä¸åŒå®¢æˆ·ç«¯å¯èƒ½ä¸åŒï¼Œè¿™é‡Œå°½é‡å…¼å®¹ data/thinkingã€‚
                    thinking_text = item.get("thinking")
                    if thinking_text is None:
                        thinking_text = item.get("data", "")
                    message_signature = item.get("signature", "")

                    # [FIX 2026-01-16] å¤šå±‚ç­¾åæ¢å¤ç­–ç•¥ï¼ˆä¸ thinking å—ç›¸åŒï¼‰
                    from src.signature_cache import get_cached_signature, get_last_signature

                    if thinking_text:
                        final_signature = None
                        recovery_source = None

                        # ä¼˜å…ˆçº§ 1: ä»ç¼“å­˜æ¢å¤
                        cached_signature = get_cached_signature(thinking_text)
                        if cached_signature:
                            final_signature = cached_signature
                            recovery_source = "cache"

                        # ä¼˜å…ˆçº§ 2: å¦‚æœç¼“å­˜æœªå‘½ä¸­ï¼Œæ£€æŸ¥æ¶ˆæ¯æä¾›çš„ç­¾åæ˜¯å¦æœ‰æ•ˆ
                        if not final_signature and message_signature and len(message_signature) >= MIN_SIGNATURE_LENGTH:
                            final_signature = message_signature
                            recovery_source = "message"
                            log.info(f"[ANTHROPIC CONVERTER] Redacted ç¼“å­˜æœªå‘½ä¸­ï¼Œä½¿ç”¨æ¶ˆæ¯ç­¾åï¼ˆå¯èƒ½æœ‰é£é™©ï¼‰: thinking_len={len(thinking_text)}")

                        # ä¼˜å…ˆçº§ 3: ä½¿ç”¨æœ€è¿‘ç¼“å­˜çš„ç­¾åå’Œé…å¯¹æ–‡æœ¬ï¼ˆfallbackï¼‰
                        # [FIX 2026-01-17] å…³é”®ä¿®å¤ï¼šå¿…é¡»åŒæ—¶è·å– signature å’Œé…å¯¹çš„ thinking_text
                        # å¦åˆ™ä¼šå¯¼è‡´ signature ä¸ thinking_text ä¸åŒ¹é…ï¼Œè§¦å‘ 400 é”™è¯¯
                        if not final_signature:
                            from src.signature_cache import get_last_signature_with_text
                            last_result = get_last_signature_with_text()
                            if last_result:
                                final_signature, cached_thinking_text = last_result
                                original_thinking_len = len(thinking_text) if thinking_text else 0
                                # å…³é”®ï¼šä½¿ç”¨ç¼“å­˜çš„ thinking_text æ›¿æ¢å½“å‰çš„ï¼Œç¡®ä¿ä¸ signature åŒ¹é…
                                thinking_text = cached_thinking_text
                                recovery_source = "last_cached_with_text"
                                log.info(f"[ANTHROPIC CONVERTER] Redacted ä½¿ç”¨æœ€è¿‘ç¼“å­˜çš„ç­¾åå’Œé…å¯¹æ–‡æœ¬ï¼ˆfallbackï¼‰: "
                                        f"original_len={original_thinking_len}, cached_len={len(thinking_text)}")

                        # å¦‚æœæˆåŠŸæ¢å¤ç­¾åï¼Œæ·»åŠ  redacted_thinking block
                        if final_signature:
                            parts.append(
                                {
                                    "text": str(thinking_text or ""),
                                    "thought": True,
                                    "thoughtSignature": final_signature,
                                }
                            )

                            # [FIX 2026-01-17] ç¼“å­˜ redacted_thinking å—ç­¾å (P0 Critical Fix)
                            # åªç¼“å­˜æ¥è‡ªæ¶ˆæ¯çš„ç­¾åï¼Œä¸ç¼“å­˜fallbackç­¾åï¼ˆé¿å…æ±¡æŸ“ï¼‰
                            if recovery_source == "message":
                                from src.signature_cache import cache_signature
                                try:
                                    cache_signature(thinking_text, final_signature)
                                    log.debug(f"[ANTHROPIC CONVERTER] Cached redacted_thinking signature from message: thinking_len={len(thinking_text)}")
                                except Exception as e:
                                    log.warning(f"[SIGNATURE_CACHE] Redactedæ€ç»´å—ç­¾åç¼“å­˜å¤±è´¥: {e}")

                            if recovery_source == "cache":
                                if message_signature and message_signature != cached_signature:
                                    log.info(f"[ANTHROPIC CONVERTER] ä½¿ç”¨ç¼“å­˜ signature æ›¿ä»£æ¶ˆæ¯ redacted signature: thinking_len={len(thinking_text)}")
                                else:
                                    log.debug(f"[ANTHROPIC CONVERTER] ä½¿ç”¨ç¼“å­˜ redacted signature: thinking_len={len(thinking_text)}")
                        else:
                            # [FIX 2026-01-17] æ‰€æœ‰æ¢å¤ç­–ç•¥éƒ½å¤±è´¥ï¼Œé™çº§ä¸º text å—è€Œä¸æ˜¯è·³è¿‡
                            # å‚è€ƒ Antigravity-Manager v3.3.35 çš„å®ç°
                            if thinking_text and thinking_text.strip():
                                parts.append({"text": f"[Previous thinking: {thinking_text}]"})
                                log.info(f"[ANTHROPIC CONVERTER] Redacted thinking block ç­¾åæ¢å¤å¤±è´¥ï¼Œé™çº§ä¸º text: "
                                         f"thinking_len={len(thinking_text)}")
                            else:
                                log.warning(f"[ANTHROPIC CONVERTER] Redacted thinking block æ‰€æœ‰æ¢å¤ç­–ç•¥å¤±è´¥ä¸”å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡æ­¤ block")
                elif item_type == "text":
                    text = item.get("text", "")
                    if _is_non_whitespace_text(text):
                        parts.append({"text": str(text)})
                elif item_type == "image":
                    source = item.get("source", {}) or {}
                    if source.get("type") == "base64":
                        parts.append(
                            {
                                "inlineData": {
                                    "mimeType": source.get("media_type", "image/png"),
                                    "data": source.get("data", ""),
                                }
                            }
                        )
                elif item_type == "tool_use":
                    # [FIX 2026-01-16] ä»ç¼–ç çš„å·¥å…·IDä¸­è§£ç ç­¾å
                    # è¿™ä½¿å¾—ç­¾åèƒ½å¤Ÿåœ¨å®¢æˆ·ç«¯å¾€è¿”ä¼ è¾“ä¸­ä¿ç•™ï¼Œå³ä½¿å®¢æˆ·ç«¯ä¼šåˆ é™¤è‡ªå®šä¹‰å­—æ®µ
                    encoded_id = item.get("id") or ""
                    original_id, thoughtsignature = decode_tool_id_and_signature(encoded_id)

                    # [FIX 2026-01-17] ç¼“å­˜å·¥å…·IDç­¾å (Layer 1)
                    # è¿™æ˜¯å…³é”®ä¿®å¤ï¼šç¡®ä¿ç­¾åè¢«ç¼“å­˜ï¼Œä»¥ä¾¿åç»­è¯·æ±‚å¯ä»¥æ¢å¤
                    if thoughtsignature and len(thoughtsignature) >= MIN_SIGNATURE_LENGTH:
                        from src.signature_cache import cache_tool_signature
                        try:
                            cache_tool_signature(original_id, thoughtsignature)
                            log.debug(f"[ANTHROPIC CONVERTER] Cached tool signature for id: {original_id}")
                        except Exception as e:
                            log.warning(f"[SIGNATURE_CACHE] å·¥å…·IDç­¾åç¼“å­˜å¤±è´¥: {e}")

                    # [FIX 2026-01-17] å¢å¼ºç­¾åæ¢å¤ç­–ç•¥ - 6å±‚å®Œæ•´å®ç°
                    # å³ä½¿å®¢æˆ·ç«¯åˆ é™¤äº† thoughtSignatureï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡ç¼“å­˜æ¢å¤
                    # [FIX 2026-01-17] å…³é”®ä¿®å¤ï¼šä½¿ç”¨åŒä¸€æ¶ˆæ¯ä¸­çš„ thinking ç­¾åä½œä¸ºä¸Šä¸‹æ–‡ç­¾å
                    # ä¼˜å…ˆä½¿ç”¨å½“å‰æ¶ˆæ¯çš„ thinking ç­¾åï¼Œå…¶æ¬¡ä½¿ç”¨æœ€è¿‘çš„ thinking ç­¾å
                    context_signature = current_msg_thinking_signature or last_thinking_signature
                    final_sig = recover_signature_for_tool_use(
                        tool_id=original_id,
                        encoded_tool_id=encoded_id,
                        signature=thoughtsignature,
                        last_thought_signature=context_signature,  # [FIX 2026-01-17] ä¼ å…¥ä¸Šä¸‹æ–‡ç­¾å
                        session_id=session_id  # [FIX 2026-01-17] ä¼ å…¥ session_id å¯ç”¨ Layer 4
                    )

                    fc_part: Dict[str, Any] = {
                        "functionCall": {
                            "id": original_id,  # [FIX 2026-01-16] ä½¿ç”¨è§£ç åçš„åŸå§‹ID
                            "name": item.get("name"),
                            "args": item.get("input", {}) or {},
                        },
                    }

                    if final_sig:
                        fc_part["thoughtSignature"] = final_sig
                        log.debug(f"[ANTHROPIC CONVERTER] Recovered thoughtSignature for tool_id: {original_id}")
                    else:
                        # âš ï¸ æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œä½¿ç”¨å ä½ç¬¦
                        log.warning(f"[ANTHROPIC CONVERTER] No signature found for tool call: {original_id}, using placeholder")
                        fc_part["thoughtSignature"] = SKIP_SIGNATURE_VALIDATOR

                    parts.append(fc_part)
                elif item_type == "tool_result":
                    encoded_tool_use_id = item.get("tool_use_id") or ""

                    # [FIX 2026-01-16] è§£ç è·å–åŸå§‹IDï¼ˆfunctionResponseä¸éœ€è¦ç­¾åï¼‰
                    original_tool_use_id, _ = decode_tool_id_and_signature(encoded_tool_use_id)

                    # [FIX 2026-01-08] éªŒè¯å¯¹åº”çš„ tool_use æ˜¯å¦å­˜åœ¨
                    # å¦‚æœ tool_use ä¸å­˜åœ¨ï¼Œè·³è¿‡è¿™ä¸ª tool_resultï¼Œé¿å… Anthropic API è¿”å› 400 é”™è¯¯ï¼š
                    # "unexpected `tool_use_id` found in `tool_result` blocks"
                    # æ³¨æ„ï¼štool_use_id_to_name æ˜ å°„ä¸­çš„ key å·²ç»æ˜¯åŸå§‹IDï¼ˆå› ä¸ºæˆ‘ä»¬åœ¨ç¬¬ä¸€éæ‰«ææ—¶ä¹Ÿè§£ç äº†ï¼‰
                    if not original_tool_use_id or str(original_tool_use_id) not in tool_use_id_to_name:
                        log.warning(f"[ANTHROPIC CONVERTER] Skipping orphan tool_result: "
                                   f"tool_use_id={original_tool_use_id} (encoded={encoded_tool_use_id}) not found in tool_use_id_to_name mapping. "
                                   f"This may happen when tool_use was filtered out (e.g., thinking disabled) "
                                   f"but tool_result was retained.")
                        continue

                    output = _extract_tool_result_output(item.get("content"))
                    # ä»æ˜ å°„ä¸­è·å– nameï¼ˆæ­¤æ—¶ä¸€å®šå­˜åœ¨ï¼Œå› ä¸ºä¸Šé¢å·²ç»éªŒè¯è¿‡ï¼‰
                    tool_name = tool_use_id_to_name[str(original_tool_use_id)]
                    parts.append(
                        {
                            "functionResponse": {
                                "id": original_tool_use_id,  # [FIX 2026-01-16] ä½¿ç”¨è§£ç åçš„åŸå§‹ID
                                "name": tool_name,
                                "response": {"output": output},
                            }
                        }
                    )
                else:
                    parts.append({"text": json.dumps(item, ensure_ascii=False)})
        else:
            if _is_non_whitespace_text(raw_content):
                parts = [{"text": str(raw_content)}]

        # é¿å…äº§ç”Ÿç©º partsï¼ˆä¸‹æ¸¸å¯èƒ½ä¼šæŠ¥é”™ï¼‰ï¼Œç›´æ¥è·³è¿‡è¯¥æ¡ç©ºæ¶ˆæ¯ã€‚
        if not parts:
            continue

        contents.append({"role": gemini_role, "parts": parts})

    # [FIX 2026-01-11] åº”ç”¨ ALLOWED_PART_KEYS ç™½åå•è¿‡æ»¤å’Œå°¾éšç©ºæ ¼æ¸…ç†
    # è¿™æ˜¯ä¸Šæ¸¸åŒæ­¥çš„å…³é”®ä¿®å¤ï¼Œé˜²æ­¢ cache_control ç­‰ä¸æ”¯æŒå­—æ®µå¯¼è‡´ 400/429 é”™è¯¯
    contents = clean_contents(contents)
    
    return contents


def reorganize_tool_messages(contents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    é‡æ–°ç»„ç»‡æ¶ˆæ¯ï¼Œå°½é‡æ»¡è¶³ Anthropic çš„ tool_use/tool_result çº¦æŸï¼š
    - æ¯ä¸ª tool_useï¼ˆä¸‹æ¸¸è¡¨ç°ä¸º functionCallï¼‰å¿…é¡»ç´§è·Ÿä¸€ä¸ªå¯¹åº”çš„ tool_resultï¼ˆä¸‹æ¸¸è¡¨ç°ä¸º functionResponseï¼‰

    è¯¥é€»è¾‘â€œå¯¹é½/ç§»æ¤â€æ ¹ç›®å½• `converter.py` çš„ `reorganize_tool_messages` è¯­ä¹‰ï¼š
    - å°†æ‰€æœ‰ functionResponse æ”¶é›†èµ·æ¥
    - å°†æ‰€æœ‰ parts å¹³é“ºä¸ºâ€œæ¯ä¸ª part ç‹¬ç«‹æˆä¸€æ¡æ¶ˆæ¯â€
    - é‡åˆ° functionCall æ—¶ï¼Œè‹¥å­˜åœ¨åŒ¹é…çš„ functionResponseï¼Œåˆ™æ’å…¥åˆ°å…¶å

    æ³¨æ„ï¼šå¦‚æœå®¢æˆ·ç«¯æ ¹æœ¬æ²¡æœ‰æä¾› tool_resultï¼Œæœ¬å‡½æ•°æ— æ³•å‡­ç©ºè¡¥é½ï¼Œåªèƒ½å°½åŠ›é‡æ’ã€‚
    """
    tool_results: Dict[str, Dict[str, Any]] = {}

    for msg in contents:
        for part in msg.get("parts", []) or []:
            if isinstance(part, dict) and "functionResponse" in part:
                tool_id = (part.get("functionResponse") or {}).get("id")
                if tool_id:
                    tool_results[str(tool_id)] = part

    flattened: List[Dict[str, Any]] = []
    for msg in contents:
        role = msg.get("role")
        for part in msg.get("parts", []) or []:
            flattened.append({"role": role, "parts": [part]})

    new_contents: List[Dict[str, Any]] = []
    i = 0
    while i < len(flattened):
        msg = flattened[i]
        part = msg["parts"][0]

        if isinstance(part, dict) and "functionResponse" in part:
            i += 1
            continue

        if isinstance(part, dict) and "functionCall" in part:
            tool_id = (part.get("functionCall") or {}).get("id")
            new_contents.append({"role": "model", "parts": [part]})

            if tool_id is not None and str(tool_id) in tool_results:
                new_contents.append({"role": "user", "parts": [tool_results[str(tool_id)]]})

            i += 1
            continue

        new_contents.append(msg)
        i += 1

    return new_contents


def build_system_instruction(system: Any) -> Optional[Dict[str, Any]]:
    """
    å°† Anthropic system å­—æ®µè½¬æ¢ä¸ºä¸‹æ¸¸ systemInstructionã€‚
    """
    if not system:
        return None

    parts: List[Dict[str, Any]] = []
    if isinstance(system, str):
        if _is_non_whitespace_text(system):
            parts.append({"text": str(system)})
    elif isinstance(system, list):
        for item in system:
            if isinstance(item, dict) and item.get("type") == "text":
                text = item.get("text", "")
                if _is_non_whitespace_text(text):
                    parts.append({"text": str(text)})
    else:
        if _is_non_whitespace_text(system):
            parts.append({"text": str(system)})

    if not parts:
        return None

    return {"role": "user", "parts": parts}


def build_generation_config(payload: Dict[str, Any]) -> tuple[Dict[str, Any], bool]:
    """
    æ ¹æ® Anthropic Messages è¯·æ±‚æ„é€ ä¸‹æ¸¸ generationConfigã€‚

    é»˜è®¤å€¼ä¸ `converter.py` ä¿æŒä¸€è‡´ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå…¼å®¹ stop_sequencesã€‚

    Returns:
        (generation_config, should_include_thinking): å…ƒç»„ï¼ŒåŒ…å«ç”Ÿæˆé…ç½®å’Œæ˜¯å¦åº”åŒ…å« thinking å—
    """
    config: Dict[str, Any] = {
        "topP": 1,
        "topK": 40,
        "candidateCount": 1,
        "stopSequences": [
            "<|user|>",
            "<|bot|>",
            "<|context_request|>",
            "<|endoftext|>",
            "<|end_of_turn|>",
        ],
    }

    temperature = payload.get("temperature", None)
    config["temperature"] = DEFAULT_TEMPERATURE if temperature is None else temperature

    top_p = payload.get("top_p", None)
    if top_p is not None:
        config["topP"] = top_p

    top_k = payload.get("top_k", None)
    if top_k is not None:
        config["topK"] = top_k

    max_tokens = payload.get("max_tokens")
    if max_tokens is not None:
        # ğŸ› ä¿®å¤ï¼šæ·»åŠ ä¸Šé™ä¿æŠ¤ï¼Œé˜²æ­¢è¿‡å¤§çš„ max_tokens å¯¼è‡´ Antigravity API è¿”å› 429
        # å‚è€ƒ gemini_router.py å’Œ openai_router.py çš„ä¸Šé™è®¾ç½®
        MAX_OUTPUT_TOKENS_LIMIT = 65535
        if isinstance(max_tokens, int) and max_tokens > MAX_OUTPUT_TOKENS_LIMIT:
            log.warning(
                f"[ANTHROPIC CONVERTER] maxOutputTokens è¶…è¿‡ä¸Šé™: {max_tokens} -> {MAX_OUTPUT_TOKENS_LIMIT}"
            )
            max_tokens = MAX_OUTPUT_TOKENS_LIMIT

        # [FIX 2026-01-11] æ·»åŠ ä¸‹é™ä¿æŠ¤ï¼Œé˜²æ­¢å®¢æˆ·ç«¯ï¼ˆå¦‚Cursorï¼‰ä¼ æ¥è¿‡å°çš„ max_tokens å¯¼è‡´è¾“å‡ºè¢«æˆªæ–­
        # å†™ MD æ–‡æ¡£å¯èƒ½éœ€è¦ 10K-30K tokensï¼Œ4096 è¿œè¿œä¸å¤Ÿ
        MIN_OUTPUT_TOKENS_FLOOR = 16384  # æœ€å°è¾“å‡ºç©ºé—´ä¿éšœ
        if isinstance(max_tokens, int) and max_tokens < MIN_OUTPUT_TOKENS_FLOOR:
            log.info(
                f"[ANTHROPIC CONVERTER] maxOutputTokens ä½äºä¸‹é™: {max_tokens} -> {MIN_OUTPUT_TOKENS_FLOOR}"
            )
            max_tokens = MIN_OUTPUT_TOKENS_FLOOR

        config["maxOutputTokens"] = max_tokens
    else:
        # [FIX 2026-01-12] å®¢æˆ·ç«¯æœªä¼  max_tokens æ—¶ï¼Œä½¿ç”¨é»˜è®¤å€¼ä¿è¯è¶³å¤Ÿè¾“å‡ºç©ºé—´
        # è¿™æ˜¯ä¹‹å‰ä¿®å¤å¤±æ•ˆçš„æ ¹å› ï¼šif å—è¢«è·³è¿‡ï¼Œconfig ä¸­æ²¡æœ‰ maxOutputTokens
        DEFAULT_MAX_OUTPUT_TOKENS = 16384
        log.info(
            f"[ANTHROPIC CONVERTER] max_tokens æœªæŒ‡å®šï¼Œä½¿ç”¨é»˜è®¤å€¼: {DEFAULT_MAX_OUTPUT_TOKENS}"
        )
        config["maxOutputTokens"] = DEFAULT_MAX_OUTPUT_TOKENS

    stop_sequences = payload.get("stop_sequences")
    if isinstance(stop_sequences, list) and stop_sequences:
        config["stopSequences"] = config["stopSequences"] + [str(s) for s in stop_sequences]

    # Anthropic çš„ extended thinking å¹¶éé»˜è®¤å¼€å¯ï¼›å¹¶ä¸”éƒ¨åˆ†å®¢æˆ·ç«¯ï¼ˆclaude-cli / CherryStudioï¼‰
    # å¯èƒ½ä¼šæºå¸¦ `thinking: null`ï¼Œæˆ–å¼€å¯ thinking ä½†ä¸å›æ”¾å†å² thinking blocksã€‚
    #
    # ä¸‹æ¸¸åœ¨ thinking å¯ç”¨æ—¶ä¼šæ›´ä¸¥æ ¼æ ¡éªŒå†å² assistant æ¶ˆæ¯ï¼š
    # - è‹¥å†å²ä¸­å­˜åœ¨ assistant æ¶ˆæ¯ï¼Œåˆ™"æœ€åä¸€æ¡ assistant æ¶ˆæ¯"å¿…é¡»ä»¥ thinking/redacted_thinking block å¼€å¤´
    # - max_tokens å¿…é¡»å¤§äº thinking.budget_tokens
    #
    # ä¸ºå…¼å®¹å®¢æˆ·ç«¯ï¼Œè¿™é‡Œä»…åœ¨ thinking å€¼"æ˜¾å¼ä¸”é null"æ—¶æ‰è€ƒè™‘ä¸‹å‘ï¼Œå¹¶åšå®‰å…¨å…œåº•ï¼š
    # - è‹¥æœ€åä¸€æ¡ assistant æ¶ˆæ¯ä¸ä»¥ thinking/redacted_thinking å¼€å¤´ï¼Œåˆ™ä¸ä¸‹å‘ thinkingConfigï¼ˆé¿å… 400ï¼‰
    # - è‹¥ budget >= max_tokensï¼Œåˆ™è‡ªåŠ¨ä¸‹è°ƒ budgetï¼ˆæœ€ä½é™åˆ° max_tokens-1ï¼‰ï¼Œå¦åˆ™ä¸ä¸‹å‘
    should_include_thinking = False
    if "thinking" in payload:
        thinking_value = payload.get("thinking")
        if thinking_value is not None:
            thinking_config = get_thinking_config(thinking_value)
            include_thoughts = bool(thinking_config.get("includeThoughts", False))

            last_assistant_first_block_type = None
            for msg in reversed(payload.get("messages") or []):
                if not isinstance(msg, dict):
                    continue
                if msg.get("role") != "assistant":
                    continue
                content = msg.get("content")
                if not isinstance(content, list) or not content:
                    continue
                first_block = content[0]
                if isinstance(first_block, dict):
                    last_assistant_first_block_type = first_block.get("type")
                else:
                    last_assistant_first_block_type = None
                break

            if include_thoughts and last_assistant_first_block_type not in {
                None,
                "thinking",
                "redacted_thinking",
            }:
                if _anthropic_debug_enabled():
                    log.info(
                        "[ANTHROPIC][thinking] è¯·æ±‚æ˜¾å¼å¯ç”¨ thinkingï¼Œä½†å†å² messages æœªå›æ”¾ "
                        "æ»¡è¶³çº¦æŸçš„ assistant thinking/redacted_thinking èµ·å§‹å—ï¼Œå·²è·³è¿‡ä¸‹å‘ thinkingConfigï¼ˆé¿å…ä¸‹æ¸¸ 400ï¼‰"
                    )
                return config, False

            max_tokens = payload.get("max_tokens")
            if include_thoughts and isinstance(max_tokens, int):
                budget = thinking_config.get("thinkingBudget")
                if isinstance(budget, int) and budget > 0:
                    # [FIX 2026-01-09] åŒå‘é™åˆ¶ç­–ç•¥
                    # æ ¸å¿ƒæ€è·¯ï¼šæ—¢è¦ä¿è¯è¶³å¤Ÿçš„è¾“å‡ºç©ºé—´ï¼Œåˆä¸èƒ½è®© max_tokens è¿‡å¤§è§¦å‘ 429
                    #
                    # ç­–ç•¥ï¼š
                    # 1. å¦‚æœ thinkingBudget + MIN_OUTPUT_TOKENS > MAX_ALLOWED_TOKENSï¼Œä¸‹è°ƒ thinkingBudget
                    # 2. maxOutputTokens ä¸èƒ½è¶…è¿‡ MAX_ALLOWED_TOKENS
                    # 3. ä¿è¯è‡³å°‘ MIN_OUTPUT_TOKENS çš„è¾“å‡ºç©ºé—´
                    
                    original_budget = budget
                    original_max_tokens = max_tokens
                    
                    # Step 1: è®¡ç®—éœ€è¦çš„æ€» tokens
                    required_tokens = budget + MIN_OUTPUT_TOKENS
                    
                    if required_tokens > MAX_ALLOWED_TOKENS:
                        # éœ€è¦ä¸‹è°ƒ thinkingBudget
                        adjusted_budget = MAX_ALLOWED_TOKENS - MIN_OUTPUT_TOKENS
                        if adjusted_budget <= 0:
                            # æ— æ³•ä¿è¯è¾“å‡ºç©ºé—´ï¼Œè·³è¿‡ thinking
                            if _anthropic_debug_enabled():
                                log.info(
                                    f"[ANTHROPIC][thinking] åŒå‘é™åˆ¶ï¼šæ— æ³•ä¿è¯ MIN_OUTPUT_TOKENS={MIN_OUTPUT_TOKENS}ï¼Œ"
                                    f"è·³è¿‡ä¸‹å‘ thinkingConfigï¼ˆbudget={budget}, MAX_ALLOWED={MAX_ALLOWED_TOKENS}ï¼‰"
                                )
                            return config, False
                        
                        thinking_config["thinkingBudget"] = adjusted_budget
                        budget = adjusted_budget
                        log.info(
                            f"[ANTHROPIC][thinking] åŒå‘é™åˆ¶ç”Ÿæ•ˆï¼šthinkingBudget ä¸‹è°ƒ {original_budget} -> {adjusted_budget} "
                            f"(MAX_ALLOWED={MAX_ALLOWED_TOKENS}, MIN_OUTPUT={MIN_OUTPUT_TOKENS})"
                        )
                    
                    # Step 2: ç¡®ä¿ max_tokens = budget + MIN_OUTPUT_TOKENSï¼Œä½†ä¸è¶…è¿‡ MAX_ALLOWED_TOKENS
                    new_max_tokens = min(budget + MIN_OUTPUT_TOKENS, MAX_ALLOWED_TOKENS)
                    
                    # Step 3: å¦‚æœå®¢æˆ·ç«¯çš„ max_tokens å°äºè®¡ç®—å€¼ï¼Œéœ€è¦è°ƒæ•´
                    if max_tokens < new_max_tokens:
                        config["maxOutputTokens"] = new_max_tokens
                        log.info(
                            f"[ANTHROPIC][thinking] åŒå‘é™åˆ¶ç”Ÿæ•ˆï¼šmaxOutputTokens æå‡ {original_max_tokens} -> {new_max_tokens} "
                            f"(thinkingBudget={budget}, å®é™…è¾“å‡ºç©ºé—´={new_max_tokens - budget})"
                        )
                    elif max_tokens > MAX_ALLOWED_TOKENS:
                        # å®¢æˆ·ç«¯ max_tokens è¿‡å¤§ï¼Œéœ€è¦é™åˆ¶
                        config["maxOutputTokens"] = MAX_ALLOWED_TOKENS
                        log.info(
                            f"[ANTHROPIC][thinking] åŒå‘é™åˆ¶ç”Ÿæ•ˆï¼šmaxOutputTokens ä¸‹è°ƒ {original_max_tokens} -> {MAX_ALLOWED_TOKENS} "
                            f"(é˜²æ­¢ 429 é”™è¯¯)"
                        )

            # [FIX 2026-01-17] ç§»é™¤ thinkingLevel é¿å…ä¸ thinkingBudget å†²çªï¼ˆå®˜æ–¹ç‰ˆæœ¬ä¿®å¤ï¼‰
            # å‚è€ƒ: gcli2api_official PR #291 (fix/thinking-budget-level-conflict)
            thinking_config.pop("thinkingLevel", None)
            
            config["thinkingConfig"] = thinking_config
            should_include_thinking = include_thoughts
            if _anthropic_debug_enabled():
                log.info(
                    f"[ANTHROPIC][thinking] å·²ä¸‹å‘ thinkingConfig: includeThoughts="
                    f"{thinking_config.get('includeThoughts')}, thinkingBudget="
                    f"{thinking_config.get('thinkingBudget')}"
                )
        else:
            if _anthropic_debug_enabled():
                log.info("[ANTHROPIC][thinking] thinking=nullï¼Œè§†ä¸ºæœªå¯ç”¨ thinkingï¼ˆä¸ä¸‹å‘ thinkingConfigï¼‰")
    else:
        if _anthropic_debug_enabled():
            log.info("[ANTHROPIC][thinking] æœªæä¾› thinking å­—æ®µï¼ˆä¸ä¸‹å‘ thinkingConfigï¼‰")
    return config, should_include_thinking


def convert_tool_choice_to_tool_config(tool_choice: Any) -> Optional[Dict[str, Any]]:
    """
    å°† Anthropic tool_choice è½¬æ¢ä¸º Gemini toolConfig

    Args:
        tool_choice: Anthropic æ ¼å¼çš„ tool_choice
        - {"type": "auto"}: æ¨¡å‹è‡ªåŠ¨å†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·
        - {"type": "any"}: æ¨¡å‹å¿…é¡»ä½¿ç”¨å·¥å…·
        - {"type": "tool", "name": "tool_name"}: æ¨¡å‹å¿…é¡»ä½¿ç”¨æŒ‡å®šå·¥å…·

    Returns:
        Gemini æ ¼å¼çš„ toolConfigï¼Œå¦‚æœæ— æ•ˆåˆ™è¿”å› None
    """
    if not tool_choice:
        return None

    if isinstance(tool_choice, dict):
        choice_type = tool_choice.get("type")

        if choice_type == "auto":
            return {"functionCallingConfig": {"mode": "AUTO"}}
        elif choice_type == "any":
            return {"functionCallingConfig": {"mode": "ANY"}}
        elif choice_type == "tool":
            tool_name = tool_choice.get("name")
            if tool_name:
                return {
                    "functionCallingConfig": {
                        "mode": "ANY",
                        "allowedFunctionNames": [tool_name],
                    }
                }
        # æ— æ•ˆæˆ–ä¸æ”¯æŒçš„ tool_choiceï¼Œè¿”å› None
        return None


def convert_anthropic_request_to_antigravity_components(payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    å°† Anthropic Messages è¯·æ±‚è½¬æ¢ä¸ºæ„é€ ä¸‹æ¸¸è¯·æ±‚æ‰€éœ€çš„ç»„ä»¶ã€‚

    è¿”å›å­—æ®µï¼š
    - model: ä¸‹æ¸¸æ¨¡å‹å
    - contents: ä¸‹æ¸¸ contents[]
    - system_instruction: ä¸‹æ¸¸ systemInstructionï¼ˆå¯é€‰ï¼‰
    - tools: ä¸‹æ¸¸ toolsï¼ˆå¯é€‰ï¼‰
    - system_instruction: ä¸‹æ¸¸ systemInstructionï¼ˆå¯é€‰ï¼‰
    - tools: ä¸‹æ¸¸ toolsï¼ˆå¯é€‰ï¼‰
    - tool_config: ä¸‹æ¸¸ toolConfigï¼ˆå¯é€‰ï¼‰
    - generation_config: ä¸‹æ¸¸ generationConfig
    """
    model = map_claude_model_to_gemini(str(payload.get("model", "")))
    messages = payload.get("messages") or []
    if not isinstance(messages, list):
        messages = []

    # ğŸ”§ ä¼˜åŒ–ï¼šæå‰æ¸…ç† thinking blocksï¼Œé¿å…ä¸‹æ¸¸æŠ¥é”™åæ‰å¤„ç†
    # æ ¸å¿ƒåŸåˆ™ï¼šåªè¦ä¸ä¼šä¸‹å‘ thinkingConfigï¼Œå°±åº”è¯¥æ¸…ç† thinking blocks
    # è¿™æ ·å¯ä»¥é¿å…æµªè´¹ tokenï¼ˆä¹‹å‰æ˜¯ä¸‹æ¸¸æŠ¥é”™åæ‰æ¸…ç†å¹¶é‡è¯•ï¼‰
    if _should_strip_thinking_blocks(payload):
        original_count = len(messages)
        messages = _strip_thinking_blocks_from_messages(messages)
        if _anthropic_debug_enabled():
            log.info(
                f"[ANTHROPIC][thinking] æ£€æµ‹åˆ° thinkingConfig ä¸ä¼šä¸‹å‘ï¼Œå·²æå‰æ¸…ç†å†å²æ¶ˆæ¯ä¸­çš„ thinking blocks "
                f"(messages={original_count})"
            )

    # [FIX 2026-01-16] è¿‡æ»¤æ— æ•ˆçš„ thinking å—ï¼Œæ¸…ç†é¢å¤–å­—æ®µï¼ˆå¦‚ cache_controlï¼‰
    # è¿™æ˜¯ P1 ä¿®å¤çš„å…³é”®ï¼šç¡®ä¿æ— æ•ˆç­¾åçš„ thinking å—ä¸ä¼šå¯¼è‡´ API 400 é”™è¯¯
    filter_invalid_thinking_blocks(messages)

    # [FIX 2026-01-17] Tool Loop Recovery - æ£€æµ‹å¹¶ä¿®å¤æ–­è£‚çš„å·¥å…·å¾ªç¯
    # å½“ Cursor IDE è¿‡æ»¤æ‰ thinking å—æ—¶ï¼Œå·¥å…·å¾ªç¯å¯èƒ½æ–­è£‚
    try:
        from src.converters.tool_loop_recovery import close_tool_loop_for_thinking
        if close_tool_loop_for_thinking(messages):
            log.info("[ANTHROPIC CONVERTER] Tool loop recovered - injected thinking block (no synthetic messages)")
    except ImportError:
        log.debug("[ANTHROPIC CONVERTER] Tool loop recovery module not available")
    except Exception as e:
        log.warning(f"[ANTHROPIC CONVERTER] Tool loop recovery failed: {e}")

    # å…ˆæ„å»º generation_config ä»¥ç¡®å®šæ˜¯å¦åº”åŒ…å« thinking
    generation_config, should_include_thinking = build_generation_config(payload)

    # æ ¹æ® thinking é…ç½®è½¬æ¢æ¶ˆæ¯
    contents = convert_messages_to_contents(messages, include_thinking=should_include_thinking)
    contents = reorganize_tool_messages(contents)
    system_instruction = build_system_instruction(payload.get("system"))
    tools = convert_tools(payload.get("tools"))
    tool_choice = payload.get("tool_choice")
    tool_config = convert_tool_choice_to_tool_config(tool_choice)

    return {
        "model": model,
        "contents": contents,
        "system_instruction": system_instruction,
        "tools": tools,
        "tool_config": tool_config,
        "generation_config": generation_config,
    }
