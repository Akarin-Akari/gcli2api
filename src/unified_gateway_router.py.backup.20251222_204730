"""
Unified Gateway Router - 统一API网关路由
将多个后端服务整合到单一端点，支持优先级路由和故障转移

优先级顺序：
1. Antigravity API (gcli2api 本地) - 优先
2. Copilot API (localhost:4141) - 备用
"""

import asyncio
import json
import re
import time
from typing import Any, AsyncGenerator, Dict, List, Optional, Tuple

import httpx
from fastapi import APIRouter, Depends, HTTPException, Request
from fastapi.responses import JSONResponse, StreamingResponse

from log import log
from src.httpx_client import http_client
from src.utils import authenticate_bearer


# ==================== 重试配置 ====================

RETRY_CONFIG = {
    "max_retries": 3,           # 最大重试次数
    "base_delay": 1.0,          # 基础延迟（秒）
    "max_delay": 10.0,          # 最大延迟（秒）
    "exponential_base": 2,      # 指数退避基数
    "retry_on_status": [500, 502, 503, 504],  # 需要重试的状态码
}


# ==================== Prompt Model Routing ====================

# Supported model names for routing
ROUTABLE_MODELS = {
    # GPT models -> Copilot
    "gpt-4", "gpt-4o", "gpt-4o-mini", "gpt-4-turbo",
    "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano",
    "gpt-5", "gpt-5.1", "gpt-5.2",
    "o1", "o1-mini", "o1-pro", "o3", "o3-mini",
    # Claude models -> Antigravity
    "claude-3-opus", "claude-3-sonnet", "claude-3-haiku",
    "claude-3.5-opus", "claude-3.5-sonnet", "claude-3.5-haiku",
    "claude-sonnet-4", "claude-opus-4", "claude-haiku-4",
    "claude-sonnet-4.5", "claude-opus-4.5", "claude-haiku-4.5",
    # Gemini models -> Antigravity
    "gemini-pro", "gemini-ultra",
    "gemini-2.5-pro", "gemini-2.5-flash",
    "gemini-3-pro", "gemini-3-pro-preview",
}

# Regex patterns for model markers
# Pattern 1: [use:model-name] - High priority
USE_PATTERN = re.compile(r'\[use:([a-zA-Z0-9._-]+)\]', re.IGNORECASE)
# Pattern 2: @model-name - Low priority (at start of message or after whitespace)
AT_PATTERN = re.compile(r'(?:^|\s)@([a-zA-Z0-9._-]+)(?=\s|$)', re.IGNORECASE)


def extract_model_from_prompt(messages: list) -> tuple:
    """
    Extract model name from prompt markers in messages.

    Priority:
    1. [use:model-name] - Highest priority
    2. @model-name - Lower priority

    Args:
        messages: List of message dicts with 'role' and 'content'

    Returns:
        Tuple of (extracted_model_name or None, cleaned_messages)
    """
    if not messages:
        return None, messages

    extracted_model = None
    cleaned_messages = []

    for msg in messages:
        if not isinstance(msg, dict):
            cleaned_messages.append(msg)
            continue

        content = msg.get("content", "")

        # Handle different content types
        if isinstance(content, list):
            # Multi-modal content (text + images)
            new_content = []
            for item in content:
                if isinstance(item, dict) and item.get("type") == "text":
                    text = item.get("text", "")
                    model, cleaned_text = _extract_and_clean(text, extracted_model)
                    if model:
                        extracted_model = model
                    new_content.append({**item, "text": cleaned_text})
                else:
                    new_content.append(item)
            cleaned_messages.append({**msg, "content": new_content})
        elif isinstance(content, str):
            model, cleaned_content = _extract_and_clean(content, extracted_model)
            if model:
                extracted_model = model
            cleaned_messages.append({**msg, "content": cleaned_content})
        else:
            cleaned_messages.append(msg)

    if extracted_model:
        log.info(f"[Gateway] Extracted model from prompt: {extracted_model}")

    return extracted_model, cleaned_messages


def _extract_and_clean(text: str, current_model: str = None) -> tuple:
    """
    Extract model marker from text and return cleaned text.

    Args:
        text: The text to search
        current_model: Currently extracted model (for priority)

    Returns:
        Tuple of (model_name or None, cleaned_text)
    """
    extracted_model = current_model
    cleaned_text = text

    # Priority 1: [use:model-name]
    use_match = USE_PATTERN.search(text)
    if use_match:
        model_name = use_match.group(1).lower()
        if model_name in ROUTABLE_MODELS or _fuzzy_match_model(model_name):
            extracted_model = model_name
            # Remove the marker from text
            cleaned_text = USE_PATTERN.sub('', cleaned_text).strip()

    # Priority 2: @model-name (only if no [use:] found)
    if not use_match:
        at_match = AT_PATTERN.search(text)
        if at_match:
            model_name = at_match.group(1).lower()
            if model_name in ROUTABLE_MODELS or _fuzzy_match_model(model_name):
                extracted_model = model_name
                # Remove the marker from text
                cleaned_text = AT_PATTERN.sub(' ', cleaned_text).strip()

    return extracted_model, cleaned_text


def _fuzzy_match_model(model_name: str) -> bool:
    """
    Fuzzy match model name against known patterns.
    Allows variations like 'gpt4o' -> 'gpt-4o', 'claude35' -> 'claude-3.5'
    """
    # Normalize: remove dashes and dots for comparison
    normalized = model_name.replace('-', '').replace('.', '').replace('_', '')

    for known_model in ROUTABLE_MODELS:
        known_normalized = known_model.replace('-', '').replace('.', '').replace('_', '')
        if normalized == known_normalized:
            return True

    # Check prefixes for model families
    model_prefixes = ['gpt', 'claude', 'gemini', 'o1', 'o3']
    for prefix in model_prefixes:
        if normalized.startswith(prefix):
            return True

    return False



def normalize_tools(tools: List[Any]) -> List[Dict[str, Any]]:
    """
    Normalize tools to standard OpenAI format.

    Standard format:
    {
        "type": "function",
        "function": {
            "name": "function_name",
            "description": "...",
            "parameters": {...}
        }
    }
    """
    normalized_tools = []

    for tool in tools:
        if not isinstance(tool, dict):
            continue

        normalized_tool = {"type": "function"}

        # Case 1: Standard format - tool has 'function' key
        if "function" in tool and isinstance(tool["function"], dict):
            func = tool["function"]
            # Ensure function has required 'name' field
            if "name" in func:
                normalized_tool["function"] = {
                    "name": func.get("name", ""),
                    "description": func.get("description", ""),
                    "parameters": func.get("parameters", {})
                }
                normalized_tools.append(normalized_tool)
            else:
                log.warning(f"[Gateway] Skipping tool without function.name: {list(func.keys())}")

        # Case 2: Flat format - tool itself has 'name' key (some clients)
        elif "name" in tool:
            normalized_tool["function"] = {
                "name": tool.get("name", ""),
                "description": tool.get("description", ""),
                "parameters": tool.get("parameters", {})
            }
            normalized_tools.append(normalized_tool)

        # Case 3: Unknown format - log and skip
        else:
            log.warning(f"[Gateway] Unknown tool format, keys: {list(tool.keys())}")

    return normalized_tools


def normalize_messages(messages: List[Any]) -> List[Dict[str, Any]]:
    """
    Normalize and filter messages array.
    - Remove null/None values
    - Remove invalid message objects
    - Ensure each message has required fields
    """
    normalized_messages = []

    for msg in messages:
        # Skip null/None values
        if msg is None:
            continue

        # Skip non-dict values
        if not isinstance(msg, dict):
            log.warning(f"[Gateway] Skipping non-dict message: {type(msg)}")
            continue

        # Ensure message has 'role' field
        if "role" not in msg:
            log.warning(f"[Gateway] Skipping message without role: {list(msg.keys())}")
            continue

        # Ensure message has 'content' field (can be None for tool calls)
        # But the message object itself must exist
        normalized_messages.append(msg)

    return normalized_messages


def normalize_request_body(body: Dict[str, Any]) -> Dict[str, Any]:
    """
    Normalize request body to standard OpenAI format.
    Handles Cursor's non-standard format and other variations.

    Cursor may send requests with:
    - messages in different locations
    - null values in messages array
    - extra fields like 'reasoning', 'text', 'metadata', etc.
    - non-standard tools format
    - missing required fields
    """
    normalized = {}

    # Extract model (required)
    normalized["model"] = body.get("model", "gpt-4")

    # Extract messages - try multiple possible locations
    messages = None

    # Standard location
    if "messages" in body and body["messages"]:
        messages = body["messages"]
    # Some clients put messages in 'prompt' or 'input'
    elif "prompt" in body:
        prompt = body["prompt"]
        if isinstance(prompt, str):
            messages = [{"role": "user", "content": prompt}]
        elif isinstance(prompt, list):
            messages = prompt
    elif "input" in body:
        input_val = body["input"]
        if isinstance(input_val, str):
            messages = [{"role": "user", "content": input_val}]
        elif isinstance(input_val, list):
            messages = input_val

    # If still no messages, create a default one
    if not messages:
        log.warning("[Gateway] No messages found in request body, creating default")
        log.warning(f"[Gateway] Request body keys: {list(body.keys())}")
        # Log first 500 chars of body for debugging
        body_str = json.dumps(body, ensure_ascii=False)
        log.warning(f"[Gateway] Request body preview: {body_str[:500]}...")
        messages = [{"role": "user", "content": "Hello"}]

    # Normalize and filter messages (remove null values, etc.)
    normalized["messages"] = normalize_messages(messages)

    # Copy standard OpenAI fields (except tools which needs special handling)
    standard_fields = [
        "stream", "temperature", "top_p", "max_tokens", "stop",
        "n", "frequency_penalty", "presence_penalty", "logit_bias",
        "logprobs", "response_format", "seed", "tool_choice",
        "user", "functions", "function_call"
    ]

    for field in standard_fields:
        if field in body:
            normalized[field] = body[field]

    # Handle tools field specially - normalize format
    if "tools" in body and body["tools"]:
        normalized_tools = normalize_tools(body["tools"])
        if normalized_tools:
            normalized["tools"] = normalized_tools

    # Set default stream to False if not specified
    if "stream" not in normalized:
        normalized["stream"] = False

    # Extract model from prompt markers (if any)
    prompt_model, cleaned_messages = extract_model_from_prompt(normalized["messages"])
    if prompt_model:
        normalized["model"] = prompt_model
        normalized["messages"] = cleaned_messages
        log.info(f"[Gateway] Model overridden by prompt marker: {prompt_model}")

    log.info(f"[Gateway] Normalized request: model={normalized['model']}, messages_count={len(normalized['messages'])}, stream={normalized.get('stream')}, tools_count={len(normalized.get('tools', []))}")

    return normalized


# 创建路由器
router = APIRouter(prefix="/gateway", tags=["Unified Gateway"])

# 后端服务配置
BACKENDS = {
    "antigravity": {
        "name": "Antigravity",
        "base_url": "http://127.0.0.1:7861/antigravity/v1",
        "priority": 1,  # 数字越小优先级越高
        "timeout": 60.0,  # 普通请求超时
        "stream_timeout": 300.0,  # 流式请求超时（5分钟）
        "max_retries": 2,  # 最大重试次数
        "enabled": True,
    },
    "copilot": {
        "name": "Copilot",
        "base_url": "http://127.0.0.1:4141/v1",
        "priority": 2,
        "timeout": 120.0,  # 思考模型需要更长时间
        "stream_timeout": 600.0,  # 流式请求超时（10分钟，GPT-5.2思考模型）
        "max_retries": 3,  # 最大重试次数
        "enabled": True,
    },
}

# 模型到后端的映射（可选，用于特定模型强制使用特定后端）
MODEL_BACKEND_MAPPING = {
    # Copilot 专属模型
    "gpt-4": "copilot",
    "gpt-4o": "copilot",
    "gpt-4o-mini": "copilot",
    "gpt-4.1": "copilot",
    "gpt-5": "copilot",
    "gpt-5.1": "copilot",
    "gpt-5.2": "copilot",
    # Antigravity 专属模型
    "gemini-2.5-pro": "antigravity",
    "gemini-2.5-flash": "antigravity",
    "gemini-3-pro-preview": "antigravity",
    # Claude 模型 - 两边都有，优先 Antigravity
    "claude-sonnet-4": "antigravity",
    "claude-sonnet-4.5": "antigravity",
    "claude-opus-4.5": "antigravity",
    "claude-haiku-4.5": "antigravity",
}


def get_sorted_backends() -> List[Tuple[str, Dict]]:
    """获取按优先级排序的后端列表"""
    enabled_backends = [(k, v) for k, v in BACKENDS.items() if v.get("enabled", True)]
    return sorted(enabled_backends, key=lambda x: x[1]["priority"])


def get_backend_for_model(model: str) -> Optional[str]:
    """根据模型名称获取指定后端"""
    # 精确匹配
    if model in MODEL_BACKEND_MAPPING:
        return MODEL_BACKEND_MAPPING[model]

    # 前缀匹配
    for model_prefix, backend in MODEL_BACKEND_MAPPING.items():
        if model.startswith(model_prefix):
            return backend

    return None  # 使用默认优先级


def calculate_retry_delay(attempt: int, config: Dict = None) -> float:
    """
    计算重试延迟时间（指数退避）
    
    Args:
        attempt: 当前重试次数（从0开始）
        config: 重试配置
    
    Returns:
        延迟时间（秒）
    """
    if config is None:
        config = RETRY_CONFIG
    
    base_delay = config.get("base_delay", 1.0)
    max_delay = config.get("max_delay", 10.0)
    exponential_base = config.get("exponential_base", 2)
    
    delay = base_delay * (exponential_base ** attempt)
    return min(delay, max_delay)


def should_retry(status_code: int, attempt: int, max_retries: int) -> bool:
    """
    判断是否应该重试
    
    Args:
        status_code: HTTP 状态码
        attempt: 当前重试次数
        max_retries: 最大重试次数
    
    Returns:
        是否应该重试
    """
    if attempt >= max_retries:
        return False
    
    retry_on_status = RETRY_CONFIG.get("retry_on_status", [500, 502, 503, 504])
    return status_code in retry_on_status


async def check_backend_health(backend_key: str) -> bool:
    """检查后端服务健康状态"""
    backend = BACKENDS.get(backend_key)
    if not backend or not backend.get("enabled", True):
        return False

    try:
        async with http_client.get_client(timeout=5.0) as client:
            response = await client.get(f"{backend['base_url']}/models")
            return response.status_code == 200
    except Exception as e:
        log.warning(f"Backend {backend_key} health check failed: {e}")
        return False


async def proxy_request_to_backend(
    backend_key: str,
    endpoint: str,
    method: str,
    headers: Dict[str, str],
    body: Any,
    stream: bool = False,
) -> Tuple[bool, Any]:
    """
    代理请求到指定后端（带重试机制）

    Returns:
        Tuple[bool, Any]: (成功标志, 响应内容或错误信息)
    """
    backend = BACKENDS.get(backend_key)
    if not backend:
        return False, f"Backend {backend_key} not found"

    url = f"{backend['base_url']}{endpoint}"

    # 根据请求类型选择超时时间
    if stream:
        timeout = backend.get("stream_timeout", backend.get("timeout", 300.0))
    else:
        timeout = backend.get("timeout", 60.0)

    # 获取最大重试次数
    max_retries = backend.get("max_retries", RETRY_CONFIG.get("max_retries", 3))

    # 构建请求头
    request_headers = {
        "Content-Type": "application/json",
        "Authorization": headers.get("authorization", "Bearer dummy"),
    }

    last_error = None
    last_status_code = None

    for attempt in range(max_retries + 1):  # +1 因为第一次不算重试
        try:
            if attempt > 0:
                delay = calculate_retry_delay(attempt - 1)
                log.info(f"[Gateway] Retry {attempt}/{max_retries} for {backend_key} after {delay:.1f}s delay")
                await asyncio.sleep(delay)

            if stream:
                # 流式请求（带超时）
                return await proxy_streaming_request_with_timeout(
                    url, method, request_headers, body, timeout, backend_key
                )
            else:
                # 非流式请求
                async with http_client.get_client(timeout=timeout) as client:
                    if method.upper() == "POST":
                        response = await client.post(url, json=body, headers=request_headers)
                    elif method.upper() == "GET":
                        response = await client.get(url, headers=request_headers)
                    else:
                        return False, f"Unsupported method: {method}"

                    last_status_code = response.status_code

                    if response.status_code >= 400:
                        error_text = response.text
                        log.warning(f"Backend {backend_key} returned error {response.status_code}: {error_text[:200]}")

                        # 检查是否应该重试
                        if should_retry(response.status_code, attempt, max_retries):
                            last_error = f"Backend error: {response.status_code}"
                            continue

                        return False, f"Backend error: {response.status_code}"

                    return True, response.json()

        except httpx.TimeoutException:
            log.warning(f"Backend {backend_key} timeout (attempt {attempt + 1}/{max_retries + 1})")
            last_error = "Request timeout"
            if attempt < max_retries:
                continue
        except httpx.ConnectError:
            log.warning(f"Backend {backend_key} connection failed (attempt {attempt + 1}/{max_retries + 1})")
            last_error = "Connection failed"
            if attempt < max_retries:
                continue
        except Exception as e:
            log.error(f"Backend {backend_key} request failed: {e}")
            last_error = str(e)
            # 对于未知错误，不重试
            break

    # 所有重试都失败
    log.error(f"Backend {backend_key} failed after {max_retries + 1} attempts. Last error: {last_error}")
    return False, last_error or "Unknown error"


async def proxy_streaming_request_with_timeout(
    url: str,
    method: str,
    headers: Dict[str, str],
    body: Any,
    timeout: float,
    backend_key: str = "unknown",
) -> Tuple[bool, Any]:
    """
    处理流式代理请求（带超时和错误处理）

    Args:
        url: 请求URL
        method: HTTP方法
        headers: 请求头
        body: 请求体
        timeout: 超时时间（秒）
        backend_key: 后端标识（用于日志）
    """
    try:
        # 创建带超时的客户端
        timeout_config = httpx.Timeout(
            connect=30.0,      # 连接超时
            read=timeout,      # 读取超时（流式数据）
            write=30.0,        # 写入超时
            pool=30.0,         # 连接池超时
        )
        client = httpx.AsyncClient(timeout=timeout_config)

        async def stream_generator():
            last_data_time = time.time()
            chunk_timeout = 120.0  # 单个chunk的超时时间（2分钟）

            try:
                async with client.stream(method, url, json=body, headers=headers) as response:
                    if response.status_code >= 400:
                        error_text = await response.aread()
                        log.warning(f"[Gateway] Streaming request to {backend_key} failed: {response.status_code}")
                        error_msg = json.dumps({'error': 'Backend error', 'status': response.status_code})
                        yield f"data: {error_msg}\n\n"
                        return

                    log.info(f"[Gateway] Streaming started from {backend_key}")

                    async for chunk in response.aiter_bytes():
                        if chunk:
                            current_time = time.time()
                            # 检查是否超过chunk超时
                            if current_time - last_data_time > chunk_timeout:
                                log.warning(f"[Gateway] Chunk timeout from {backend_key} after {chunk_timeout}s")
                                error_msg = json.dumps({'error': 'Chunk timeout', 'message': 'No data received for too long'})
                                yield f"data: {error_msg}\n\n"
                                break

                            last_data_time = current_time
                            yield chunk.decode("utf-8", errors="ignore")

                    log.info(f"[Gateway] Streaming completed from {backend_key}")

            except httpx.ReadTimeout:
                log.warning(f"[Gateway] Read timeout from {backend_key} after {timeout}s")
                error_msg = json.dumps({'error': 'Read timeout', 'message': f'No response within {timeout}s'})
                yield f"data: {error_msg}\n\n"
            except httpx.ConnectTimeout:
                log.warning(f"[Gateway] Connect timeout to {backend_key}")
                error_msg = json.dumps({'error': 'Connect timeout'})
                yield f"data: {error_msg}\n\n"
            except Exception as e:
                log.error(f"[Gateway] Streaming error from {backend_key}: {e}")
                error_msg = json.dumps({'error': str(e)})
                yield f"data: {error_msg}\n\n"
            finally:
                await client.aclose()

        return True, stream_generator()

    except Exception as e:
        log.error(f"[Gateway] Failed to start streaming from {backend_key}: {e}")
        return False, str(e)


async def proxy_streaming_request(
    url: str,
    method: str,
    headers: Dict[str, str],
    body: Any,
    timeout: float,
) -> Tuple[bool, Any]:
    """处理流式代理请求（兼容旧接口）"""
    try:
        client = httpx.AsyncClient(timeout=None)

        async def stream_generator():
            try:
                async with client.stream(method, url, json=body, headers=headers) as response:
                    if response.status_code >= 400:
                        error_text = await response.aread()
                        log.warning(f"Streaming request failed: {response.status_code}")
                        error_msg = json.dumps({'error': 'Backend error', 'status': response.status_code})
                        yield f"data: {error_msg}\n\n"
                        return

                    async for chunk in response.aiter_bytes():
                        if chunk:
                            yield chunk.decode("utf-8", errors="ignore")
            finally:
                await client.aclose()

        return True, stream_generator()

    except Exception as e:
        log.error(f"Streaming request failed: {e}")
        return False, str(e)


async def route_request_with_fallback(
    endpoint: str,
    method: str,
    headers: Dict[str, str],
    body: Any,
    model: Optional[str] = None,
    stream: bool = False,
) -> Any:
    """
    带故障转移的请求路由

    优先使用指定后端，失败时自动切换到备用后端
    """
    # 确定后端顺序
    specified_backend = get_backend_for_model(model) if model else None
    sorted_backends = get_sorted_backends()

    if specified_backend:
        # 将指定后端移到最前面
        sorted_backends = [(k, v) for k, v in sorted_backends if k == specified_backend] + \
                         [(k, v) for k, v in sorted_backends if k != specified_backend]

    last_error = None

    for backend_key, backend_config in sorted_backends:
        log.info(f"Trying backend: {backend_config['name']} for {endpoint}")

        success, result = await proxy_request_to_backend(
            backend_key, endpoint, method, headers, body, stream
        )

        if success:
            log.info(f"Request succeeded via {backend_config['name']}")
            return result

        last_error = result
        log.warning(f"Backend {backend_config['name']} failed: {result}, trying next...")

    # 所有后端都失败
    raise HTTPException(
        status_code=503,
        detail=f"All backends failed. Last error: {last_error}"
    )


# ==================== API 端点 ====================


@router.get("/v1/models")
@router.get("/models")  # 别名路由，兼容不同客户端配置
async def list_models(request: Request):
    """获取所有后端的模型列表（合并去重）"""
    log.info(f"[Gateway] Received models request at {request.url.path}")
    all_models = set()

    for backend_key, backend_config in get_sorted_backends():
        try:
            async with http_client.get_client(timeout=10.0) as client:
                response = await client.get(
                    f"{backend_config['base_url']}/models",
                    headers={"Authorization": "Bearer dummy"}
                )
                if response.status_code == 200:
                    data = response.json()
                    models = data.get("data", [])
                    for model in models:
                        model_id = model.get("id") if isinstance(model, dict) else model
                        if model_id:
                            all_models.add(model_id)
        except Exception as e:
            log.warning(f"Failed to get models from {backend_key}: {e}")

    return {
        "object": "list",
        "data": [{"id": m, "object": "model", "owned_by": "gateway"} for m in sorted(all_models)]
    }


@router.post("/v1/chat/completions")
@router.post("/chat/completions")  # 别名路由，兼容 Base URL 为 /gateway 的客户端
async def chat_completions(
    request: Request,
    token: str = Depends(authenticate_bearer)
):
    """统一聊天完成端点 - 自动路由到最佳后端"""
    log.info(f"[Gateway] Received chat request at {request.url.path}")
    try:
        raw_body = await request.json()
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Invalid JSON: {e}")

    # Normalize request body to standard OpenAI format
    body = normalize_request_body(raw_body)

    model = body.get("model", "")
    stream = body.get("stream", False)

    headers = dict(request.headers)

    result = await route_request_with_fallback(
        endpoint="/chat/completions",
        method="POST",
        headers=headers,
        body=body,
        model=model,
        stream=stream,
    )

    if stream and hasattr(result, "__anext__"):
        return StreamingResponse(
            result,
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
            }
        )

    return JSONResponse(content=result)


@router.post("/v1/messages")
@router.post("/messages")  # 别名路由，兼容 Base URL 为 /gateway 的客户端
async def anthropic_messages(
    request: Request,
    token: str = Depends(authenticate_bearer)
):
    """Anthropic Messages API 兼容端点"""
    log.info(f"[Gateway] Received messages request at {request.url.path}")
    try:
        body = await request.json()
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Invalid JSON: {e}")

    model = body.get("model", "")
    stream = body.get("stream", False)

    headers = dict(request.headers)

    result = await route_request_with_fallback(
        endpoint="/messages",
        method="POST",
        headers=headers,
        body=body,
        model=model,
        stream=stream,
    )

    if stream and hasattr(result, "__anext__"):
        return StreamingResponse(
            result,
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            }
        )

    return JSONResponse(content=result)


@router.get("/health")
async def gateway_health():
    """网关健康检查 - 返回所有后端状态"""
    backend_status = {}

    for backend_key, backend_config in BACKENDS.items():
        is_healthy = await check_backend_health(backend_key)
        backend_status[backend_key] = {
            "name": backend_config["name"],
            "url": backend_config["base_url"],
            "priority": backend_config["priority"],
            "enabled": backend_config.get("enabled", True),
            "healthy": is_healthy,
        }

    all_healthy = any(s["healthy"] for s in backend_status.values())

    return {
        "status": "healthy" if all_healthy else "degraded",
        "backends": backend_status,
        "timestamp": time.time(),
    }


@router.post("/config/backend/{backend_key}/toggle")
async def toggle_backend(
    backend_key: str,
    token: str = Depends(authenticate_bearer)
):
    """启用/禁用指定后端"""
    if backend_key not in BACKENDS:
        raise HTTPException(status_code=404, detail=f"Backend {backend_key} not found")

    BACKENDS[backend_key]["enabled"] = not BACKENDS[backend_key].get("enabled", True)

    return {
        "backend": backend_key,
        "enabled": BACKENDS[backend_key]["enabled"],
    }
