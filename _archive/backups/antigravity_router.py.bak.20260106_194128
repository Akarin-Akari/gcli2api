"""
Antigravity Router - Handles OpenAI and Gemini format requests and converts to Antigravity API
处理 OpenAI 和 Gemini 格式请求并转换为 Antigravity API 格式
"""

import json
import time
import uuid
from typing import Any, Dict, List, Optional

from fastapi import APIRouter, Depends, HTTPException, Path, Request
from fastapi.responses import JSONResponse, StreamingResponse

from config import get_anti_truncation_max_attempts
from log import log
from src.utils import is_anti_truncation_model, authenticate_bearer, authenticate_gemini_flexible, authenticate_sdwebui_flexible

from .antigravity_api import (
    build_antigravity_request_body,
    send_antigravity_request_no_stream,
    send_antigravity_request_stream,
    fetch_available_models,
)
from .credential_manager import CredentialManager
from .models import (
    ChatCompletionRequest,
    GeminiGenerationConfig,
    Model,
    ModelList,
    model_to_dict,
    OpenAIChatCompletionChoice,
    OpenAIChatCompletionResponse,
    OpenAIChatMessage,
    OpenAIToolCall,
    OpenAIToolFunction,
)
from .anti_truncation import (
    apply_anti_truncation_to_stream,
)

# 导入格式转换器
from .converters import (
    # model_config
    model_mapping,
    get_fallback_models,
    should_fallback_on_error,
    is_thinking_model,
    # message_converter
    extract_images_from_content,
    strip_thinking_from_openai_messages,
    openai_messages_to_antigravity_contents,
    gemini_contents_to_antigravity_contents,
    # tool_converter
    extract_tool_params_summary,
    convert_openai_tools_to_antigravity,
    generate_generation_config,
    # tool_converter (validation)
    validate_tool_name,
)

# 导入自定义功能模块
from .stream_error_handler import (
    build_safety_blocked_message,
    build_no_response_message,
    build_context_too_long_message,
    build_streaming_error_message,
    SSEChunkBuilder,
    try_non_streaming_fallback,
    StreamState,
)
from .tool_cleaner import (
    clean_tools_list,
    should_enable_cross_pool_fallback,
    log_tools_info,
    get_client_info,
)
from .context_analyzer import (
    build_context_info,
    should_use_non_streaming_fallback as check_should_use_non_streaming_fallback,
    has_valid_thinking_in_messages,
    should_disable_thinking,
    check_thinking_in_messages,
)

# 创建路由器
router = APIRouter()

# 全局凭证管理器实例
credential_manager = None


async def get_credential_manager():
    """获取全局凭证管理器实例"""
    global credential_manager
    if not credential_manager:
        credential_manager = CredentialManager()
        await credential_manager.initialize()
    return credential_manager


# ====================== 工具调用验证 ======================

def validate_tool_call(
    function_call: Dict[str, Any],
    available_tools: Optional[List[Dict[str, Any]]] = None
) -> tuple[bool, str, Optional[Dict[str, Any]]]:
    """
    验证工具调用的有效性

    检查：
    1. 工具名称是否存在
    2. 工具名称是否在可用工具列表中（如果提供）
    3. 工具参数是否为有效的 JSON 对象

    Args:
        function_call: Antigravity 格式的函数调用
        available_tools: 可用工具列表（OpenAI 格式）

    Returns:
        (is_valid, message, fixed_call)
        - is_valid: 是否有效
        - message: 验证消息
        - fixed_call: 修复后的调用（如果可以修复）
    """
    if not isinstance(function_call, dict):
        return False, "Tool call must be a dictionary", None

    name = function_call.get("name")
    args = function_call.get("args", {})
    call_id = function_call.get("id")


    # [ENHANCED] Use validate_tool_name from tool_converter for thorough validation
    name_valid, name_error, sanitized_name = validate_tool_name(name)
    if not name_valid:
        log.warning(f"[TOOL VALIDATION] Invalid tool name: {name_error}. Original: {name}")
        return False, f"Invalid tool name: {name_error}", None
    
    # Use sanitized name (may have been fixed)
    if sanitized_name and sanitized_name != name:
        log.info(f"[TOOL VALIDATION] Tool name sanitized: '{name}' -> '{sanitized_name}'")
        name = sanitized_name

    # 验证参数
    if args is None:
        args = {}
    
    if not isinstance(args, dict):
        # 尝试解析 JSON 字符串
        if isinstance(args, str):
            try:
                args = json.loads(args)
                if not isinstance(args, dict):
                    return False, f"Tool call 'args' must be a JSON object, got {type(args).__name__}", None
            except json.JSONDecodeError as e:
                return False, f"Tool call 'args' is not valid JSON: {e}", None
        else:
            return False, f"Tool call 'args' must be a dictionary, got {type(args).__name__}", None

    # 如果提供了可用工具列表，验证工具名称是否存在
    if available_tools:
        tool_names = set()
        for tool in available_tools:
            if isinstance(tool, dict):
                # OpenAI 格式: {"type": "function", "function": {"name": "..."}}
                if "function" in tool:
                    func_def = tool.get("function", {})
                    if isinstance(func_def, dict):
                        tool_names.add(func_def.get("name", ""))
                # 扁平格式: {"name": "..."}
                elif "name" in tool:
                    tool_names.add(tool.get("name", ""))

        if name not in tool_names and tool_names:
            # 工具名称不在列表中，但仍然允许调用（可能是动态工具）
            log.warning(f"[TOOL VALIDATION] Tool '{name}' not found in available tools list. "
                       f"Available: {list(tool_names)[:5]}... This may be a dynamic tool.")

    # 构建修复后的调用
    fixed_call = {
        "name": name,
        "args": args,
    }
    if call_id:
        fixed_call["id"] = call_id

    return True, "Valid tool call", fixed_call


def validate_tool_call_result(
    tool_result: Dict[str, Any],
    expected_tool_id: Optional[str] = None
) -> tuple[bool, str]:
    """
    验证工具调用结果的有效性

    Args:
        tool_result: 工具调用结果
        expected_tool_id: 期望的工具调用 ID

    Returns:
        (is_valid, message)
    """
    if not isinstance(tool_result, dict):
        return False, "Tool result must be a dictionary"

    # 检查必需字段
    tool_call_id = tool_result.get("tool_call_id")
    if not tool_call_id:
        return False, "Tool result missing 'tool_call_id' field"

    # 如果提供了期望的 ID，验证匹配
    if expected_tool_id and tool_call_id != expected_tool_id:
        log.warning(f"[TOOL VALIDATION] Tool result ID mismatch: expected '{expected_tool_id}', got '{tool_call_id}'")

    # 检查内容
    content = tool_result.get("content")
    if content is None:
        log.debug("[TOOL VALIDATION] Tool result has no 'content' field")

    return True, "Valid tool result"


def convert_to_openai_tool_call(function_call: Dict[str, Any], index: int = None) -> Dict[str, Any]:
    """
    将 Antigravity functionCall 转换为 OpenAI tool_call，使用 OpenAIToolCall 模型

    Args:
        function_call: Antigravity 格式的函数调用
        index: 工具调用索引（流式响应必需）
    """
    tool_call = OpenAIToolCall(
        index=index,
        id=function_call.get("id", f"call_{uuid.uuid4().hex[:24]}"),
        type="function",
        function=OpenAIToolFunction(
            name=function_call.get("name", ""),
            arguments=json.dumps(function_call.get("args", {}))
        )
    )
    result = model_to_dict(tool_call)
    # Remove None values for cleaner output (non-streaming doesn't need index)
    if result.get("index") is None:
        del result["index"]
    return result


async def convert_antigravity_stream_to_openai(
    lines_generator: Any,
    stream_ctx: Any,
    client: Any,
    model: str,
    request_id: str,
    credential_manager: Any,
    credential_name: str,
    request_body: Optional[Dict[str, Any]] = None,  # ✅ 新增：用于获取上下文信息
    cred_mgr: Optional[Any] = None,  # ✅ 新增：用于 fallback（未来可能使用）
    context_info: Optional[Dict[str, Any]] = None  # ✅ 新增：上下文信息（token 数、工具结果数量等）
):
    """
    将 Antigravity 流式响应转换为 OpenAI 格式的 SSE 流

    Args:
        lines_generator: 行生成器 (已经过滤的 SSE 行)
    """
    state = {
        "thinking_started": False,
        "tool_calls": [],
        "content_buffer": "",
        "thinking_buffer": "",
        "success_recorded": False,
        "sse_lines_received": 0,  # 记录收到的 SSE 行数
        "chunks_sent": 0,  # 记录发送的 chunk 数
        "tool_calls_sent": False,  # 标记工具调用是否已发送
        "finish_reason_sent": False,  # 标记 finish_reason 是否已发送
        "has_valid_content": False,  # 标记是否收到了有效内容（text 或 functionCall）
        "empty_parts_count": 0,  # 记录空 parts 的次数
        # [OBSERVABILITY] Thinking 退化状态检测
        "thinking_parts_count": 0,  # thinking parts 数量
        "total_thinking_chars": 0,  # thinking 内容总字符数
        "short_thinking_parts": 0,  # 短 thinking parts 数量（<50字符，可能是退化状态）
    }

    created = int(time.time())

    try:
        def build_content_chunk(content: str) -> str:
            chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {"content": content},
                    "finish_reason": None
                }]
            }
            return f"data: {json.dumps(chunk)}\n\n"

        def flush_thinking_buffer() -> Optional[str]:
            if not state["thinking_started"]:
                return None
            state["thinking_buffer"] += "\n</think>\n"
            thinking_block = state["thinking_buffer"]
            state["content_buffer"] += thinking_block
            state["thinking_buffer"] = ""
            state["thinking_started"] = False
            return thinking_block

        async for line in lines_generator:
            if not line or not line.startswith("data: "):
                continue

            # 记录第一次成功响应
            if not state["success_recorded"]:
                if credential_name and credential_manager:
                    await credential_manager.record_api_call_result(credential_name, True, is_antigravity=True)
                state["success_recorded"] = True

            # 解析 SSE 数据
            try:
                data = json.loads(line[6:])  # 去掉 "data: " 前缀
            except:
                continue

            state["sse_lines_received"] += 1

            # DEBUG: 记录收到的原始数据（用于诊断空响应问题）
            log.info(f"[ANTIGRAVITY STREAM] SSE line {state['sse_lines_received']}: {json.dumps(data)[:500]}")
            log.debug(f"[ANTIGRAVITY STREAM] Received data keys: {list(data.keys())}")

            # ✅ 新增：提取 cachedContentTokenCount（如果可用）
            # 这个信息可能在第一个 chunk 就出现，用于判断实际处理的 tokens
            usage_metadata = data.get("response", {}).get("usageMetadata", {})
            if usage_metadata:
                cached_content_token_count = usage_metadata.get("cachedContentTokenCount", 0)
                if cached_content_token_count > 0 and "cached_content_token_count" not in state:
                    # 只在第一次提取时保存（避免覆盖）
                    state["cached_content_token_count"] = cached_content_token_count
                    prompt_token_count = usage_metadata.get("promptTokenCount", 0)
                    if prompt_token_count > 0:
                        state["prompt_token_count"] = prompt_token_count
                        actual_processed_tokens = prompt_token_count - cached_content_token_count
                        state["actual_processed_tokens"] = actual_processed_tokens
                        log.info(f"[ANTIGRAVITY STREAM] Cache detected: {cached_content_token_count:,} tokens cached, "
                               f"{actual_processed_tokens:,} tokens actually processed (out of {prompt_token_count:,} total)")

            # 检查是否有错误
            if "error" in data:
                log.error(f"[ANTIGRAVITY STREAM] Error in response: {data.get('error')}")

            # 提取 candidates 和 parts
            response = data.get("response", {})
            candidates = response.get("candidates", [])
            log.info(f"[ANTIGRAVITY STREAM] Response has {len(candidates)} candidates")

            if candidates:
                candidate = candidates[0]
                content_obj = candidate.get("content", {})
                finish_reason_raw = candidate.get("finishReason")
                log.info(f"[ANTIGRAVITY STREAM] Candidate 0: finishReason={finish_reason_raw}, content_keys={list(content_obj.keys()) if content_obj else 'None'}")
                parts = content_obj.get("parts", [])

                # 检测空 parts 情况
                if not parts and finish_reason_raw:
                    state["empty_parts_count"] += 1
                    log.warning(f"[ANTIGRAVITY STREAM] Empty parts with finishReason={finish_reason_raw}! This may cause empty response.")
            else:
                parts = []
                log.warning(f"[ANTIGRAVITY STREAM] No candidates in response!")

            # DEBUG: 记录 parts 信息
            if parts:
                part_types = [list(p.keys()) if isinstance(p, dict) else type(p).__name__ for p in parts]
                log.debug(f"[ANTIGRAVITY STREAM] Parts count: {len(parts)}, types: {part_types}")

            # 检查 finishReason（提前检查以便记录）
            candidates = data.get("response", {}).get("candidates", [])
            if candidates:
                candidate = candidates[0]
                fr = candidate.get("finishReason")
                if fr:
                    log.info(f"[ANTIGRAVITY STREAM] finishReason detected: {fr}")
                    # 检查是否有内容
                    content_obj = candidate.get("content", {})
                    log.debug(f"[ANTIGRAVITY STREAM] Final content keys: {list(content_obj.keys()) if content_obj else 'None'}")

            for part in parts:
                # 处理思考内容
                if part.get("thought") is True:
                    if not state["thinking_started"]:
                        state["thinking_buffer"] = "<think>\n"
                        state["thinking_started"] = True

                    thinking_text = part.get("text", "")
                    state["thinking_buffer"] += thinking_text

                    # [OBSERVABILITY] 统计 thinking parts 用于退化检测
                    state["thinking_parts_count"] += 1
                    state["total_thinking_chars"] += len(thinking_text)
                    if len(thinking_text) < 50:
                        state["short_thinking_parts"] += 1
                        log.debug(f"[THINKING OBSERVABILITY] Short thinking part detected: "
                                 f"length={len(thinking_text)}, content='{thinking_text[:100]}...'")


                # 处理图片数据 (inlineData)
                elif "inlineData" in part:
                    # 如果之前在思考，先结束思考
                    thinking_block = flush_thinking_buffer()
                    if thinking_block:
                        yield build_content_chunk(thinking_block)

                    # 提取图片数据
                    inline_data = part["inlineData"]
                    mime_type = inline_data.get("mimeType", "image/png")
                    base64_data = inline_data.get("data", "")

                    # 转换为 Markdown 格式的图片
                    image_markdown = f"\n\n![生成的图片](data:{mime_type};base64,{base64_data})\n\n"
                    state["content_buffer"] += image_markdown

                    # 发送图片块
                    chunk = {
                        "id": request_id,
                        "object": "chat.completion.chunk",
                        "created": created,
                        "model": model,
                        "choices": [{
                            "index": 0,
                            "delta": {"content": image_markdown},
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(chunk)}\n\n"

                # 处理普通文本
                elif "text" in part:
                    # 如果之前在思考，先结束思考
                    thinking_block = flush_thinking_buffer()
                    if thinking_block:
                        yield build_content_chunk(thinking_block)

                    # 添加文本内容
                    text = part.get("text", "")

                    # 只有当 text 非空时才发送和标记为有效内容
                    # 修复：Antigravity 有时返回 {"text": ""} 空字符串，不应视为有效内容
                    if text:  # 非空字符串才处理
                        state["content_buffer"] += text

                        # 发送文本块
                        chunk = {
                            "id": request_id,
                            "object": "chat.completion.chunk",
                            "created": created,
                            "model": model,
                            "choices": [{
                                "index": 0,
                                "delta": {"content": text},
                                "finish_reason": None
                            }]
                        }
                        yield f"data: {json.dumps(chunk)}\n\n"
                        state["chunks_sent"] += 1
                        state["has_valid_content"] = True  # 收到了有效的文本内容
                    else:
                        # ✅ 新增：记录空文本，但如果是第一个 chunk 且无 finishReason，继续等待
                        state["empty_parts_count"] = state.get("empty_parts_count", 0) + 1
                        if state["sse_lines_received"] == 1 and not finish_reason_raw:
                            log.info(f"[ANTIGRAVITY STREAM] First chunk has empty text and no finishReason, "
                                   f"waiting for more chunks (this may indicate stream is starting or context is too long)")
                            # 不立即标记为无效，继续等待后续 chunk
                        else:
                            log.debug(f"[ANTIGRAVITY STREAM] Skipping empty text in part")

                # 处理工具调用
                elif "functionCall" in part:
                    tool_index = len(state["tool_calls"])
                    fc = part["functionCall"]
                    
                    # ✅ 新增：验证工具调用
                    is_valid, validation_msg, fixed_fc = validate_tool_call(fc)
                    if not is_valid:
                        log.warning(f"[ANTIGRAVITY STREAM] Invalid tool call: {validation_msg}. Original: {fc}")
                        # 跳过无效的工具调用，但记录日志
                        continue
                    
                    # 使用修复后的工具调用（如果有修复）
                    if fixed_fc:
                        fc = fixed_fc
                    
                    log.info(f"[ANTIGRAVITY STREAM] Tool call detected: name={fc.get('name')}, id={fc.get('id')}")
                    tool_call = convert_to_openai_tool_call(fc, index=tool_index)
                    state["tool_calls"].append(tool_call)
                    state["has_valid_content"] = True  # 收到了有效的工具调用
                    log.debug(f"[ANTIGRAVITY STREAM] Converted tool_call: {json.dumps(tool_call)[:200]}")

            # 检查是否结束
            finish_reason = data.get("response", {}).get("candidates", [{}])[0].get("finishReason")

            # ✅ 新增：处理 SAFETY/RECITATION finishReason
            if finish_reason in ("SAFETY", "RECITATION"):
                log.warning(f"[ANTIGRAVITY STREAM] Response blocked by {finish_reason} filter")

                # ✅ 构建包含工具提示的错误消息
                error_msg_parts = []
                error_msg_parts.append(f"[Response blocked by {finish_reason} filter. The content may have triggered safety policies.]")
                error_msg_parts.append("")
                error_msg_parts.append("⚠️ **Tool Call Format Reminder**:")
                error_msg_parts.append("If you encounter 'invalid arguments' errors when calling tools:")
                error_msg_parts.append("- Use EXACT parameter names from the current tool schema")
                error_msg_parts.append("- Do NOT use parameters from previous conversations")
                error_msg_parts.append("- For terminal/command tools: verify parameter name (may be `command`, `input`, or `cmd`)")

                error_chunk = {
                    "id": request_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": model,
                    "choices": [{
                        "index": 0,
                        "delta": {"content": "\n".join(error_msg_parts)},
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(error_chunk)}\n\n"
                state["chunks_sent"] += 1
                state["has_valid_content"] = True  # 标记为有内容（错误消息）

            if finish_reason:
                thinking_block = flush_thinking_buffer()
                if thinking_block:
                    yield build_content_chunk(thinking_block)

                # 发送工具调用
                if state["tool_calls"]:
                    log.info(f"[ANTIGRAVITY STREAM] Sending {len(state['tool_calls'])} tool calls")
                    chunk = {
                        "id": request_id,
                        "object": "chat.completion.chunk",
                        "created": created,
                        "model": model,
                        "choices": [{
                            "index": 0,
                            "delta": {"tool_calls": state["tool_calls"]},
                            "finish_reason": None
                        }]
                    }
                    log.debug(f"[ANTIGRAVITY STREAM] Tool calls chunk: {json.dumps(chunk)[:500]}")
                    yield f"data: {json.dumps(chunk)}\n\n"
                    state["tool_calls_sent"] = True  # 标记工具调用已发送

                # 发送使用统计
                usage_metadata = data.get("response", {}).get("usageMetadata", {})
                prompt_token_count = usage_metadata.get("promptTokenCount", 0)
                cached_content_token_count = usage_metadata.get("cachedContentTokenCount", 0)
                # ✅ 新增：保存 promptTokenCount 和 cachedContentTokenCount 到 state，用于错误消息
                state["prompt_token_count"] = prompt_token_count
                state["cached_content_token_count"] = cached_content_token_count
                # ✅ 计算实际处理的 tokens（排除缓存的部分）
                actual_processed_tokens = prompt_token_count - cached_content_token_count
                state["actual_processed_tokens"] = actual_processed_tokens
                usage = {
                    "prompt_tokens": prompt_token_count,
                    "completion_tokens": usage_metadata.get("candidatesTokenCount", 0),
                    "total_tokens": usage_metadata.get("totalTokenCount", 0)
                }

                # 确定 finish_reason
                openai_finish_reason = "stop"
                if state["tool_calls"]:
                    openai_finish_reason = "tool_calls"
                elif finish_reason == "MAX_TOKENS":
                    openai_finish_reason = "length"

                chunk = {
                    "id": request_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": model,
                    "choices": [{
                        "index": 0,
                        "delta": {},
                        "finish_reason": openai_finish_reason
                    }],
                    "usage": usage
                }
                yield f"data: {json.dumps(chunk)}\n\n"

        # 在流结束前，检查是否有未发送的工具调用
        # 这是一个保底逻辑，用于处理 finishReason 没有被正确检测到的情况
        if state["tool_calls"] and not state.get("tool_calls_sent", False):
            log.warning(f"[ANTIGRAVITY STREAM] Tool calls not sent yet, sending now (fallback logic)")
            log.info(f"[ANTIGRAVITY STREAM] Sending {len(state['tool_calls'])} tool calls (fallback)")

            # 发送工具调用
            chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {"tool_calls": state["tool_calls"]},
                    "finish_reason": None
                }]
            }
            yield f"data: {json.dumps(chunk)}\n\n"
            state["chunks_sent"] += 1

            # 发送 finish_reason
            finish_chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {},
                    "finish_reason": "tool_calls"
                }]
            }
            yield f"data: {json.dumps(finish_chunk)}\n\n"
            state["chunks_sent"] += 1
            state["finish_reason_sent"] = True

        # 检查是否收到了任何有效数据但没有发送任何内容
        # 这可能表示 Antigravity 后端返回了空响应
        if state["sse_lines_received"] == 0:
            log.warning(f"[ANTIGRAVITY STREAM] No SSE data received from Antigravity backend!")

            # ✅ 构建包含工具提示的错误消息
            error_msg_parts = []
            error_msg_parts.append("[Error: No response from backend. Please try again.]")
            error_msg_parts.append("")
            error_msg_parts.append("⚠️ **Tool Call Format Reminder**:")
            error_msg_parts.append("If you encounter 'invalid arguments' errors when calling tools:")
            error_msg_parts.append("- Use EXACT parameter names from the current tool schema")
            error_msg_parts.append("- Do NOT use parameters from previous conversations")
            error_msg_parts.append("- For terminal/command tools: verify parameter name (may be `command`, `input`, or `cmd`)")
            error_msg_parts.append("- When in doubt: re-read the tool definition")

            error_chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {"content": "\n".join(error_msg_parts)},
                    "finish_reason": None
                }]
            }
            yield f"data: {json.dumps(error_chunk)}\n\n"

            # 发送 finish_reason
            finish_chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {},
                    "finish_reason": "stop"
                }]
            }
            yield f"data: {json.dumps(finish_chunk)}\n\n"
            state["finish_reason_sent"] = True

        elif not state.get("has_valid_content", False):
            # 没有收到任何有效内容（text 或 functionCall）
            # 这是后端异常，应该明确告知前端
            log.error(f"[ANTIGRAVITY STREAM] No valid content received! "
                      f"SSE lines: {state['sse_lines_received']}, "
                      f"empty_parts_count: {state.get('empty_parts_count', 0)}, "
                      f"finish_reason: {finish_reason}")
            
            # ✅ 新增：检查是否只有空文本 chunk（可能是上下文过长或流式响应被截断）
            if state.get("empty_parts_count", 0) > 0 and state["chunks_sent"] == 0:
                log.warning(f"[ANTIGRAVITY STREAM] Stream ended with only empty text chunks. "
                           f"This may indicate: 1) Context too long (check promptTokenCount), "
                           f"2) API capacity issue, 3) Stream truncated before finishReason")

            # ✅ 新增：构建明确的、可操作的错误消息，让 Cursor 的 AI agent 知道需要压缩上下文
            # 从 state 中获取 promptTokenCount 和缓存信息（如果可用）
            prompt_token_count = state.get("prompt_token_count", 0)
            cached_content_token_count = state.get("cached_content_token_count", 0)
            actual_processed_tokens = state.get("actual_processed_tokens", 0)
            
            # ✅ 增强：基于多种条件进行动态 fallback
            # 1. 如果实际处理的 tokens 超过阈值（50K），尝试非流式 fallback
            # 2. 如果没有 token 信息但估算 tokens 超过阈值，也尝试 fallback
            # 3. 如果收到了空 parts 但没有有效内容，也尝试 fallback
            ACTUAL_PROCESSED_TOKENS_THRESHOLD = 50000  # 50K tokens 阈值
            ESTIMATED_TOKENS_THRESHOLD = 60000  # 60K tokens 估算阈值

            # [FIX] Extract estimated_tokens BEFORE usage in fallback condition (fix variable scope bug)
            estimated_tokens = context_info.get("estimated_tokens", 0) if context_info else 0
            
            # 判断是否应该尝试 fallback
            should_try_non_streaming_fallback = (
                request_body and 
                cred_mgr and 
                not state.get("fallback_attempted", False) and  # 避免重复尝试
                (
                    # 条件1：实际处理的 tokens 超过阈值
                    actual_processed_tokens > ACTUAL_PROCESSED_TOKENS_THRESHOLD or
                    # 条件2：估算 tokens 超过阈值（当没有实际 token 信息时）
                    (actual_processed_tokens == 0 and estimated_tokens > ESTIMATED_TOKENS_THRESHOLD) or
                    # 条件3：收到了空 parts 且 SSE 行数 > 0（表示 API 有响应但内容为空）
                    (state.get("empty_parts_count", 0) > 0 and state["sse_lines_received"] > 0)
                )
            )
            
            # 记录 fallback 决策原因
            if should_try_non_streaming_fallback:
                fallback_reason = []
                if actual_processed_tokens > ACTUAL_PROCESSED_TOKENS_THRESHOLD:
                    fallback_reason.append(f"actual_tokens={actual_processed_tokens:,} > {ACTUAL_PROCESSED_TOKENS_THRESHOLD:,}")
                if actual_processed_tokens == 0 and estimated_tokens > ESTIMATED_TOKENS_THRESHOLD:
                    fallback_reason.append(f"estimated_tokens={estimated_tokens:,} > {ESTIMATED_TOKENS_THRESHOLD:,}")
                if state.get("empty_parts_count", 0) > 0:
                    fallback_reason.append(f"empty_parts={state.get('empty_parts_count', 0)}")
                
                log.warning(f"[ANTIGRAVITY STREAM] Triggering non-streaming fallback. Reason: {', '.join(fallback_reason)}")
                state["fallback_attempted"] = True
                
                try:
                    # 尝试非流式请求
                    from .antigravity_api import send_antigravity_request_no_stream
                    response_data, _, _ = await send_antigravity_request_no_stream(
                        request_body, cred_mgr
                    )
                    
                    # 转换非流式响应为 OpenAI 格式
                    openai_response_dict = convert_antigravity_response_to_openai(response_data, model, request_id)
                    
                    # 提取内容和工具调用
                    fallback_content = ""
                    fallback_tool_calls = []
                    if openai_response_dict and openai_response_dict.get("choices"):
                        choice = openai_response_dict["choices"][0]
                        message = choice.get("message", {})
                        if message and message.get("content"):
                            fallback_content = message["content"]
                        if message and message.get("tool_calls"):
                            fallback_tool_calls = message["tool_calls"]
                    
                    if fallback_content or fallback_tool_calls:
                        log.info(f"[ANTIGRAVITY STREAM] Non-streaming fallback successful! "
                               f"Content length: {len(fallback_content)}, Tool calls: {len(fallback_tool_calls)}")
                        
                        # 发送工具调用（如果存在）
                        if fallback_tool_calls:
                            tool_chunk = {
                                "id": request_id,
                                "object": "chat.completion.chunk",
                                "created": created,
                                "model": model,
                                "choices": [{
                                    "index": 0,
                                    "delta": {"tool_calls": fallback_tool_calls},
                                    "finish_reason": None
                                }]
                            }
                            yield f"data: {json.dumps(tool_chunk)}\n\n"
                            state["chunks_sent"] += 1
                            state["has_valid_content"] = True
                        
                        # 发送内容（如果存在，分块发送以保持流式体验）
                        if fallback_content:
                            # 将内容分成小块发送，模拟流式响应
                            chunk_size = 100  # 每块约100字符
                            for i in range(0, len(fallback_content), chunk_size):
                                chunk_text = fallback_content[i:i + chunk_size]
                                content_chunk = {
                                    "id": request_id,
                                    "object": "chat.completion.chunk",
                                    "created": created,
                                    "model": model,
                                    "choices": [{
                                        "index": 0,
                                        "delta": {"content": chunk_text},
                                        "finish_reason": None
                                    }]
                                }
                                yield f"data: {json.dumps(content_chunk)}\n\n"
                                state["chunks_sent"] += 1
                            state["has_valid_content"] = True
                        
                        # 发送 finish_reason
                        finish_reason_fallback = "stop"
                        if fallback_tool_calls:
                            finish_reason_fallback = "tool_calls"
                        
                        finish_chunk = {
                            "id": request_id,
                            "object": "chat.completion.chunk",
                            "created": created,
                            "model": model,
                            "choices": [{
                                "index": 0,
                                "delta": {},
                                "finish_reason": finish_reason_fallback
                            }]
                        }
                        yield f"data: {json.dumps(finish_chunk)}\n\n"
                        state["chunks_sent"] += 1
                        state["finish_reason_sent"] = True
                        state["has_valid_content"] = True
                        return  # 成功 fallback，结束流
                    else:
                        log.warning("[ANTIGRAVITY STREAM] Non-streaming fallback returned empty response. "
                                   "This may indicate: 1) Context too long for any request type, "
                                   "2) API capacity exhausted, 3) Content safety filter triggered.")
                        # 标记 fallback 已尝试但失败
                        state["fallback_result"] = "empty_response"
                except Exception as fallback_e:
                    log.error(f"[ANTIGRAVITY STREAM] Non-streaming fallback failed: {fallback_e}")
                    state["fallback_result"] = f"error: {str(fallback_e)[:100]}"
                    # Fallback 失败，继续执行错误消息逻辑

            # 从 context_info 获取上下文信息（如果可用）
            # [FIX] Moved above - estimated_tokens already extracted before fallback condition
            tool_result_count = context_info.get("tool_result_count", 0) if context_info else 0
            total_tool_results_length = context_info.get("total_tool_results_length", 0) if context_info else 0

            # ✅ 简化错误消息：简短、明确、可操作
            # 长篇大论的错误消息效果差，因为 AI agent 可能会尝试"处理"这个消息而不是触发 summarize
            token_info = f"{prompt_token_count:,}" if prompt_token_count > 0 else f"~{estimated_tokens:,}"

            # 简短的错误消息，关键是让 Cursor 知道需要 summarize
            error_msg = (
                f"[Context limit exceeded: {token_info} tokens] "
                f"Please use /summarize or compact the conversation to continue."
            )

            error_chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {"content": error_msg},
                    "finish_reason": None
                }]
            }
            yield f"data: {json.dumps(error_chunk)}\n\n"
            state["chunks_sent"] += 1

            # ✅ 修复：使用 finish_reason: "length" 来触发 Cursor 的 summarize 机制
            # 当上下文过长导致空响应时，返回 "length" 而不是 "stop"
            # 这样 Cursor 会识别为"输出被截断"，可能触发自动 summarize
            context_exceeded_finish_reason = "length"  # 关键修复！

            finish_chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {},
                    "finish_reason": context_exceeded_finish_reason
                }],
                # ✅ 新增：返回准确的 usage 信息，帮助 Cursor 了解上下文大小
                "usage": {
                    "prompt_tokens": prompt_token_count if prompt_token_count > 0 else estimated_tokens,
                    "completion_tokens": 0,
                    "total_tokens": prompt_token_count if prompt_token_count > 0 else estimated_tokens
                }
            }
            yield f"data: {json.dumps(finish_chunk)}\n\n"
            state["chunks_sent"] += 1
            state["finish_reason_sent"] = True

            log.warning(f"[ANTIGRAVITY STREAM] Context exceeded - sent finish_reason='length' to trigger Cursor summarize. "
                       f"prompt_tokens={prompt_token_count}, actual_processed={actual_processed_tokens}")

        # 检查是否已发送 finish_reason
        # 这是最后的保底逻辑，确保 Cursor 总是收到 finish_reason
        if not state.get("finish_reason_sent", False):
            log.warning(f"[ANTIGRAVITY STREAM] finish_reason not sent yet, sending now (final fallback)")
            # 确定 finish_reason
            final_finish_reason = "tool_calls" if state["tool_calls"] else "stop"
            finish_chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {},
                    "finish_reason": final_finish_reason
                }]
            }
            yield f"data: {json.dumps(finish_chunk)}\n\n"
            state["finish_reason_sent"] = True
            state["chunks_sent"] += 1

        # [OBSERVABILITY] Thinking 退化状态检测和警告
        if state["thinking_parts_count"] > 0:
            avg_thinking_chars = state["total_thinking_chars"] / state["thinking_parts_count"]
            short_ratio = state["short_thinking_parts"] / state["thinking_parts_count"]

            # 退化状态判断条件：
            # 1. 平均每个 thinking part 少于 100 字符（正常推理应该更长）
            # 2. 超过 50% 的 thinking parts 是短句（<50字符）
            is_degraded = avg_thinking_chars < 100 or short_ratio > 0.5

            if is_degraded:
                log.warning(f"[THINKING DEGRADATION] Possible thinking degradation detected! "
                           f"thinking_parts={state['thinking_parts_count']}, "
                           f"total_chars={state['total_thinking_chars']}, "
                           f"avg_chars={avg_thinking_chars:.1f}, "
                           f"short_parts={state['short_thinking_parts']} ({short_ratio*100:.1f}%). "
                           f"Model may be producing pseudo-thinking instead of real reasoning. "
                           f"Consider using Sequential Thinking MCP tool for complex tasks.")
            else:
                log.info(f"[THINKING OBSERVABILITY] Thinking stats: "
                        f"parts={state['thinking_parts_count']}, "
                        f"total_chars={state['total_thinking_chars']}, "
                        f"avg_chars={avg_thinking_chars:.1f}, "
                        f"short_parts={state['short_thinking_parts']} ({short_ratio*100:.1f}%)")

        # 发送结束标记
        log.info(f"[ANTIGRAVITY STREAM] Stream ending. SSE lines: {state['sse_lines_received']}, Chunks sent: {state['chunks_sent']}, Content buffer: {len(state['content_buffer'])}, Tool calls: {len(state['tool_calls'])}, has_valid_content: {state.get('has_valid_content', False)}, empty_parts_count: {state.get('empty_parts_count', 0)}, finish_reason_sent: {state.get('finish_reason_sent', False)}")
        yield "data: [DONE]\n\n"

    except Exception as e:
        log.error(f"[ANTIGRAVITY] Streaming error: {e}")

        # ✅ 构建包含工具提示的错误响应
        error_msg_parts = []
        error_msg_parts.append(f"Streaming error: {str(e)}")
        error_msg_parts.append("")
        error_msg_parts.append("⚠️ **Tool Call Format Reminder**:")
        error_msg_parts.append("If you encounter 'invalid arguments' errors when calling tools:")
        error_msg_parts.append("- Use EXACT parameter names from the current tool schema")
        error_msg_parts.append("- Do NOT use parameters from previous conversations")
        error_msg_parts.append("- For terminal/command tools: verify parameter name (may be `command`, `input`, or `cmd`)")

        error_response = {
            "error": {
                "message": "\n".join(error_msg_parts),
                "type": "api_error",
                "code": 500
            }
        }
        yield f"data: {json.dumps(error_response)}\n\n"
    finally:
        # 确保清理所有资源
        try:
            await stream_ctx.__aexit__(None, None, None)
        except Exception as e:
            log.debug(f"[ANTIGRAVITY] Error closing stream context: {e}")
        try:
            await client.aclose()
        except Exception as e:
            log.debug(f"[ANTIGRAVITY] Error closing client: {e}")


def convert_antigravity_response_to_openai(
    response_data: Dict[str, Any],
    model: str,
    request_id: str
) -> Dict[str, Any]:
    """
    将 Antigravity 非流式响应转换为 OpenAI 格式
    """
    # 提取 parts
    parts = response_data.get("response", {}).get("candidates", [{}])[0].get("content", {}).get("parts", [])

    content = ""
    thinking_content = ""
    tool_calls_list = []

    for part in parts:
        # 处理思考内容
        if part.get("thought") is True:
            thinking_content += part.get("text", "")

        # 处理图片数据 (inlineData)
        elif "inlineData" in part:
            inline_data = part["inlineData"]
            mime_type = inline_data.get("mimeType", "image/png")
            base64_data = inline_data.get("data", "")
            # 转换为 Markdown 格式的图片
            content += f"\n\n![生成的图片](data:{mime_type};base64,{base64_data})\n\n"

        # 处理普通文本
        elif "text" in part:
            content += part.get("text", "")

        # 处理工具调用
        elif "functionCall" in part:
            fc = part["functionCall"]
            
            # ✅ 新增：验证工具调用
            is_valid, validation_msg, fixed_fc = validate_tool_call(fc)
            if not is_valid:
                log.warning(f"[ANTIGRAVITY] Invalid tool call in non-streaming response: {validation_msg}. Original: {fc}")
                # 跳过无效的工具调用
                continue
            
            # 使用修复后的工具调用（如果有修复）
            if fixed_fc:
                fc = fixed_fc
            
            tool_index = len(tool_calls_list)
            tool_calls_list.append(convert_to_openai_tool_call(fc, index=tool_index))

    # 拼接思考内容
    if thinking_content:
        content = f"<think>\n{thinking_content}\n</think>\n{content}"

    # 使用 OpenAIChatMessage 模型构建消息
    message = OpenAIChatMessage(
        role="assistant",
        content=content,
        tool_calls=tool_calls_list if tool_calls_list else None
    )

    # 确定 finish_reason
    finish_reason = "stop"
    if tool_calls_list:
        finish_reason = "tool_calls"

    finish_reason_raw = response_data.get("response", {}).get("candidates", [{}])[0].get("finishReason")
    if finish_reason_raw == "MAX_TOKENS":
        finish_reason = "length"

    # 提取使用统计
    usage_metadata = response_data.get("response", {}).get("usageMetadata", {})
    usage = {
        "prompt_tokens": usage_metadata.get("promptTokenCount", 0),
        "completion_tokens": usage_metadata.get("candidatesTokenCount", 0),
        "total_tokens": usage_metadata.get("totalTokenCount", 0)
    }

    # 使用 OpenAIChatCompletionChoice 模型
    choice = OpenAIChatCompletionChoice(
        index=0,
        message=message,
        finish_reason=finish_reason
    )

    # 使用 OpenAIChatCompletionResponse 模型
    response = OpenAIChatCompletionResponse(
        id=request_id,
        object="chat.completion",
        created=int(time.time()),
        model=model,
        choices=[choice],
        usage=usage
    )

    return model_to_dict(response)


def convert_antigravity_response_to_gemini(
    response_data: Dict[str, Any]
) -> Dict[str, Any]:
    """
    将 Antigravity 非流式响应转换为 Gemini 格式
    Antigravity 的响应格式与 Gemini 非常相似，只需要提取 response 字段
    """
    # Antigravity 响应格式: {"response": {...}}
    # Gemini 响应格式: {...}
    return response_data.get("response", response_data)


async def convert_antigravity_stream_to_gemini(
    lines_generator: Any,
    stream_ctx: Any,
    client: Any,
    credential_manager: Any,
    credential_name: str
):
    """
    将 Antigravity 流式响应转换为 Gemini 格式的 SSE 流

    Args:
        lines_generator: 行生成器 (已经过滤的 SSE 行)
    """
    success_recorded = False

    try:
        async for line in lines_generator:
            if not line or not line.startswith("data: "):
                continue

            # 记录第一次成功响应
            if not success_recorded:
                if credential_name and credential_manager:
                    await credential_manager.record_api_call_result(credential_name, True, is_antigravity=True)
                success_recorded = True

            # 解析 SSE 数据
            try:
                data = json.loads(line[6:])  # 去掉 "data: " 前缀
            except:
                continue

            # Antigravity 流式响应格式: {"response": {...}}
            # Gemini 流式响应格式: {...}
            gemini_data = data.get("response", data)

            # 发送 Gemini 格式的数据
            yield f"data: {json.dumps(gemini_data)}\n\n"

    except Exception as e:
        log.error(f"[ANTIGRAVITY GEMINI] Streaming error: {e}")
        error_response = {
            "error": {
                "message": str(e),
                "code": 500,
                "status": "INTERNAL"
            }
        }
        yield f"data: {json.dumps(error_response)}\n\n"
    finally:
        # 确保清理所有资源
        try:
            await stream_ctx.__aexit__(None, None, None)
        except Exception as e:
            log.debug(f"[ANTIGRAVITY GEMINI] Error closing stream context: {e}")
        try:
            await client.aclose()
        except Exception as e:
            log.debug(f"[ANTIGRAVITY GEMINI] Error closing client: {e}")


@router.get("/antigravity/v1/models", response_model=ModelList)
async def list_models():
    """返回 OpenAI 格式的模型列表 - 动态从 Antigravity API 获取"""

    try:
        # 获取凭证管理器
        cred_mgr = await get_credential_manager()

        # 从 Antigravity API 获取模型列表（返回 OpenAI 格式的字典列表）
        models = await fetch_available_models(cred_mgr)

        if not models:
            # 如果获取失败，直接返回空列表
            log.warning("[ANTIGRAVITY] Failed to fetch models from API, returning empty list")
            return ModelList(data=[])

        # models 已经是 OpenAI 格式的字典列表，扩展为包含抗截断版本
        expanded_models = []
        for model in models:
            # 添加原始模型
            expanded_models.append(Model(**model))

            # 添加流式抗截断版本
            anti_truncation_model = model.copy()
            anti_truncation_model["id"] = f"流式抗截断/{model['id']}"
            expanded_models.append(Model(**anti_truncation_model))

        return ModelList(data=expanded_models)

    except Exception as e:
        log.error(f"[ANTIGRAVITY] Error fetching models: {e}")
        # 返回空列表
        return ModelList(data=[])


@router.post("/antigravity/v1/chat/completions")
async def chat_completions(
    request: Request,
    token: str = Depends(authenticate_bearer)
):
    """
    处理 OpenAI 格式的聊天完成请求，转换为 Antigravity API
    """
    # ✅ 使用统一的 User-Agent 检测逻辑（来自 tool_cleaner.py）
    user_agent = request.headers.get("user-agent", "")
    client_info = get_client_info(user_agent)
    client_type = client_info["type"]
    enable_cross_pool_fallback = client_info["enable_cross_pool_fallback"]

    log.info(f"[ANTIGRAVITY] Request from {client_info['name']} ({client_type}), "
             f"User-Agent: {user_agent[:100]}{'...' if len(user_agent) > 100 else ''}")

    if enable_cross_pool_fallback:
        log.info(f"[ANTIGRAVITY] Cross-pool fallback ENABLED for {client_info['name']}")
    else:
        log.debug(f"[ANTIGRAVITY] Cross-pool fallback DISABLED for {client_info['name']}")

    # 获取原始请求数据
    try:
        raw_data = await request.json()
    except Exception as e:
        log.error(f"Failed to parse JSON request: {e}")
        raise HTTPException(status_code=400, detail=f"Invalid JSON: {str(e)}")

    # 清理工具格式：确保所有工具都有正确的结构
    # 网关应该已经规范化了工具，但为了安全起见，我们再次清理和验证
    if "tools" in raw_data and raw_data["tools"]:
        cleaned_tools = []
        for tool in raw_data["tools"]:
            if isinstance(tool, dict):
                # 检查工具是否有正确的结构
                has_type = "type" in tool
                has_function = "function" in tool
                has_name = "name" in tool
                
                # Case 1: 标准格式 - 已经有 type 和 function
                if has_type and has_function:
                    cleaned_tools.append(tool)
                # Case 2: Custom 类型 - 需要转换
                elif tool.get("type") == "custom":
                    custom_tool = tool.get("custom", {})
                    if isinstance(custom_tool, dict) and "name" in custom_tool:
                        from src.anthropic_converter import clean_json_schema
                        input_schema = custom_tool.get("input_schema", {}) or {}
                        if isinstance(input_schema, dict):
                            cleaned_schema = clean_json_schema(input_schema)
                            if "type" not in cleaned_schema:
                                cleaned_schema["type"] = "object"
                            input_schema = cleaned_schema
                        elif not input_schema:
                            input_schema = {"type": "object", "properties": {}}
                        
                        cleaned_tools.append({
                            "type": "function",
                            "function": {
                                "name": custom_tool.get("name", ""),
                                "description": custom_tool.get("description", ""),
                                "parameters": input_schema
                            }
                        })
                        log.warning(f"[ANTIGRAVITY] Converted custom tool '{custom_tool.get('name')}' to function format (should have been normalized by gateway)")
                    else:
                        log.warning(f"[ANTIGRAVITY] Skipping invalid custom tool: {list(custom_tool.keys()) if isinstance(custom_tool, dict) else 'not a dict'}")
                # Case 3: 扁平格式 - 只有 name，没有 type 和 function 包装
                # Cursor 可能使用 input_schema 而不是 parameters
                elif has_name and not has_function:
                    # 优先使用 parameters，如果没有则使用 input_schema（Cursor 可能使用 input_schema）
                    parameters = tool.get("parameters")
                    if parameters is None:
                        # Cursor 可能使用 input_schema 而不是 parameters
                        input_schema = tool.get("input_schema")
                        if input_schema is not None:
                            # 使用 clean_json_schema 清理 input_schema
                            from src.anthropic_converter import clean_json_schema
                            if isinstance(input_schema, dict):
                                parameters = clean_json_schema(input_schema)
                                # 确保有 type 字段
                                if "type" not in parameters:
                                    parameters["type"] = "object"
                            else:
                                log.warning(f"[ANTIGRAVITY] Tool '{tool.get('name')}' has non-dict input_schema: {type(input_schema)}, converting to empty dict")
                                parameters = {}
                        else:
                            parameters = {}
                    
                    # 确保 parameters 是字典
                    if not isinstance(parameters, dict):
                        log.warning(f"[ANTIGRAVITY] Tool '{tool.get('name')}' has non-dict parameters: {type(parameters)}, converting to dict")
                        parameters = {}
                    
                    cleaned_tools.append({
                        "type": "function",
                        "function": {
                            "name": tool.get("name", ""),
                            "description": tool.get("description", ""),
                            "parameters": parameters
                        }
                    })
                    log.warning(f"[ANTIGRAVITY] Converted flat format tool '{tool.get('name')}' to standard format (should have been normalized by gateway). Tool keys: {list(tool.keys())}, used_input_schema={'input_schema' in tool}")
                # Case 4: 其他格式 - 尝试修复或跳过
                else:
                    log.warning(f"[ANTIGRAVITY] Unknown tool format: type={tool.get('type')}, has_function={has_function}, has_name={has_name}, keys={list(tool.keys())}")
                    # 尝试修复：如果有 name，就包装成标准格式
                    if has_name:
                        # 优先使用 parameters，如果没有则使用 input_schema
                        parameters = tool.get("parameters")
                        if parameters is None:
                            input_schema = tool.get("input_schema")
                            if input_schema is not None:
                                from src.anthropic_converter import clean_json_schema
                                if isinstance(input_schema, dict):
                                    parameters = clean_json_schema(input_schema)
                                    if "type" not in parameters:
                                        parameters["type"] = "object"
                                else:
                                    parameters = {}
                            else:
                                parameters = {}
                        
                        if not isinstance(parameters, dict):
                            parameters = {}
                        
                        cleaned_tools.append({
                            "type": "function",
                            "function": {
                                "name": tool.get("name", ""),
                                "description": tool.get("description", ""),
                                "parameters": parameters
                            }
                        })
                    else:
                        log.warning(f"[ANTIGRAVITY] Skipping tool without name field")
            else:
                # 非字典类型，直接使用（可能是 Pydantic 模型，稍后处理）
                cleaned_tools.append(tool)
        raw_data["tools"] = cleaned_tools

    # 创建请求对象
    try:
        request_data = ChatCompletionRequest(**raw_data)
    except Exception as e:
        log.error(f"Request validation failed: {e}")
        raise HTTPException(status_code=400, detail=f"Request validation error: {str(e)}")

    # 健康检查
    if (
        len(request_data.messages) == 1
        and getattr(request_data.messages[0], "role", None) == "user"
        and getattr(request_data.messages[0], "content", None) == "Hi"
    ):
        return JSONResponse(
            content={
                "choices": [{"message": {"role": "assistant", "content": "antigravity API 正常工作中"}}]
            }
        )

    # 获取凭证管理器
    from src.credential_manager import get_credential_manager
    cred_mgr = await get_credential_manager()

    # 提取参数
    model = request_data.model
    messages = request_data.messages
    stream = getattr(request_data, "stream", False)
    tools = getattr(request_data, "tools", None)

    # DEBUG: Log tools format received by Antigravity router
    if tools:
        # 将 Pydantic 模型转换为字典以便检查
        tool_dicts = []
        for tool in tools:
            if not isinstance(tool, dict):
                if hasattr(tool, "model_dump"):
                    tool_dicts.append(tool.model_dump())
                elif hasattr(tool, "dict"):
                    tool_dicts.append(tool.dict())
                else:
                    # 尝试使用 getattr 获取属性
                    tool_dict = {}
                    for attr in ["type", "function", "custom"]:
                        if hasattr(tool, attr):
                            value = getattr(tool, attr)
                            if hasattr(value, "model_dump"):
                                tool_dict[attr] = value.model_dump()
                            elif hasattr(value, "dict"):
                                tool_dict[attr] = value.dict()
                            else:
                                tool_dict[attr] = value
                    tool_dicts.append(tool_dict)
            else:
                tool_dicts.append(tool)
        
        tool_types = [tool.get("type", "unknown") for tool in tool_dicts]
        log.info(f"[ANTIGRAVITY] Received {len(tools)} tools, types={tool_types[:10]}... (showing first 10)")
        
        # 检查是否有 custom 格式的工具
        has_custom = any(tool.get("type") == "custom" for tool in tool_dicts)
        if has_custom:
            log.warning(f"[ANTIGRAVITY] WARNING: Received custom tool format! This should have been normalized by gateway.")
            first_custom = next((tool for tool in tool_dicts if tool.get("type") == "custom"), None)
            if first_custom:
                custom_tool = first_custom.get("custom", {})
                input_schema = custom_tool.get("input_schema", {})
                schema_type = input_schema.get("type") if isinstance(input_schema, dict) else type(input_schema).__name__
                log.warning(f"[ANTIGRAVITY] First custom tool: name={custom_tool.get('name')}, input_schema_type={schema_type}, has_properties={'properties' in input_schema if isinstance(input_schema, dict) else False}")
        
        # 将工具转换为字典格式（如果还不是字典）
        if tool_dicts != tools:
            tools = tool_dicts

    # 检测并处理抗截断模式
    use_anti_truncation = is_anti_truncation_model(model)
    if use_anti_truncation:
        # 去掉 "流式抗截断/" 前缀
        from src.utils import get_base_model_from_feature_model
        model = get_base_model_from_feature_model(model)

    # 模型名称映射
    actual_model = model_mapping(model)
    enable_thinking = is_thinking_model(model)

    # 检查历史消息中是否有有效的 thinking block（包含 signature）
    # 如果 thinking 启用但历史消息没有有效的 thinking block，则禁用 thinking
    # 这可以避免 400 错误："thinking.signature: Field required"
    if enable_thinking:
        has_valid_thinking = False
        for msg in messages:
            # 检查消息是否有 thinking 内容
            content = getattr(msg, "content", None) if hasattr(msg, "content") else (msg.get("content") if isinstance(msg, dict) else None)
            
            if content:
                # 检查字符串格式的 content
                if isinstance(content, str):
                    import re
                    # 检查是否有 thinking 标签（但无法验证 signature）
                    if re.search(r'<(?:redacted_)?reasoning>.*?</(?:redacted_)?reasoning>', content, flags=re.DOTALL | re.IGNORECASE):
                        # 有 thinking 标签，但无法验证 signature，假设有效
                        has_valid_thinking = True
                        break
                # 检查数组格式的 content
                elif isinstance(content, list):
                    for item in content:
                        if isinstance(item, dict):
                            item_type = item.get("type")
                            if item_type in ("thinking", "redacted_thinking"):
                                # 检查是否有 signature
                                signature = item.get("signature")
                                if signature and signature.strip():
                                    has_valid_thinking = True
                                    break
                    if has_valid_thinking:
                        break
        
        if not has_valid_thinking:
            log.warning(f"[ANTIGRAVITY] Thinking 已启用，但历史消息中没有有效的 thinking block（包含 signature），禁用 thinking 模式以避免 400 错误")
            enable_thinking = False

    log.info(f"[ANTIGRAVITY] Request: model={model} -> {actual_model}, stream={stream}, thinking={enable_thinking}, anti_truncation={use_anti_truncation}")

    # 如果 thinking 被禁用，清理消息中的 thinking 内容块
    # 这可以避免 400 错误："When thinking is disabled, an `assistant` message..."
    if not enable_thinking:
        original_count = len(messages)
        # 检查是否有 thinking 内容需要清理
        has_thinking_before = any(
            (hasattr(msg, "role") and getattr(msg, "role", None) == "assistant" and 
             hasattr(msg, "content") and isinstance(getattr(msg, "content", ""), str) and 
             ("<think>" in getattr(msg, "content", "").lower() or 
              "<think>" in getattr(msg, "content", "").lower() or
              "<think>" in getattr(msg, "content", "").lower()))
            or (isinstance(msg, dict) and msg.get("role") == "assistant" and 
                isinstance(msg.get("content"), str) and 
                ("<think>" in msg.get("content", "").lower() or 
                 "<think>" in msg.get("content", "").lower() or
                 "<think>" in msg.get("content", "").lower()))
            or (isinstance(msg, dict) and msg.get("role") == "assistant" and 
                isinstance(msg.get("content"), list) and
                any(isinstance(item, dict) and item.get("type") in ("thinking", "redacted_thinking") 
                    for item in msg.get("content", [])))
            for msg in messages
        )
        if has_thinking_before:
            messages = strip_thinking_from_openai_messages(messages)
            log.info(f"[ANTIGRAVITY] Thinking 已禁用，已清理历史消息中的 thinking 内容块 (messages={original_count})")

    # 转换消息格式
    try:
        contents = openai_messages_to_antigravity_contents(messages, enable_thinking=enable_thinking, tools=tools)
        
        # 如果 thinking 启用，确保最后一条 assistant 消息以 thinking block 开头
        if enable_thinking and contents:
            # 找到最后一条 assistant 消息（role="model"）
            last_model_index = -1
            for i in range(len(contents) - 1, -1, -1):
                if contents[i].get("role") == "model":
                    last_model_index = i
                    break
            
            if last_model_index >= 0:
                last_model = contents[last_model_index]
                parts = last_model.get("parts", [])
                if parts:
                    first_part = parts[0]
                    # 检查第一个 part 是否是 thinking block
                    if not isinstance(first_part, dict) or first_part.get("thought") is not True:
                        # 没有 thinking block，需要添加一个
                        # 尝试从之前的消息中提取 thinking block
                        thinking_part = None
                        for i in range(last_model_index - 1, -1, -1):
                            prev_msg = contents[i]
                            if prev_msg.get("role") == "model":
                                prev_parts = prev_msg.get("parts", [])
                                for part in prev_parts:
                                    if isinstance(part, dict) and part.get("thought") is True:
                                        # 检查 signature 是否有效（非空）
                                        signature = part.get("thoughtSignature", "")
                                        if signature and signature.strip():
                                            # 找到有效的 thinking block，复制它
                                            thinking_part = {
                                                "text": part.get("text", ""),
                                                "thought": True,
                                                "thoughtSignature": signature
                                            }
                                            break
                                if thinking_part:
                                    break
                        
                        if thinking_part:
                            # 在开头插入 thinking block
                            parts.insert(0, thinking_part)
                            log.info(f"[ANTIGRAVITY] Added thinking block to last assistant message from previous message")
                        else:
                            # 无法从之前的消息中提取有效的 thinking block（包含 signature）
                            # 根据之前的检查，如果历史消息没有有效的 thinking block，enable_thinking 应该已经被禁用
                            # 但为了安全起见，如果确实没有，禁用 thinking 并清理消息
                            log.warning(f"[ANTIGRAVITY] Last assistant message does not start with thinking block, cannot find previous thinking block with valid signature. Disabling thinking mode.")
                            enable_thinking = False
                            # 清理消息中的 thinking 内容块
                            messages = strip_thinking_from_openai_messages(messages)
                            # 重新转换消息（不使用 thinking）
                            contents = openai_messages_to_antigravity_contents(messages, enable_thinking=False, tools=tools)
                            # 注意：这里不需要 break，因为我们已经重新转换了消息
                            # 后续代码会使用新的 contents
                    # 更新 parts（如果有修改）
                    if thinking_part:
                        last_model["parts"] = parts
    except Exception as e:
        log.error(f"Failed to convert messages: {e}")
        raise HTTPException(status_code=500, detail=f"Message conversion failed: {str(e)}")

    # ✅ 新增：验证 contents 是否为空（根本原因修复）
    # 参考：antigravity_anthropic_router.py:570 和 gemini_generate_content 的验证逻辑
    if not contents:
        log.error(f"[ANTIGRAVITY] Contents is empty after conversion! Messages count: {len(messages)}")
        # 记录原始消息，帮助排查问题
        for i, msg in enumerate(messages[:5]):  # 只记录前5条
            role = getattr(msg, "role", None) if hasattr(msg, "role") else (msg.get("role") if isinstance(msg, dict) else None)
            content_val = getattr(msg, "content", None) if hasattr(msg, "content") else (msg.get("content") if isinstance(msg, dict) else None)
            content_len = len(str(content_val)) if content_val else 0
            log.error(f"[ANTIGRAVITY] Message {i}: role={role}, content_type={type(content_val).__name__}, content_length={content_len}")

        raise HTTPException(
            status_code=400,
            detail="Messages converted to empty contents. Please ensure messages contain valid user content (non-empty text or images). This usually happens when: 1) Only system messages are provided, 2) User message content is empty, 3) Message format is invalid."
        )

    # 验证每个 content 的 parts 不为空
    for i, content_item in enumerate(contents):
        parts = content_item.get("parts", [])
        if not parts:
            log.error(f"[ANTIGRAVITY] Content {i} (role={content_item.get('role')}) has empty parts!")
            raise HTTPException(
                status_code=400,
                detail=f"Content at index {i} (role={content_item.get('role')}) has empty parts. Please ensure all messages contain valid content."
            )

    # 转换工具定义
    antigravity_tools = convert_openai_tools_to_antigravity(tools)

    # 生成配置参数
    parameters = {
        "temperature": getattr(request_data, "temperature", None),
        "top_p": getattr(request_data, "top_p", None),
        "max_tokens": getattr(request_data, "max_tokens", None),
    }
    # 过滤 None 值
    parameters = {k: v for k, v in parameters.items() if v is not None}

    # 使用更新后的 enable_thinking（可能已被禁用）
    generation_config = generate_generation_config(parameters, enable_thinking, actual_model)

    # 获取凭证信息（用于 project_id 和 session_id）
    cred_result = await cred_mgr.get_valid_credential(is_antigravity=True)
    if not cred_result:
        log.error("当前无可用 antigravity 凭证")
        raise HTTPException(status_code=500, detail="当前无可用 antigravity 凭证")

    _, credential_data = cred_result
    project_id = credential_data.get("project_id", "default-project")
    session_id = f"session-{uuid.uuid4().hex}"

    # ✅ 新增：估算输入 token 数（粗略估算，用于检测上下文过长）
    def estimate_input_tokens(contents: List[Dict[str, Any]]) -> int:
        """粗略估算输入 token 数"""
        total_chars = 0
        for content in contents:
            parts = content.get("parts", [])
            for part in parts:
                if "text" in part:
                    total_chars += len(str(part["text"]))
                elif "functionResponse" in part:
                    # 工具结果可能很大
                    response = part.get("functionResponse", {}).get("response", {})
                    output = response.get("output", "")
                    total_chars += len(str(output))
        # 粗略估算：1 token ≈ 4 字符
        return total_chars // 4
    
    estimated_tokens = estimate_input_tokens(contents)
    
    # ✅ 新增：上下文长度阈值配置
    # 这些阈值用于主动检测和拒绝过长的上下文，避免 API 返回空响应
    # Cursor 用户可以使用 /summarize 命令来压缩对话历史
    CONTEXT_WARNING_THRESHOLD = 80000   # 80K tokens - 警告阈值，记录日志
    CONTEXT_CRITICAL_THRESHOLD = 120000  # 120K tokens - 拒绝阈值，返回错误
    
    # ✅ 主动拒绝过长的上下文请求
    # 这样可以避免 API 返回空响应，让用户立即知道需要压缩上下文
    if estimated_tokens > CONTEXT_CRITICAL_THRESHOLD:
        log.error(f"[ANTIGRAVITY] Context too long: ~{estimated_tokens:,} tokens exceeds critical threshold ({CONTEXT_CRITICAL_THRESHOLD:,})")
        raise HTTPException(
            status_code=400,
            detail=(
                f"Context length limit exceeded. Your conversation has approximately {estimated_tokens:,} tokens, "
                f"which exceeds the limit of {CONTEXT_CRITICAL_THRESHOLD:,} tokens.\n\n"
                f"To resolve this issue:\n"
                f"1. Use the /summarize command in Cursor to compress the conversation history\n"
                f"2. Or start a new chat session\n"
                f"3. Or reduce the number of files and tool results in context\n\n"
                f"This limit exists to prevent API errors and ensure reliable responses."
            )
        )
    elif estimated_tokens > CONTEXT_WARNING_THRESHOLD:
        log.warning(f"[ANTIGRAVITY] Context approaching limit: ~{estimated_tokens:,} tokens (warning threshold: {CONTEXT_WARNING_THRESHOLD:,}). "
                   f"Consider using /summarize command to compress conversation history.")
    elif estimated_tokens > 50000:  # 50K tokens 阈值 - 保留原有的警告
        log.warning(f"[ANTIGRAVITY] Large input context detected: ~{estimated_tokens} tokens. "
                   f"This may cause API to return empty response or stream truncation. "
                   f"Consider: 1) Reducing tool result size, 2) Using non-streaming request, 3) Splitting context")

    # 构建 Antigravity 请求体
    request_body = build_antigravity_request_body(
        contents=contents,
        model=actual_model,
        project_id=project_id,
        session_id=session_id,
        tools=antigravity_tools,
        generation_config=generation_config,
    )

    # ✅ 新增：增强诊断日志（特别是工具调用场景）
    log.info(f"[ANTIGRAVITY DEBUG] Request summary: contents_count={len(contents)}, "
             f"has_tools={bool(antigravity_tools)}, model={actual_model}, "
             f"enable_thinking={enable_thinking}")
    
    # ✅ 新增：检查是否是工具调用后的请求
    has_tool_messages = any(
        content.get("role") == "user" and any(
            part.get("functionResponse") for part in content.get("parts", [])
        )
        for content in contents
    )
    # ✅ 新增：统计工具结果信息，用于传递给错误消息
    tool_result_count = 0
    total_tool_results_length = 0
    if has_tool_messages:
        log.info(f"[ANTIGRAVITY DEBUG] This is a tool-call-follow-up request")
        # 记录工具消息的详细信息并统计
        for i, content in enumerate(contents):
            if content.get("role") == "user":
                parts = content.get("parts", [])
                for j, part in enumerate(parts):
                    if "functionResponse" in part:
                        fr = part["functionResponse"]
                        output = str(fr.get("response", {}).get("output", ""))
                        tool_result_count += 1
                        total_tool_results_length += len(output)
                        log.info(f"[ANTIGRAVITY DEBUG] Tool result {i}-{j}: "
                               f"id={fr.get('id')}, name={fr.get('name')}, "
                               f"output_type={type(fr.get('response', {}).get('output')).__name__}, "
                               f"output_length={len(output)}")
    
    # ✅ 新增：构建上下文信息，用于传递给错误消息
    context_info = {
        "estimated_tokens": estimated_tokens,
        "tool_result_count": tool_result_count,
        "total_tool_results_length": total_tool_results_length,
        "has_tool_messages": has_tool_messages
    }

    # 生成请求 ID
    request_id = f"chatcmpl-{int(time.time() * 1000)}"

    # ✅ 优化：检测上下文长度，如果过长且是流式请求，考虑使用非流式 fallback
    # 注意：这里使用 estimated_tokens 作为初步判断，实际处理的 tokens 可能在流式请求中动态检测
    # 非流式请求通常对长上下文更稳定，但会失去实时反馈的优势
    USE_NON_STREAMING_FALLBACK_THRESHOLD = 60000  # 60K tokens 阈值（基于 estimated_tokens）
    # 注意：实际处理的 tokens 阈值（50K）在流式转换器中动态检测
    should_use_non_streaming_fallback = (
        stream and
        estimated_tokens > USE_NON_STREAMING_FALLBACK_THRESHOLD and
        has_tool_messages  # 只在工具调用场景下启用 fallback
    )

    if should_use_non_streaming_fallback:
        log.warning(f"[ANTIGRAVITY] Large estimated context ({estimated_tokens:,} tokens) with tool calls detected. "
                   f"Using non-streaming request as fallback for better stability. "
                   f"Note: If API caches content, actual processed tokens may be lower. "
                   f"Streaming request will dynamically check actual processed tokens and fallback if needed.")
        # 可以选择直接切换到非流式，或者让流式请求自己处理
        # 为了保持实时反馈，我们让流式请求自己处理，只在极端情况下才直接切换
        # stream = False  # 取消直接切换，让流式请求动态处理

    # ====================== 带模型降级的请求发送 ======================
    # 获取当前模型的降级链
    fallback_models = get_fallback_models(actual_model)
    models_to_try = [actual_model] + fallback_models

    # 发送请求（带模型降级）
    last_error = None
    for attempt_idx, attempt_model in enumerate(models_to_try):
        try:
            # 如果不是第一个模型，更新请求体中的模型
            if attempt_idx > 0:
                log.warning(f"[ANTIGRAVITY FALLBACK] 模型降级: {actual_model} -> {attempt_model} (尝试 {attempt_idx + 1}/{len(models_to_try)})")
                request_body["model"] = attempt_model

            if stream:
                # 处理抗截断功能（仅流式传输时有效）
                if use_anti_truncation:
                    log.info("[ANTIGRAVITY] 启用流式抗截断功能")
                    max_attempts = await get_anti_truncation_max_attempts()

                    # 包装请求函数以适配抗截断处理器
                    async def antigravity_request_func(payload):
                        resources, cred_name, cred_data = await send_antigravity_request_stream(
                            payload, cred_mgr, enable_cross_pool_fallback=enable_cross_pool_fallback
                        )
                        response, stream_ctx, client = resources
                        return StreamingResponse(
                            convert_antigravity_stream_to_openai(
                                response, stream_ctx, client, model, request_id, cred_mgr, cred_name,
                                request_body=request_body,  # 传递请求体用于 fallback
                                cred_mgr=cred_mgr,  # 传递凭证管理器用于 fallback
                                context_info=context_info  # ✅ 新增：传递上下文信息用于错误消息
                            ),
                            media_type="text/event-stream"
                        )

                    return await apply_anti_truncation_to_stream(
                        antigravity_request_func, request_body, max_attempts
                    )

                # 流式请求（无抗截断）
                resources, cred_name, cred_data = await send_antigravity_request_stream(
                    request_body, cred_mgr, enable_cross_pool_fallback=enable_cross_pool_fallback
                )
                # resources 是一个元组: (response, stream_ctx, client)
                response, stream_ctx, client = resources

                # 转换并返回流式响应,传递资源管理对象
                # response 现在是 filtered_lines 生成器
                # ✅ 新增：传递请求体和凭证管理器用于 fallback，以及上下文信息用于错误消息
                return StreamingResponse(
                    convert_antigravity_stream_to_openai(
                        response, stream_ctx, client, model, request_id, cred_mgr, cred_name,
                        request_body=request_body,  # 传递请求体用于 fallback
                        cred_mgr=cred_mgr,  # 传递凭证管理器用于 fallback
                        context_info=context_info  # ✅ 新增：传递上下文信息用于错误消息
                    ),
                    media_type="text/event-stream"
                )
            else:
                # 非流式请求
                response_data, cred_name, cred_data = await send_antigravity_request_no_stream(
                    request_body, cred_mgr
                )

                # 转换并返回响应
                openai_response = convert_antigravity_response_to_openai(response_data, model, request_id)
                return JSONResponse(content=openai_response)

        except Exception as e:
            last_error = e
            error_msg = str(e)
            log.error(f"[ANTIGRAVITY] Request failed with model {attempt_model}: {error_msg}")

            # ====================== 使用新的智能降级逻辑 ======================
            from .fallback_manager import (
                is_quota_exhausted_error, is_retryable_error, is_403_error,
                is_credential_unavailable_error,
                get_cross_pool_fallback, is_haiku_model, HAIKU_FALLBACK_TARGET
            )

            # 1. 403 错误 - 需要触发凭证验证（暂时直接失败，后续可添加验证逻辑）
            if is_403_error(error_msg):
                log.warning(f"[ANTIGRAVITY FALLBACK] 检测到 403 错误，需要验证凭证")
                raise HTTPException(status_code=403, detail=f"Credential verification required: {error_msg}")

            # 2. 可重试错误（400, 普通429, 5xx）- 不降级，直接失败让客户端重试
            if is_retryable_error(error_msg):
                log.info(f"[ANTIGRAVITY] 可重试错误，不触发降级，返回错误让客户端重试")
                raise HTTPException(status_code=500, detail=f"Antigravity API request failed (retryable): {error_msg}")

            # 3. 额度用尽错误 - 触发跨池降级
            if is_quota_exhausted_error(error_msg):
                log.warning(f"[ANTIGRAVITY FALLBACK] 检测到额度用尽错误")

                # 检查是否还有降级模型可用
                if attempt_idx < len(models_to_try) - 1:
                    log.warning(f"[ANTIGRAVITY FALLBACK] 将尝试下一个降级模型")
                    continue
                else:
                    # 已经尝试过降级，检查是否可以路由到 Copilot
                    log.error(f"[ANTIGRAVITY FALLBACK] 所有降级模型均已尝试")
                    # TODO: 后续可添加 Copilot 路由逻辑
                    raise HTTPException(
                        status_code=429,
                        detail=f"All quota pools exhausted. Original error: {error_msg}"
                    )

            # 4. 凭证不可用错误 - 返回 503 让 Gateway 路由到 Copilot
            if is_credential_unavailable_error(error_msg):
                log.warning(f"[ANTIGRAVITY] 凭证不可用，返回 503 让 Gateway 路由到备用后端")
                raise HTTPException(status_code=503, detail=f"Antigravity credentials unavailable: {error_msg}")

            # 5. 其他错误 - 直接失败
            log.info(f"[ANTIGRAVITY] 未知错误类型，不触发降级")
            raise HTTPException(status_code=500, detail=f"Antigravity API request failed: {error_msg}")

    # 所有模型都失败了
    log.error(f"[ANTIGRAVITY FALLBACK] 所有模型均失败: {last_error}")
    raise HTTPException(status_code=500, detail=f"Antigravity API request failed (已尝试所有降级模型): {str(last_error)}")


# ==================== Gemini 格式 API 端点 ====================

@router.get("/antigravity/v1beta/models")
@router.get("/antigravity/v1/models")
async def gemini_list_models(api_key: str = Depends(authenticate_gemini_flexible)):
    """返回 Gemini 格式的模型列表 - 动态从 Antigravity API 获取"""

    try:
        # 获取凭证管理器
        cred_mgr = await get_credential_manager()

        # 从 Antigravity API 获取模型列表（返回 OpenAI 格式的字典列表）
        models = await fetch_available_models(cred_mgr)

        if not models:
            # 如果获取失败，返回空列表
            log.warning("[ANTIGRAVITY GEMINI] Failed to fetch models from API, returning empty list")
            return JSONResponse(content={"models": []})

        # 将 OpenAI 格式转换为 Gemini 格式，同时添加抗截断版本
        gemini_models = []
        for model in models:
            model_id = model.get("id", "")

            # 添加原始模型
            gemini_models.append({
                "name": f"models/{model_id}",
                "version": "001",
                "displayName": model_id,
                "description": f"Antigravity API - {model_id}",
                "supportedGenerationMethods": ["generateContent", "streamGenerateContent"],
            })

            # 添加流式抗截断版本
            anti_truncation_id = f"流式抗截断/{model_id}"
            gemini_models.append({
                "name": f"models/{anti_truncation_id}",
                "version": "001",
                "displayName": anti_truncation_id,
                "description": f"Antigravity API - {anti_truncation_id} (带流式抗截断功能)",
                "supportedGenerationMethods": ["generateContent", "streamGenerateContent"],
            })

        return JSONResponse(content={"models": gemini_models})

    except Exception as e:
        log.error(f"[ANTIGRAVITY GEMINI] Error fetching models: {e}")
        # 返回空列表
        return JSONResponse(content={"models": []})


@router.post("/antigravity/v1beta/models/{model:path}:generateContent")
@router.post("/antigravity/v1/models/{model:path}:generateContent")
async def gemini_generate_content(
    model: str = Path(..., description="Model name"),
    request: Request = None,
    api_key: str = Depends(authenticate_gemini_flexible),
):
    """处理 Gemini 格式的非流式内容生成请求（通过 Antigravity API）"""
    log.debug(f"[ANTIGRAVITY GEMINI] Non-streaming request for model: {model}")

    # 获取原始请求数据
    try:
        request_data = await request.json()
    except Exception as e:
        log.error(f"Failed to parse JSON request: {e}")
        raise HTTPException(status_code=400, detail=f"Invalid JSON: {str(e)}")

    # 验证必要字段
    if "contents" not in request_data or not request_data["contents"]:
        raise HTTPException(status_code=400, detail="Missing required field: contents")

    # 健康检查
    if (
        len(request_data["contents"]) == 1
        and request_data["contents"][0].get("role") == "user"
        and request_data["contents"][0].get("parts", [{}])[0].get("text") == "Hi"
    ):
        return JSONResponse(
            content={
                "candidates": [
                    {
                        "content": {"parts": [{"text": "antigravity API 正常工作中"}], "role": "model"},
                        "finishReason": "STOP",
                        "index": 0,
                    }
                ]
            }
        )

    # 获取凭证管理器
    from src.credential_manager import get_credential_manager
    cred_mgr = await get_credential_manager()

    # 提取模型名称（移除 "models/" 前缀）
    if model.startswith("models/"):
        model = model[7:]

    # 检测并处理抗截断模式（虽然非流式不会使用，但要处理模型名）
    use_anti_truncation = is_anti_truncation_model(model)
    if use_anti_truncation:
        # 去掉 "流式抗截断/" 前缀
        from src.utils import get_base_model_from_feature_model
        model = get_base_model_from_feature_model(model)

    # 模型名称映射
    actual_model = model_mapping(model)
    enable_thinking = is_thinking_model(model)

    log.info(f"[ANTIGRAVITY GEMINI] Request: model={model} -> {actual_model}, thinking={enable_thinking}")

    # 转换 Gemini contents 为 Antigravity contents
    try:
        contents = gemini_contents_to_antigravity_contents(request_data["contents"])
    except Exception as e:
        log.error(f"Failed to convert Gemini contents: {e}")
        raise HTTPException(status_code=500, detail=f"Message conversion failed: {str(e)}")

    # 提取 Gemini generationConfig
    gemini_config = request_data.get("generationConfig", {})

    # 转换为 Antigravity generation_config
    parameters = {
        "temperature": gemini_config.get("temperature"),
        "top_p": gemini_config.get("topP"),
        "top_k": gemini_config.get("topK"),
        "max_tokens": gemini_config.get("maxOutputTokens"),
        # 图片生成相关参数
        "response_modalities": gemini_config.get("response_modalities"),
        "image_config": gemini_config.get("image_config"),
    }
    # 过滤 None 值
    parameters = {k: v for k, v in parameters.items() if v is not None}

    generation_config = generate_generation_config(parameters, enable_thinking, actual_model)

    # 获取凭证信息（用于 project_id 和 session_id）
    cred_result = await cred_mgr.get_valid_credential(is_antigravity=True)
    if not cred_result:
        log.error("当前无可用 antigravity 凭证")
        raise HTTPException(status_code=500, detail="当前无可用 antigravity 凭证")

    _, credential_data = cred_result
    project_id = credential_data.get("project_id", "default-project")
    session_id = credential_data.get("session_id", f"session-{uuid.uuid4().hex}")

    # 处理 systemInstruction
    system_instruction = None
    if "systemInstruction" in request_data:
        system_instruction = request_data["systemInstruction"]

    # 处理 tools
    antigravity_tools = None
    if "tools" in request_data:
        # Gemini 和 Antigravity 的 tools 格式基本一致
        antigravity_tools = request_data["tools"]

    # 构建 Antigravity 请求体
    request_body = build_antigravity_request_body(
        contents=contents,
        model=actual_model,
        project_id=project_id,
        session_id=session_id,
        system_instruction=system_instruction,
        tools=antigravity_tools,
        generation_config=generation_config,
    )

    # 发送非流式请求
    try:
        response_data, cred_name, cred_data = await send_antigravity_request_no_stream(
            request_body, cred_mgr
        )

        # 转换并返回 Gemini 格式响应
        gemini_response = convert_antigravity_response_to_gemini(response_data)
        return JSONResponse(content=gemini_response)

    except Exception as e:
        log.error(f"[ANTIGRAVITY GEMINI] Request failed: {e}")
        raise HTTPException(status_code=500, detail=f"Antigravity API request failed: {str(e)}")


@router.post("/antigravity/v1beta/models/{model:path}:streamGenerateContent")
@router.post("/antigravity/v1/models/{model:path}:streamGenerateContent")
async def gemini_stream_generate_content(
    model: str = Path(..., description="Model name"),
    request: Request = None,
    api_key: str = Depends(authenticate_gemini_flexible),
):
    """处理 Gemini 格式的流式内容生成请求（通过 Antigravity API）"""
    log.debug(f"[ANTIGRAVITY GEMINI] Streaming request for model: {model}")

    # 获取原始请求数据
    try:
        request_data = await request.json()
    except Exception as e:
        log.error(f"Failed to parse JSON request: {e}")
        raise HTTPException(status_code=400, detail=f"Invalid JSON: {str(e)}")

    # 验证必要字段
    if "contents" not in request_data or not request_data["contents"]:
        raise HTTPException(status_code=400, detail="Missing required field: contents")

    # 获取凭证管理器
    from src.credential_manager import get_credential_manager
    cred_mgr = await get_credential_manager()

    # 提取模型名称（移除 "models/" 前缀）
    if model.startswith("models/"):
        model = model[7:]

    # 检测并处理抗截断模式
    use_anti_truncation = is_anti_truncation_model(model)
    if use_anti_truncation:
        # 去掉 "流式抗截断/" 前缀
        from src.utils import get_base_model_from_feature_model
        model = get_base_model_from_feature_model(model)

    # 模型名称映射
    actual_model = model_mapping(model)
    enable_thinking = is_thinking_model(model)

    log.info(f"[ANTIGRAVITY GEMINI] Stream request: model={model} -> {actual_model}, thinking={enable_thinking}, anti_truncation={use_anti_truncation}")

    # 转换 Gemini contents 为 Antigravity contents
    try:
        contents = gemini_contents_to_antigravity_contents(request_data["contents"])
    except Exception as e:
        log.error(f"Failed to convert Gemini contents: {e}")
        raise HTTPException(status_code=500, detail=f"Message conversion failed: {str(e)}")

    # 提取 Gemini generationConfig
    gemini_config = request_data.get("generationConfig", {})

    # 转换为 Antigravity generation_config
    parameters = {
        "temperature": gemini_config.get("temperature"),
        "top_p": gemini_config.get("topP"),
        "top_k": gemini_config.get("topK"),
        "max_tokens": gemini_config.get("maxOutputTokens"),
        # 图片生成相关参数
        "response_modalities": gemini_config.get("response_modalities"),
        "image_config": gemini_config.get("image_config"),
    }
    # 过滤 None 值
    parameters = {k: v for k, v in parameters.items() if v is not None}

    generation_config = generate_generation_config(parameters, enable_thinking, actual_model)

    # 获取凭证信息（用于 project_id 和 session_id）
    cred_result = await cred_mgr.get_valid_credential(is_antigravity=True)
    if not cred_result:
        log.error("当前无可用 antigravity 凭证")
        raise HTTPException(status_code=500, detail="当前无可用 antigravity 凭证")

    _, credential_data = cred_result
    project_id = credential_data.get("project_id", "default-project")
    session_id = credential_data.get("session_id", f"session-{uuid.uuid4().hex}")

    # 处理 systemInstruction
    system_instruction = None
    if "systemInstruction" in request_data:
        system_instruction = request_data["systemInstruction"]

    # 处理 tools
    antigravity_tools = None
    if "tools" in request_data:
        # Gemini 和 Antigravity 的 tools 格式基本一致
        antigravity_tools = request_data["tools"]

    # 构建 Antigravity 请求体
    request_body = build_antigravity_request_body(
        contents=contents,
        model=actual_model,
        project_id=project_id,
        session_id=session_id,
        system_instruction=system_instruction,
        tools=antigravity_tools,
        generation_config=generation_config,
    )

    # 发送流式请求
    try:
        # 处理抗截断功能（仅流式传输时有效）
        if use_anti_truncation:
            log.info("[ANTIGRAVITY GEMINI] 启用流式抗截断功能")
            max_attempts = await get_anti_truncation_max_attempts()

            # 包装请求函数以适配抗截断处理器
            async def antigravity_gemini_request_func(payload):
                resources, cred_name, cred_data = await send_antigravity_request_stream(
                    payload, cred_mgr
                )
                response, stream_ctx, client = resources
                return StreamingResponse(
                    convert_antigravity_stream_to_gemini(
                        response, stream_ctx, client, cred_mgr, cred_name
                    ),
                    media_type="text/event-stream"
                )

            return await apply_anti_truncation_to_stream(
                antigravity_gemini_request_func, request_body, max_attempts
            )

        # 流式请求（无抗截断）
        resources, cred_name, cred_data = await send_antigravity_request_stream(
            request_body, cred_mgr
        )
        # resources 是一个元组: (response, stream_ctx, client)
        response, stream_ctx, client = resources

        # 转换并返回流式响应
        # response 现在是 filtered_lines 生成器
        return StreamingResponse(
            convert_antigravity_stream_to_gemini(
                response, stream_ctx, client, cred_mgr, cred_name
            ),
            media_type="text/event-stream"
        )

    except Exception as e:
        log.error(f"[ANTIGRAVITY GEMINI] Stream request failed: {e}")
        raise HTTPException(status_code=500, detail=f"Antigravity API request failed: {str(e)}")


# ==================== SD-WebUI 格式 API 端点 ====================

@router.get("/sdapi/v1/options")
@router.get("/antigravity/sdapi/v1/options")
async def sdwebui_get_options(_: str = Depends(authenticate_sdwebui_flexible)):
    """返回 SD-WebUI 格式的配置选项"""
    log.info("[ANTIGRAVITY SD-WebUI] Received options request")
    # 返回基本的配置选项
    return {
        "sd_model_checkpoint": "gemini-3-pro-image",
        "sd_checkpoint_hash": None,
        "samples_save": True,
        "samples_format": "png",
        "save_images_add_number": True,
        "grid_save": True,
        "return_grid": True,
        "enable_pnginfo": True,
        "save_txt": False,
        "CLIP_stop_at_last_layers": 1,
    }


@router.get("/sdapi/v1/sd-models")
@router.get("/antigravity/sdapi/v1/sd-models")
async def sdwebui_list_models(_: str = Depends(authenticate_sdwebui_flexible)):
    """返回 SD-WebUI 格式的模型列表 - 只包含带 image 关键词的模型"""

    try:
        # 获取凭证管理器
        cred_mgr = await get_credential_manager()

        # 从 Antigravity API 获取模型列表
        models = await fetch_available_models(cred_mgr)

        if not models:
            log.warning("[ANTIGRAVITY SD-WebUI] Failed to fetch models from API, returning empty list")
            return []

        # 过滤只包含 "image" 关键词的模型
        image_models = []
        for model in models:
            model_id = model.get("id", "")
            if "image" in model_id.lower():
                # SD-WebUI 格式: {"title": "model_name", "model_name": "model_name", "hash": null}
                image_models.append({
                    "title": model_id,
                    "model_name": model_id,
                    "hash": None,
                    "sha256": None,
                    "filename": model_id,
                    "config": None
                })

        return image_models

    except Exception as e:
        log.error(f"[ANTIGRAVITY SD-WebUI] Error fetching models: {e}")
        return []


@router.post("/sdapi/v1/txt2img")
@router.post("/antigravity/sdapi/v1/txt2img")
async def sdwebui_txt2img(request: Request, _: str = Depends(authenticate_sdwebui_flexible)):
    """处理 SD-WebUI 格式的 txt2img 请求，转换为 Antigravity API"""
    # 获取原始请求数据
    try:
        request_data = await request.json()
    except Exception as e:
        log.error(f"Failed to parse JSON request: {e}")
        raise HTTPException(status_code=400, detail=f"Invalid JSON: {str(e)}")

    # 提取基本参数
    prompt = request_data.get("prompt", "")
    if not prompt:
        raise HTTPException(status_code=400, detail="Missing required field: prompt")

    negative_prompt = request_data.get("negative_prompt", "")

    # 提取图片生成相关参数
    width = request_data.get("width", 1024)
    height = request_data.get("height", 1024)

    # 计算 aspect_ratio - 映射到支持的比例
    # 支持的比例: "1:1", "2:3", "3:2", "3:4", "4:3", "4:5", "5:4", "9:16", "16:9", "21:9"
    def find_closest_aspect_ratio(w: int, h: int) -> str:
        ratio = w / h
        supported_ratios = {
            "1:1": 1.0,
            "2:3": 0.667,
            "3:2": 1.5,
            "3:4": 0.75,
            "4:3": 1.333,
            "4:5": 0.8,
            "5:4": 1.25,
            "9:16": 0.5625,
            "16:9": 1.778,
            "21:9": 2.333
        }
        closest = min(supported_ratios.items(), key=lambda x: abs(x[1] - ratio))
        return closest[0]

    aspect_ratio = find_closest_aspect_ratio(width, height)

    # 简化的尺寸映射到 image_size
    max_dimension = max(width, height)
    if max_dimension <= 1024:
        image_size = "1K"
    elif max_dimension <= 2048:
        image_size = "2K"
    else:
        image_size = "4K"

    # 提取模型（如果指定）
    model = request_data.get("override_settings", {}).get("sd_model_checkpoint", "gemini-3-pro-image")
    if not model or "image" not in model.lower():
        model = "gemini-3-pro-image"

    log.info(f"[ANTIGRAVITY SD-WebUI] txt2img request: model={model}, prompt={prompt[:50]}..., aspect_ratio={aspect_ratio}, image_size={image_size}")

    cred_mgr = await get_credential_manager()

    # 构建 Gemini 格式的 contents
    full_prompt = prompt
    if negative_prompt:
        full_prompt = f"{prompt}\n\nNegative prompt: {negative_prompt}"

    contents = [{
        "role": "user",
        "parts": [{"text": full_prompt}]
    }]

    # 构建 generation_config，包含图片生成参数
    parameters = {
        "response_modalities": ["TEXT", "IMAGE"],
        "image_config": {
            "aspect_ratio": aspect_ratio,
            "image_size": image_size
        }
    }

    # 模型名称映射
    actual_model = model_mapping(model)
    enable_thinking = is_thinking_model(model)

    generation_config = generate_generation_config(parameters, enable_thinking, actual_model)

    # 获取凭证信息
    cred_result = await cred_mgr.get_valid_credential(is_antigravity=True)
    if not cred_result:
        log.error("当前无可用 antigravity 凭证")
        raise HTTPException(status_code=500, detail="当前无可用 antigravity 凭证")

    _, credential_data = cred_result
    project_id = credential_data.get("project_id", "default-project")
    session_id = f"session-{uuid.uuid4().hex}"

    # 构建 Antigravity 请求体
    request_body = build_antigravity_request_body(
        contents=contents,
        model=actual_model,
        project_id=project_id,
        session_id=session_id,
        tools=None,
        generation_config=generation_config,
    )

    # 发送非流式请求
    try:
        response_data, cred_name, cred_data = await send_antigravity_request_no_stream(
            request_body, cred_mgr
        )

        # 提取生成的图片
        parts = response_data.get("response", {}).get("candidates", [{}])[0].get("content", {}).get("parts", [])

        images = []
        info_text = ""

        for part in parts:
            if "inlineData" in part:
                inline_data = part["inlineData"]
                base64_data = inline_data.get("data", "")
                images.append(base64_data)
            elif "text" in part:
                info_text += part.get("text", "")

        if not images:
            raise HTTPException(status_code=500, detail="No images generated")

        # 构建 SD-WebUI 格式的响应
        sdwebui_response = {
            "images": images,
            "parameters": request_data,
            "info": info_text or f"Generated by {model}"
        }

        return JSONResponse(content=sdwebui_response)

    except Exception as e:
        log.error(f"[ANTIGRAVITY SD-WebUI] txt2img request failed: {e}")
        raise HTTPException(status_code=500, detail=f"Image generation failed: {str(e)}")

