"""
Antigravity Router - Handles OpenAI and Gemini format requests and converts to Antigravity API
å¤„ç† OpenAI å’Œ Gemini æ ¼å¼è¯·æ±‚å¹¶è½¬æ¢ä¸º Antigravity API æ ¼å¼
"""

import json
import time
import uuid
from typing import Any, Dict, List, Optional

from fastapi import APIRouter, Depends, HTTPException, Path, Request
from fastapi.responses import JSONResponse, StreamingResponse

from config import get_anti_truncation_max_attempts
from log import log
from src.utils import is_anti_truncation_model, authenticate_bearer, authenticate_gemini_flexible, authenticate_sdwebui_flexible

from .antigravity_api import (
    build_antigravity_request_body,
    send_antigravity_request_no_stream,
    send_antigravity_request_stream,
    fetch_available_models,
)
from .credential_manager import CredentialManager
from .models import (
    ChatCompletionRequest,
    GeminiGenerationConfig,
    Model,
    ModelList,
    model_to_dict,
    OpenAIChatCompletionChoice,
    OpenAIChatCompletionResponse,
    OpenAIChatMessage,
    OpenAIToolCall,
    OpenAIToolFunction,
)
from .anti_truncation import (
    apply_anti_truncation_to_stream,
)

# å¯¼å…¥æ ¼å¼è½¬æ¢å™¨
from .converters import (
    # model_config
    model_mapping,
    get_fallback_models,
    should_fallback_on_error,
    is_thinking_model,
    # message_converter
    extract_images_from_content,
    strip_thinking_from_openai_messages,
    openai_messages_to_antigravity_contents,
    gemini_contents_to_antigravity_contents,
    # tool_converter
    extract_tool_params_summary,
    convert_openai_tools_to_antigravity,
    generate_generation_config,
)

# åˆ›å»ºè·¯ç”±å™¨
router = APIRouter()

# å…¨å±€å‡­è¯ç®¡ç†å™¨å®ä¾‹
credential_manager = None


async def get_credential_manager():
    """è·å–å…¨å±€å‡­è¯ç®¡ç†å™¨å®ä¾‹"""
    global credential_manager
    if not credential_manager:
        credential_manager = CredentialManager()
        await credential_manager.initialize()
    return credential_manager



def convert_to_openai_tool_call(function_call: Dict[str, Any], index: int = None) -> Dict[str, Any]:
    """
    å°† Antigravity functionCall è½¬æ¢ä¸º OpenAI tool_callï¼Œä½¿ç”¨ OpenAIToolCall æ¨¡å‹

    Args:
        function_call: Antigravity æ ¼å¼çš„å‡½æ•°è°ƒç”¨
        index: å·¥å…·è°ƒç”¨ç´¢å¼•ï¼ˆæµå¼å“åº”å¿…éœ€ï¼‰
    """
    tool_call = OpenAIToolCall(
        index=index,
        id=function_call.get("id", f"call_{uuid.uuid4().hex[:24]}"),
        type="function",
        function=OpenAIToolFunction(
            name=function_call.get("name", ""),
            arguments=json.dumps(function_call.get("args", {}))
        )
    )
    result = model_to_dict(tool_call)
    # Remove None values for cleaner output (non-streaming doesn't need index)
    if result.get("index") is None:
        del result["index"]
    return result


async def convert_antigravity_stream_to_openai(
    lines_generator: Any,
    stream_ctx: Any,
    client: Any,
    model: str,
    request_id: str,
    credential_manager: Any,
    credential_name: str,
    request_body: Optional[Dict[str, Any]] = None,  # âœ… æ–°å¢ï¼šç”¨äºè·å–ä¸Šä¸‹æ–‡ä¿¡æ¯
    cred_mgr: Optional[Any] = None,  # âœ… æ–°å¢ï¼šç”¨äº fallbackï¼ˆæœªæ¥å¯èƒ½ä½¿ç”¨ï¼‰
    context_info: Optional[Dict[str, Any]] = None  # âœ… æ–°å¢ï¼šä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆtoken æ•°ã€å·¥å…·ç»“æœæ•°é‡ç­‰ï¼‰
):
    """
    å°† Antigravity æµå¼å“åº”è½¬æ¢ä¸º OpenAI æ ¼å¼çš„ SSE æµ

    Args:
        lines_generator: è¡Œç”Ÿæˆå™¨ (å·²ç»è¿‡æ»¤çš„ SSE è¡Œ)
    """
    state = {
        "thinking_started": False,
        "tool_calls": [],
        "content_buffer": "",
        "thinking_buffer": "",
        "success_recorded": False,
        "sse_lines_received": 0,  # è®°å½•æ”¶åˆ°çš„ SSE è¡Œæ•°
        "chunks_sent": 0,  # è®°å½•å‘é€çš„ chunk æ•°
        "tool_calls_sent": False,  # æ ‡è®°å·¥å…·è°ƒç”¨æ˜¯å¦å·²å‘é€
        "finish_reason_sent": False,  # æ ‡è®° finish_reason æ˜¯å¦å·²å‘é€
        "has_valid_content": False,  # æ ‡è®°æ˜¯å¦æ”¶åˆ°äº†æœ‰æ•ˆå†…å®¹ï¼ˆtext æˆ– functionCallï¼‰
        "empty_parts_count": 0,  # è®°å½•ç©º parts çš„æ¬¡æ•°
    }

    created = int(time.time())

    try:
        def build_content_chunk(content: str) -> str:
            chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {"content": content},
                    "finish_reason": None
                }]
            }
            return f"data: {json.dumps(chunk)}\n\n"

        def flush_thinking_buffer() -> Optional[str]:
            if not state["thinking_started"]:
                return None
            state["thinking_buffer"] += "\n</think>\n"
            thinking_block = state["thinking_buffer"]
            state["content_buffer"] += thinking_block
            state["thinking_buffer"] = ""
            state["thinking_started"] = False
            return thinking_block

        async for line in lines_generator:
            if not line or not line.startswith("data: "):
                continue

            # è®°å½•ç¬¬ä¸€æ¬¡æˆåŠŸå“åº”
            if not state["success_recorded"]:
                if credential_name and credential_manager:
                    await credential_manager.record_api_call_result(credential_name, True, is_antigravity=True)
                state["success_recorded"] = True

            # è§£æ SSE æ•°æ®
            try:
                data = json.loads(line[6:])  # å»æ‰ "data: " å‰ç¼€
            except:
                continue

            state["sse_lines_received"] += 1

            # DEBUG: è®°å½•æ”¶åˆ°çš„åŸå§‹æ•°æ®ï¼ˆç”¨äºè¯Šæ–­ç©ºå“åº”é—®é¢˜ï¼‰
            log.info(f"[ANTIGRAVITY STREAM] SSE line {state['sse_lines_received']}: {json.dumps(data)[:500]}")
            log.debug(f"[ANTIGRAVITY STREAM] Received data keys: {list(data.keys())}")

            # âœ… æ–°å¢ï¼šæå– cachedContentTokenCountï¼ˆå¦‚æœå¯ç”¨ï¼‰
            # è¿™ä¸ªä¿¡æ¯å¯èƒ½åœ¨ç¬¬ä¸€ä¸ª chunk å°±å‡ºç°ï¼Œç”¨äºåˆ¤æ–­å®é™…å¤„ç†çš„ tokens
            usage_metadata = data.get("response", {}).get("usageMetadata", {})
            if usage_metadata:
                cached_content_token_count = usage_metadata.get("cachedContentTokenCount", 0)
                if cached_content_token_count > 0 and "cached_content_token_count" not in state:
                    # åªåœ¨ç¬¬ä¸€æ¬¡æå–æ—¶ä¿å­˜ï¼ˆé¿å…è¦†ç›–ï¼‰
                    state["cached_content_token_count"] = cached_content_token_count
                    prompt_token_count = usage_metadata.get("promptTokenCount", 0)
                    if prompt_token_count > 0:
                        state["prompt_token_count"] = prompt_token_count
                        actual_processed_tokens = prompt_token_count - cached_content_token_count
                        state["actual_processed_tokens"] = actual_processed_tokens
                        log.info(f"[ANTIGRAVITY STREAM] Cache detected: {cached_content_token_count:,} tokens cached, "
                               f"{actual_processed_tokens:,} tokens actually processed (out of {prompt_token_count:,} total)")

            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if "error" in data:
                log.error(f"[ANTIGRAVITY STREAM] Error in response: {data.get('error')}")

            # æå– candidates å’Œ parts
            response = data.get("response", {})
            candidates = response.get("candidates", [])
            log.info(f"[ANTIGRAVITY STREAM] Response has {len(candidates)} candidates")

            if candidates:
                candidate = candidates[0]
                content_obj = candidate.get("content", {})
                finish_reason_raw = candidate.get("finishReason")
                log.info(f"[ANTIGRAVITY STREAM] Candidate 0: finishReason={finish_reason_raw}, content_keys={list(content_obj.keys()) if content_obj else 'None'}")
                parts = content_obj.get("parts", [])

                # æ£€æµ‹ç©º parts æƒ…å†µ
                if not parts and finish_reason_raw:
                    state["empty_parts_count"] += 1
                    log.warning(f"[ANTIGRAVITY STREAM] Empty parts with finishReason={finish_reason_raw}! This may cause empty response.")
            else:
                parts = []
                log.warning(f"[ANTIGRAVITY STREAM] No candidates in response!")

            # DEBUG: è®°å½• parts ä¿¡æ¯
            if parts:
                part_types = [list(p.keys()) if isinstance(p, dict) else type(p).__name__ for p in parts]
                log.debug(f"[ANTIGRAVITY STREAM] Parts count: {len(parts)}, types: {part_types}")

            # æ£€æŸ¥ finishReasonï¼ˆæå‰æ£€æŸ¥ä»¥ä¾¿è®°å½•ï¼‰
            candidates = data.get("response", {}).get("candidates", [])
            if candidates:
                candidate = candidates[0]
                fr = candidate.get("finishReason")
                if fr:
                    log.info(f"[ANTIGRAVITY STREAM] finishReason detected: {fr}")
                    # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹
                    content_obj = candidate.get("content", {})
                    log.debug(f"[ANTIGRAVITY STREAM] Final content keys: {list(content_obj.keys()) if content_obj else 'None'}")

            for part in parts:
                # å¤„ç†æ€è€ƒå†…å®¹
                if part.get("thought") is True:
                    if not state["thinking_started"]:
                        state["thinking_buffer"] = "<think>\n"
                        state["thinking_started"] = True
                    state["thinking_buffer"] += part.get("text", "")

                # å¤„ç†å›¾ç‰‡æ•°æ® (inlineData)
                elif "inlineData" in part:
                    # å¦‚æœä¹‹å‰åœ¨æ€è€ƒï¼Œå…ˆç»“æŸæ€è€ƒ
                    thinking_block = flush_thinking_buffer()
                    if thinking_block:
                        yield build_content_chunk(thinking_block)

                    # æå–å›¾ç‰‡æ•°æ®
                    inline_data = part["inlineData"]
                    mime_type = inline_data.get("mimeType", "image/png")
                    base64_data = inline_data.get("data", "")

                    # è½¬æ¢ä¸º Markdown æ ¼å¼çš„å›¾ç‰‡
                    image_markdown = f"\n\n![ç”Ÿæˆçš„å›¾ç‰‡](data:{mime_type};base64,{base64_data})\n\n"
                    state["content_buffer"] += image_markdown

                    # å‘é€å›¾ç‰‡å—
                    chunk = {
                        "id": request_id,
                        "object": "chat.completion.chunk",
                        "created": created,
                        "model": model,
                        "choices": [{
                            "index": 0,
                            "delta": {"content": image_markdown},
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(chunk)}\n\n"

                # å¤„ç†æ™®é€šæ–‡æœ¬
                elif "text" in part:
                    # å¦‚æœä¹‹å‰åœ¨æ€è€ƒï¼Œå…ˆç»“æŸæ€è€ƒ
                    thinking_block = flush_thinking_buffer()
                    if thinking_block:
                        yield build_content_chunk(thinking_block)

                    # æ·»åŠ æ–‡æœ¬å†…å®¹
                    text = part.get("text", "")

                    # åªæœ‰å½“ text éç©ºæ—¶æ‰å‘é€å’Œæ ‡è®°ä¸ºæœ‰æ•ˆå†…å®¹
                    # ä¿®å¤ï¼šAntigravity æœ‰æ—¶è¿”å› {"text": ""} ç©ºå­—ç¬¦ä¸²ï¼Œä¸åº”è§†ä¸ºæœ‰æ•ˆå†…å®¹
                    if text:  # éç©ºå­—ç¬¦ä¸²æ‰å¤„ç†
                        state["content_buffer"] += text

                        # å‘é€æ–‡æœ¬å—
                        chunk = {
                            "id": request_id,
                            "object": "chat.completion.chunk",
                            "created": created,
                            "model": model,
                            "choices": [{
                                "index": 0,
                                "delta": {"content": text},
                                "finish_reason": None
                            }]
                        }
                        yield f"data: {json.dumps(chunk)}\n\n"
                        state["chunks_sent"] += 1
                        state["has_valid_content"] = True  # æ”¶åˆ°äº†æœ‰æ•ˆçš„æ–‡æœ¬å†…å®¹
                    else:
                        # âœ… æ–°å¢ï¼šè®°å½•ç©ºæ–‡æœ¬ï¼Œä½†å¦‚æœæ˜¯ç¬¬ä¸€ä¸ª chunk ä¸”æ—  finishReasonï¼Œç»§ç»­ç­‰å¾…
                        state["empty_parts_count"] = state.get("empty_parts_count", 0) + 1
                        if state["sse_lines_received"] == 1 and not finish_reason_raw:
                            log.info(f"[ANTIGRAVITY STREAM] First chunk has empty text and no finishReason, "
                                   f"waiting for more chunks (this may indicate stream is starting or context is too long)")
                            # ä¸ç«‹å³æ ‡è®°ä¸ºæ— æ•ˆï¼Œç»§ç»­ç­‰å¾…åç»­ chunk
                        else:
                            log.debug(f"[ANTIGRAVITY STREAM] Skipping empty text in part")

                # å¤„ç†å·¥å…·è°ƒç”¨
                elif "functionCall" in part:
                    tool_index = len(state["tool_calls"])
                    fc = part["functionCall"]
                    log.info(f"[ANTIGRAVITY STREAM] Tool call detected: name={fc.get('name')}, id={fc.get('id')}")
                    tool_call = convert_to_openai_tool_call(fc, index=tool_index)
                    state["tool_calls"].append(tool_call)
                    state["has_valid_content"] = True  # æ”¶åˆ°äº†æœ‰æ•ˆçš„å·¥å…·è°ƒç”¨
                    log.debug(f"[ANTIGRAVITY STREAM] Converted tool_call: {json.dumps(tool_call)[:200]}")

            # æ£€æŸ¥æ˜¯å¦ç»“æŸ
            finish_reason = data.get("response", {}).get("candidates", [{}])[0].get("finishReason")

            # âœ… æ–°å¢ï¼šå¤„ç† SAFETY/RECITATION finishReason
            if finish_reason in ("SAFETY", "RECITATION"):
                log.warning(f"[ANTIGRAVITY STREAM] Response blocked by {finish_reason} filter")

                # âœ… æ„å»ºåŒ…å«å·¥å…·æç¤ºçš„é”™è¯¯æ¶ˆæ¯
                error_msg_parts = []
                error_msg_parts.append(f"[Response blocked by {finish_reason} filter. The content may have triggered safety policies.]")
                error_msg_parts.append("")
                error_msg_parts.append("âš ï¸ **Tool Call Format Reminder**:")
                error_msg_parts.append("If you encounter 'invalid arguments' errors when calling tools:")
                error_msg_parts.append("- Use EXACT parameter names from the current tool schema")
                error_msg_parts.append("- Do NOT use parameters from previous conversations")
                error_msg_parts.append("- For terminal/command tools: verify parameter name (may be `command`, `input`, or `cmd`)")

                error_chunk = {
                    "id": request_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": model,
                    "choices": [{
                        "index": 0,
                        "delta": {"content": "\n".join(error_msg_parts)},
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(error_chunk)}\n\n"
                state["chunks_sent"] += 1
                state["has_valid_content"] = True  # æ ‡è®°ä¸ºæœ‰å†…å®¹ï¼ˆé”™è¯¯æ¶ˆæ¯ï¼‰

            if finish_reason:
                thinking_block = flush_thinking_buffer()
                if thinking_block:
                    yield build_content_chunk(thinking_block)

                # å‘é€å·¥å…·è°ƒç”¨
                if state["tool_calls"]:
                    log.info(f"[ANTIGRAVITY STREAM] Sending {len(state['tool_calls'])} tool calls")
                    chunk = {
                        "id": request_id,
                        "object": "chat.completion.chunk",
                        "created": created,
                        "model": model,
                        "choices": [{
                            "index": 0,
                            "delta": {"tool_calls": state["tool_calls"]},
                            "finish_reason": None
                        }]
                    }
                    log.debug(f"[ANTIGRAVITY STREAM] Tool calls chunk: {json.dumps(chunk)[:500]}")
                    yield f"data: {json.dumps(chunk)}\n\n"
                    state["tool_calls_sent"] = True  # æ ‡è®°å·¥å…·è°ƒç”¨å·²å‘é€

                # å‘é€ä½¿ç”¨ç»Ÿè®¡
                usage_metadata = data.get("response", {}).get("usageMetadata", {})
                prompt_token_count = usage_metadata.get("promptTokenCount", 0)
                cached_content_token_count = usage_metadata.get("cachedContentTokenCount", 0)
                # âœ… æ–°å¢ï¼šä¿å­˜ promptTokenCount å’Œ cachedContentTokenCount åˆ° stateï¼Œç”¨äºé”™è¯¯æ¶ˆæ¯
                state["prompt_token_count"] = prompt_token_count
                state["cached_content_token_count"] = cached_content_token_count
                # âœ… è®¡ç®—å®é™…å¤„ç†çš„ tokensï¼ˆæ’é™¤ç¼“å­˜çš„éƒ¨åˆ†ï¼‰
                actual_processed_tokens = prompt_token_count - cached_content_token_count
                state["actual_processed_tokens"] = actual_processed_tokens
                usage = {
                    "prompt_tokens": prompt_token_count,
                    "completion_tokens": usage_metadata.get("candidatesTokenCount", 0),
                    "total_tokens": usage_metadata.get("totalTokenCount", 0)
                }

                # ç¡®å®š finish_reason
                openai_finish_reason = "stop"
                if state["tool_calls"]:
                    openai_finish_reason = "tool_calls"
                elif finish_reason == "MAX_TOKENS":
                    openai_finish_reason = "length"

                chunk = {
                    "id": request_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": model,
                    "choices": [{
                        "index": 0,
                        "delta": {},
                        "finish_reason": openai_finish_reason
                    }],
                    "usage": usage
                }
                yield f"data: {json.dumps(chunk)}\n\n"

        # åœ¨æµç»“æŸå‰ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æœªå‘é€çš„å·¥å…·è°ƒç”¨
        # è¿™æ˜¯ä¸€ä¸ªä¿åº•é€»è¾‘ï¼Œç”¨äºå¤„ç† finishReason æ²¡æœ‰è¢«æ­£ç¡®æ£€æµ‹åˆ°çš„æƒ…å†µ
        if state["tool_calls"] and not state.get("tool_calls_sent", False):
            log.warning(f"[ANTIGRAVITY STREAM] Tool calls not sent yet, sending now (fallback logic)")
            log.info(f"[ANTIGRAVITY STREAM] Sending {len(state['tool_calls'])} tool calls (fallback)")

            # å‘é€å·¥å…·è°ƒç”¨
            chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {"tool_calls": state["tool_calls"]},
                    "finish_reason": None
                }]
            }
            yield f"data: {json.dumps(chunk)}\n\n"
            state["chunks_sent"] += 1

            # å‘é€ finish_reason
            finish_chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {},
                    "finish_reason": "tool_calls"
                }]
            }
            yield f"data: {json.dumps(finish_chunk)}\n\n"
            state["chunks_sent"] += 1
            state["finish_reason_sent"] = True

        # æ£€æŸ¥æ˜¯å¦æ”¶åˆ°äº†ä»»ä½•æœ‰æ•ˆæ•°æ®ä½†æ²¡æœ‰å‘é€ä»»ä½•å†…å®¹
        # è¿™å¯èƒ½è¡¨ç¤º Antigravity åç«¯è¿”å›äº†ç©ºå“åº”
        if state["sse_lines_received"] == 0:
            log.warning(f"[ANTIGRAVITY STREAM] No SSE data received from Antigravity backend!")

            # âœ… æ„å»ºåŒ…å«å·¥å…·æç¤ºçš„é”™è¯¯æ¶ˆæ¯
            error_msg_parts = []
            error_msg_parts.append("[Error: No response from backend. Please try again.]")
            error_msg_parts.append("")
            error_msg_parts.append("âš ï¸ **Tool Call Format Reminder**:")
            error_msg_parts.append("If you encounter 'invalid arguments' errors when calling tools:")
            error_msg_parts.append("- Use EXACT parameter names from the current tool schema")
            error_msg_parts.append("- Do NOT use parameters from previous conversations")
            error_msg_parts.append("- For terminal/command tools: verify parameter name (may be `command`, `input`, or `cmd`)")
            error_msg_parts.append("- When in doubt: re-read the tool definition")

            error_chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {"content": "\n".join(error_msg_parts)},
                    "finish_reason": None
                }]
            }
            yield f"data: {json.dumps(error_chunk)}\n\n"

            # å‘é€ finish_reason
            finish_chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {},
                    "finish_reason": "stop"
                }]
            }
            yield f"data: {json.dumps(finish_chunk)}\n\n"
            state["finish_reason_sent"] = True

        elif not state.get("has_valid_content", False):
            # æ²¡æœ‰æ”¶åˆ°ä»»ä½•æœ‰æ•ˆå†…å®¹ï¼ˆtext æˆ– functionCallï¼‰
            # è¿™æ˜¯åç«¯å¼‚å¸¸ï¼Œåº”è¯¥æ˜ç¡®å‘ŠçŸ¥å‰ç«¯
            log.error(f"[ANTIGRAVITY STREAM] No valid content received! "
                      f"SSE lines: {state['sse_lines_received']}, "
                      f"empty_parts_count: {state.get('empty_parts_count', 0)}, "
                      f"finish_reason: {finish_reason}")
            
            # âœ… æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦åªæœ‰ç©ºæ–‡æœ¬ chunkï¼ˆå¯èƒ½æ˜¯ä¸Šä¸‹æ–‡è¿‡é•¿æˆ–æµå¼å“åº”è¢«æˆªæ–­ï¼‰
            if state.get("empty_parts_count", 0) > 0 and state["chunks_sent"] == 0:
                log.warning(f"[ANTIGRAVITY STREAM] Stream ended with only empty text chunks. "
                           f"This may indicate: 1) Context too long (check promptTokenCount), "
                           f"2) API capacity issue, 3) Stream truncated before finishReason")

            # âœ… æ–°å¢ï¼šæ„å»ºæ˜ç¡®çš„ã€å¯æ“ä½œçš„é”™è¯¯æ¶ˆæ¯ï¼Œè®© Cursor çš„ AI agent çŸ¥é“éœ€è¦å‹ç¼©ä¸Šä¸‹æ–‡
            # ä» state ä¸­è·å– promptTokenCount å’Œç¼“å­˜ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            prompt_token_count = state.get("prompt_token_count", 0)
            cached_content_token_count = state.get("cached_content_token_count", 0)
            actual_processed_tokens = state.get("actual_processed_tokens", 0)
            
            # âœ… æ–°å¢ï¼šåŸºäºå®é™…å¤„ç†çš„ tokens è¿›è¡ŒåŠ¨æ€ fallback
            # å¦‚æœå®é™…å¤„ç†çš„ tokens è¶…è¿‡é˜ˆå€¼ï¼ˆ50Kï¼‰ï¼Œä¸”è¿˜æ²¡æœ‰æ”¶åˆ°æœ‰æ•ˆå†…å®¹ï¼Œå°è¯•éæµå¼ fallback
            ACTUAL_PROCESSED_TOKENS_THRESHOLD = 50000  # 50K tokens é˜ˆå€¼
            should_try_non_streaming_fallback = (
                request_body and 
                cred_mgr and 
                actual_processed_tokens > ACTUAL_PROCESSED_TOKENS_THRESHOLD and
                not state.get("fallback_attempted", False)  # é¿å…é‡å¤å°è¯•
            )
            
            if should_try_non_streaming_fallback:
                log.warning(f"[ANTIGRAVITY STREAM] Actual processed tokens ({actual_processed_tokens:,}) exceed threshold ({ACTUAL_PROCESSED_TOKENS_THRESHOLD:,}). "
                           f"Attempting non-streaming fallback...")
                state["fallback_attempted"] = True
                
                try:
                    # å°è¯•éæµå¼è¯·æ±‚
                    from .antigravity_api import send_antigravity_request_no_stream
                    response_data, _, _ = await send_antigravity_request_no_stream(
                        request_body, cred_mgr
                    )
                    
                    # è½¬æ¢éæµå¼å“åº”ä¸º OpenAI æ ¼å¼
                    openai_response_dict = convert_antigravity_response_to_openai(response_data, model, request_id)
                    
                    # æå–å†…å®¹å’Œå·¥å…·è°ƒç”¨
                    fallback_content = ""
                    fallback_tool_calls = []
                    if openai_response_dict and openai_response_dict.get("choices"):
                        choice = openai_response_dict["choices"][0]
                        message = choice.get("message", {})
                        if message and message.get("content"):
                            fallback_content = message["content"]
                        if message and message.get("tool_calls"):
                            fallback_tool_calls = message["tool_calls"]
                    
                    if fallback_content or fallback_tool_calls:
                        log.info(f"[ANTIGRAVITY STREAM] Non-streaming fallback successful! "
                               f"Content length: {len(fallback_content)}, Tool calls: {len(fallback_tool_calls)}")
                        
                        # å‘é€å·¥å…·è°ƒç”¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        if fallback_tool_calls:
                            tool_chunk = {
                                "id": request_id,
                                "object": "chat.completion.chunk",
                                "created": created,
                                "model": model,
                                "choices": [{
                                    "index": 0,
                                    "delta": {"tool_calls": fallback_tool_calls},
                                    "finish_reason": None
                                }]
                            }
                            yield f"data: {json.dumps(tool_chunk)}\n\n"
                            state["chunks_sent"] += 1
                            state["has_valid_content"] = True
                        
                        # å‘é€å†…å®¹ï¼ˆå¦‚æœå­˜åœ¨ï¼Œåˆ†å—å‘é€ä»¥ä¿æŒæµå¼ä½“éªŒï¼‰
                        if fallback_content:
                            # å°†å†…å®¹åˆ†æˆå°å—å‘é€ï¼Œæ¨¡æ‹Ÿæµå¼å“åº”
                            chunk_size = 100  # æ¯å—çº¦100å­—ç¬¦
                            for i in range(0, len(fallback_content), chunk_size):
                                chunk_text = fallback_content[i:i + chunk_size]
                                content_chunk = {
                                    "id": request_id,
                                    "object": "chat.completion.chunk",
                                    "created": created,
                                    "model": model,
                                    "choices": [{
                                        "index": 0,
                                        "delta": {"content": chunk_text},
                                        "finish_reason": None
                                    }]
                                }
                                yield f"data: {json.dumps(content_chunk)}\n\n"
                                state["chunks_sent"] += 1
                            state["has_valid_content"] = True
                        
                        # å‘é€ finish_reason
                        finish_reason_fallback = "stop"
                        if fallback_tool_calls:
                            finish_reason_fallback = "tool_calls"
                        
                        finish_chunk = {
                            "id": request_id,
                            "object": "chat.completion.chunk",
                            "created": created,
                            "model": model,
                            "choices": [{
                                "index": 0,
                                "delta": {},
                                "finish_reason": finish_reason_fallback
                            }]
                        }
                        yield f"data: {json.dumps(finish_chunk)}\n\n"
                        state["chunks_sent"] += 1
                        state["finish_reason_sent"] = True
                        state["has_valid_content"] = True
                        return  # æˆåŠŸ fallbackï¼Œç»“æŸæµ
                    else:
                        log.warning("[ANTIGRAVITY STREAM] Non-streaming fallback returned empty response.")
                except Exception as fallback_e:
                    log.error(f"[ANTIGRAVITY STREAM] Non-streaming fallback failed: {fallback_e}")
                    # Fallback å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œé”™è¯¯æ¶ˆæ¯é€»è¾‘
            
            # ä» context_info è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            estimated_tokens = context_info.get("estimated_tokens", 0) if context_info else 0
            tool_result_count = context_info.get("tool_result_count", 0) if context_info else 0
            total_tool_results_length = context_info.get("total_tool_results_length", 0) if context_info else 0
            
            # æ„å»ºè¯¦ç»†çš„é”™è¯¯æ¶ˆæ¯ï¼Œå‘Šè¯‰ Cursor çš„ AI agent éœ€è¦åšä»€ä¹ˆ
            error_parts = []
            error_parts.append("âŒ **Context Length Limit Exceeded**: Backend returned empty response due to context being too long.")
            error_parts.append("")
            
            # æ·»åŠ ä¸Šä¸‹æ–‡ç»Ÿè®¡ä¿¡æ¯ï¼ˆåŒ…å«ç¼“å­˜ä¿¡æ¯ï¼‰
            if prompt_token_count > 0:
                error_parts.append(f"ğŸ“Š **Input tokens**: {prompt_token_count:,}")
                if cached_content_token_count > 0:
                    error_parts.append(f"   - **Cached tokens**: {cached_content_token_count:,} (reused from previous requests)")
                    error_parts.append(f"   - **Actual processed tokens**: {actual_processed_tokens:,}")
                    if actual_processed_tokens < 50000:
                        error_parts.append(f"   âš ï¸ Note: Actual processed tokens ({actual_processed_tokens:,}) are within API limit, but API still returned empty response.")
                        error_parts.append(f"   This may indicate: 1) API capacity issue, 2) Stream truncation, 3) Safety filter")
                else:
                    error_parts.append(f"   âš ï¸ **No cache available**: All {prompt_token_count:,} tokens need to be processed")
            elif estimated_tokens > 0:
                error_parts.append(f"ğŸ“Š **Estimated input tokens**: {estimated_tokens:,} (exceeds API limit)")
            
            if tool_result_count > 0:
                error_parts.append(f"ğŸ”§ **Tool results**: {tool_result_count} results, {total_tool_results_length:,} characters")
            
            error_parts.append("")
            error_parts.append("ğŸ’¡ **Action Required**: You need to compress the context before retrying:")
            error_parts.append("")
            error_parts.append("1. **Summarize tool results**: Extract only essential information (errors, summaries, key findings)")
            error_parts.append("2. **Remove old tool results**: Keep only the most recent and relevant tool results")
            error_parts.append("3. **Truncate large results**: For large tool results, keep only the beginning and end, or extract key sections")
            error_parts.append("4. **Reduce tool calls**: Use fewer tool calls in the next request if possible")
            error_parts.append("")

            # âœ… å·¥å…·è°ƒç”¨æ ¼å¼æç¤ºï¼Œå¸®åŠ© agent è‡ªæˆ‘çº æ­£å‚æ•°æ ¼å¼é—®é¢˜
            error_parts.append("âš ï¸ **Tool Call Format Reminder** (IMPORTANT - Read carefully before making tool calls):")
            error_parts.append("")
            error_parts.append("If you encounter 'invalid arguments' errors when calling tools, please note:")
            error_parts.append("- **Always use the EXACT parameter names** as defined in the current tool schema")
            error_parts.append("- **Do NOT use parameters from previous conversations** - tool schemas may have changed")
            error_parts.append("- **Common mistakes to avoid**:")
            error_parts.append("  - `should_read_entire_file` â†’ Use `target_file` with `offset`/`limit` instead")
            error_parts.append("  - `start_line_one_indexed` / `end_line_one_indexed` â†’ Use `offset` / `limit` instead")
            error_parts.append("  - `command` for terminal â†’ Check if it should be `input`, `cmd`, or other name")
            error_parts.append("  - Unknown parameters â†’ Check the tool definition in current context")
            error_parts.append("- **Terminal/Command tools**: Parameter name varies - could be `command`, `input`, `cmd`, or `shell_command`")
            error_parts.append("- **When in doubt**: Re-read the tool definition and use only the parameters listed there")
            error_parts.append("")

            if cached_content_token_count > 0:
                error_parts.append(f"ğŸ’¡ **Cache Hint**: API cached {cached_content_token_count:,} tokens from previous requests.")
                error_parts.append(f"   If you keep similar context in subsequent requests, API may cache more tokens and reduce processing load.")
            error_parts.append("")
            error_parts.append("After compressing the context, retry the request with the reduced context.")
            
            error_msg = "\n".join(error_parts)
            error_chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {"content": error_msg},
                    "finish_reason": None
                }]
            }
            yield f"data: {json.dumps(error_chunk)}\n\n"
            state["chunks_sent"] += 1

            # å‘é€ finish_reason
            finish_chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {},
                    "finish_reason": "stop"
                }]
            }
            yield f"data: {json.dumps(finish_chunk)}\n\n"
            state["chunks_sent"] += 1
            state["finish_reason_sent"] = True

        # æ£€æŸ¥æ˜¯å¦å·²å‘é€ finish_reason
        # è¿™æ˜¯æœ€åçš„ä¿åº•é€»è¾‘ï¼Œç¡®ä¿ Cursor æ€»æ˜¯æ”¶åˆ° finish_reason
        if not state.get("finish_reason_sent", False):
            log.warning(f"[ANTIGRAVITY STREAM] finish_reason not sent yet, sending now (final fallback)")
            # ç¡®å®š finish_reason
            final_finish_reason = "tool_calls" if state["tool_calls"] else "stop"
            finish_chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {},
                    "finish_reason": final_finish_reason
                }]
            }
            yield f"data: {json.dumps(finish_chunk)}\n\n"
            state["finish_reason_sent"] = True
            state["chunks_sent"] += 1

        # å‘é€ç»“æŸæ ‡è®°
        log.info(f"[ANTIGRAVITY STREAM] Stream ending. SSE lines: {state['sse_lines_received']}, Chunks sent: {state['chunks_sent']}, Content buffer: {len(state['content_buffer'])}, Tool calls: {len(state['tool_calls'])}, has_valid_content: {state.get('has_valid_content', False)}, empty_parts_count: {state.get('empty_parts_count', 0)}, finish_reason_sent: {state.get('finish_reason_sent', False)}")
        yield "data: [DONE]\n\n"

    except Exception as e:
        log.error(f"[ANTIGRAVITY] Streaming error: {e}")

        # âœ… æ„å»ºåŒ…å«å·¥å…·æç¤ºçš„é”™è¯¯å“åº”
        error_msg_parts = []
        error_msg_parts.append(f"Streaming error: {str(e)}")
        error_msg_parts.append("")
        error_msg_parts.append("âš ï¸ **Tool Call Format Reminder**:")
        error_msg_parts.append("If you encounter 'invalid arguments' errors when calling tools:")
        error_msg_parts.append("- Use EXACT parameter names from the current tool schema")
        error_msg_parts.append("- Do NOT use parameters from previous conversations")
        error_msg_parts.append("- For terminal/command tools: verify parameter name (may be `command`, `input`, or `cmd`)")

        error_response = {
            "error": {
                "message": "\n".join(error_msg_parts),
                "type": "api_error",
                "code": 500
            }
        }
        yield f"data: {json.dumps(error_response)}\n\n"
    finally:
        # ç¡®ä¿æ¸…ç†æ‰€æœ‰èµ„æº
        try:
            await stream_ctx.__aexit__(None, None, None)
        except Exception as e:
            log.debug(f"[ANTIGRAVITY] Error closing stream context: {e}")
        try:
            await client.aclose()
        except Exception as e:
            log.debug(f"[ANTIGRAVITY] Error closing client: {e}")


def convert_antigravity_response_to_openai(
    response_data: Dict[str, Any],
    model: str,
    request_id: str
) -> Dict[str, Any]:
    """
    å°† Antigravity éæµå¼å“åº”è½¬æ¢ä¸º OpenAI æ ¼å¼
    """
    # æå– parts
    parts = response_data.get("response", {}).get("candidates", [{}])[0].get("content", {}).get("parts", [])

    content = ""
    thinking_content = ""
    tool_calls_list = []

    for part in parts:
        # å¤„ç†æ€è€ƒå†…å®¹
        if part.get("thought") is True:
            thinking_content += part.get("text", "")

        # å¤„ç†å›¾ç‰‡æ•°æ® (inlineData)
        elif "inlineData" in part:
            inline_data = part["inlineData"]
            mime_type = inline_data.get("mimeType", "image/png")
            base64_data = inline_data.get("data", "")
            # è½¬æ¢ä¸º Markdown æ ¼å¼çš„å›¾ç‰‡
            content += f"\n\n![ç”Ÿæˆçš„å›¾ç‰‡](data:{mime_type};base64,{base64_data})\n\n"

        # å¤„ç†æ™®é€šæ–‡æœ¬
        elif "text" in part:
            content += part.get("text", "")

        # å¤„ç†å·¥å…·è°ƒç”¨
        elif "functionCall" in part:
            tool_index = len(tool_calls_list)
            tool_calls_list.append(convert_to_openai_tool_call(part["functionCall"], index=tool_index))

    # æ‹¼æ¥æ€è€ƒå†…å®¹
    if thinking_content:
        content = f"<think>\n{thinking_content}\n</think>\n{content}"

    # ä½¿ç”¨ OpenAIChatMessage æ¨¡å‹æ„å»ºæ¶ˆæ¯
    message = OpenAIChatMessage(
        role="assistant",
        content=content,
        tool_calls=tool_calls_list if tool_calls_list else None
    )

    # ç¡®å®š finish_reason
    finish_reason = "stop"
    if tool_calls_list:
        finish_reason = "tool_calls"

    finish_reason_raw = response_data.get("response", {}).get("candidates", [{}])[0].get("finishReason")
    if finish_reason_raw == "MAX_TOKENS":
        finish_reason = "length"

    # æå–ä½¿ç”¨ç»Ÿè®¡
    usage_metadata = response_data.get("response", {}).get("usageMetadata", {})
    usage = {
        "prompt_tokens": usage_metadata.get("promptTokenCount", 0),
        "completion_tokens": usage_metadata.get("candidatesTokenCount", 0),
        "total_tokens": usage_metadata.get("totalTokenCount", 0)
    }

    # ä½¿ç”¨ OpenAIChatCompletionChoice æ¨¡å‹
    choice = OpenAIChatCompletionChoice(
        index=0,
        message=message,
        finish_reason=finish_reason
    )

    # ä½¿ç”¨ OpenAIChatCompletionResponse æ¨¡å‹
    response = OpenAIChatCompletionResponse(
        id=request_id,
        object="chat.completion",
        created=int(time.time()),
        model=model,
        choices=[choice],
        usage=usage
    )

    return model_to_dict(response)


def convert_antigravity_response_to_gemini(
    response_data: Dict[str, Any]
) -> Dict[str, Any]:
    """
    å°† Antigravity éæµå¼å“åº”è½¬æ¢ä¸º Gemini æ ¼å¼
    Antigravity çš„å“åº”æ ¼å¼ä¸ Gemini éå¸¸ç›¸ä¼¼ï¼Œåªéœ€è¦æå– response å­—æ®µ
    """
    # Antigravity å“åº”æ ¼å¼: {"response": {...}}
    # Gemini å“åº”æ ¼å¼: {...}
    return response_data.get("response", response_data)


async def convert_antigravity_stream_to_gemini(
    lines_generator: Any,
    stream_ctx: Any,
    client: Any,
    credential_manager: Any,
    credential_name: str
):
    """
    å°† Antigravity æµå¼å“åº”è½¬æ¢ä¸º Gemini æ ¼å¼çš„ SSE æµ

    Args:
        lines_generator: è¡Œç”Ÿæˆå™¨ (å·²ç»è¿‡æ»¤çš„ SSE è¡Œ)
    """
    success_recorded = False

    try:
        async for line in lines_generator:
            if not line or not line.startswith("data: "):
                continue

            # è®°å½•ç¬¬ä¸€æ¬¡æˆåŠŸå“åº”
            if not success_recorded:
                if credential_name and credential_manager:
                    await credential_manager.record_api_call_result(credential_name, True, is_antigravity=True)
                success_recorded = True

            # è§£æ SSE æ•°æ®
            try:
                data = json.loads(line[6:])  # å»æ‰ "data: " å‰ç¼€
            except:
                continue

            # Antigravity æµå¼å“åº”æ ¼å¼: {"response": {...}}
            # Gemini æµå¼å“åº”æ ¼å¼: {...}
            gemini_data = data.get("response", data)

            # å‘é€ Gemini æ ¼å¼çš„æ•°æ®
            yield f"data: {json.dumps(gemini_data)}\n\n"

    except Exception as e:
        log.error(f"[ANTIGRAVITY GEMINI] Streaming error: {e}")
        error_response = {
            "error": {
                "message": str(e),
                "code": 500,
                "status": "INTERNAL"
            }
        }
        yield f"data: {json.dumps(error_response)}\n\n"
    finally:
        # ç¡®ä¿æ¸…ç†æ‰€æœ‰èµ„æº
        try:
            await stream_ctx.__aexit__(None, None, None)
        except Exception as e:
            log.debug(f"[ANTIGRAVITY GEMINI] Error closing stream context: {e}")
        try:
            await client.aclose()
        except Exception as e:
            log.debug(f"[ANTIGRAVITY GEMINI] Error closing client: {e}")


@router.get("/antigravity/v1/models", response_model=ModelList)
async def list_models():
    """è¿”å› OpenAI æ ¼å¼çš„æ¨¡å‹åˆ—è¡¨ - åŠ¨æ€ä» Antigravity API è·å–"""

    try:
        # è·å–å‡­è¯ç®¡ç†å™¨
        cred_mgr = await get_credential_manager()

        # ä» Antigravity API è·å–æ¨¡å‹åˆ—è¡¨ï¼ˆè¿”å› OpenAI æ ¼å¼çš„å­—å…¸åˆ—è¡¨ï¼‰
        models = await fetch_available_models(cred_mgr)

        if not models:
            # å¦‚æœè·å–å¤±è´¥ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
            log.warning("[ANTIGRAVITY] Failed to fetch models from API, returning empty list")
            return ModelList(data=[])

        # models å·²ç»æ˜¯ OpenAI æ ¼å¼çš„å­—å…¸åˆ—è¡¨ï¼Œæ‰©å±•ä¸ºåŒ…å«æŠ—æˆªæ–­ç‰ˆæœ¬
        expanded_models = []
        for model in models:
            # æ·»åŠ åŸå§‹æ¨¡å‹
            expanded_models.append(Model(**model))

            # æ·»åŠ æµå¼æŠ—æˆªæ–­ç‰ˆæœ¬
            anti_truncation_model = model.copy()
            anti_truncation_model["id"] = f"æµå¼æŠ—æˆªæ–­/{model['id']}"
            expanded_models.append(Model(**anti_truncation_model))

        return ModelList(data=expanded_models)

    except Exception as e:
        log.error(f"[ANTIGRAVITY] Error fetching models: {e}")
        # è¿”å›ç©ºåˆ—è¡¨
        return ModelList(data=[])


@router.post("/antigravity/v1/chat/completions")
async def chat_completions(
    request: Request,
    token: str = Depends(authenticate_bearer)
):
    """
    å¤„ç† OpenAI æ ¼å¼çš„èŠå¤©å®Œæˆè¯·æ±‚ï¼Œè½¬æ¢ä¸º Antigravity API
    """
    # âœ… æ£€æµ‹è¯·æ±‚æ¥æºï¼ˆUser-Agentï¼‰ä»¥å†³å®šé™çº§ç­–ç•¥
    user_agent = request.headers.get("user-agent", "").lower()
    log.info(f"[ANTIGRAVITY] Request User-Agent: {user_agent}")

    # åˆ¤æ–­æ˜¯å¦æ˜¯ Claude Code è¯·æ±‚ï¼ˆéœ€è¦è·¨æ± é™çº§ï¼‰
    # Claude Code çš„ User-Agent é€šå¸¸åŒ…å« "claude" æˆ– "anthropic"
    # Cursor çš„ User-Agent é€šå¸¸åŒ…å« "cursor"
    # å¦‚æœæ— æ³•åˆ¤æ–­ï¼Œé»˜è®¤ä¸å¯ç”¨è·¨æ± é™çº§ï¼ˆæ›´å®‰å…¨ï¼‰
    is_claude_code = any(keyword in user_agent for keyword in ["claude", "anthropic"])
    is_cursor = "cursor" in user_agent

    # Claude Code å¯ç”¨è·¨æ± é™çº§ï¼ŒCursor ä¸å¯ç”¨
    enable_cross_pool_fallback = is_claude_code and not is_cursor

    if enable_cross_pool_fallback:
        log.info("[ANTIGRAVITY] Detected Claude Code request - cross-pool fallback ENABLED")
    else:
        log.debug(f"[ANTIGRAVITY] Detected {'Cursor' if is_cursor else 'unknown'} request - cross-pool fallback DISABLED")

    # è·å–åŸå§‹è¯·æ±‚æ•°æ®
    try:
        raw_data = await request.json()
    except Exception as e:
        log.error(f"Failed to parse JSON request: {e}")
        raise HTTPException(status_code=400, detail=f"Invalid JSON: {str(e)}")

    # æ¸…ç†å·¥å…·æ ¼å¼ï¼šç¡®ä¿æ‰€æœ‰å·¥å…·éƒ½æœ‰æ­£ç¡®çš„ç»“æ„
    # ç½‘å…³åº”è¯¥å·²ç»è§„èŒƒåŒ–äº†å·¥å…·ï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§ï¼Œæˆ‘ä»¬å†æ¬¡æ¸…ç†å’ŒéªŒè¯
    if "tools" in raw_data and raw_data["tools"]:
        cleaned_tools = []
        for tool in raw_data["tools"]:
            if isinstance(tool, dict):
                # æ£€æŸ¥å·¥å…·æ˜¯å¦æœ‰æ­£ç¡®çš„ç»“æ„
                has_type = "type" in tool
                has_function = "function" in tool
                has_name = "name" in tool
                
                # Case 1: æ ‡å‡†æ ¼å¼ - å·²ç»æœ‰ type å’Œ function
                if has_type and has_function:
                    cleaned_tools.append(tool)
                # Case 2: Custom ç±»å‹ - éœ€è¦è½¬æ¢
                elif tool.get("type") == "custom":
                    custom_tool = tool.get("custom", {})
                    if isinstance(custom_tool, dict) and "name" in custom_tool:
                        from src.anthropic_converter import clean_json_schema
                        input_schema = custom_tool.get("input_schema", {}) or {}
                        if isinstance(input_schema, dict):
                            cleaned_schema = clean_json_schema(input_schema)
                            if "type" not in cleaned_schema:
                                cleaned_schema["type"] = "object"
                            input_schema = cleaned_schema
                        elif not input_schema:
                            input_schema = {"type": "object", "properties": {}}
                        
                        cleaned_tools.append({
                            "type": "function",
                            "function": {
                                "name": custom_tool.get("name", ""),
                                "description": custom_tool.get("description", ""),
                                "parameters": input_schema
                            }
                        })
                        log.warning(f"[ANTIGRAVITY] Converted custom tool '{custom_tool.get('name')}' to function format (should have been normalized by gateway)")
                    else:
                        log.warning(f"[ANTIGRAVITY] Skipping invalid custom tool: {list(custom_tool.keys()) if isinstance(custom_tool, dict) else 'not a dict'}")
                # Case 3: æ‰å¹³æ ¼å¼ - åªæœ‰ nameï¼Œæ²¡æœ‰ type å’Œ function åŒ…è£…
                # Cursor å¯èƒ½ä½¿ç”¨ input_schema è€Œä¸æ˜¯ parameters
                elif has_name and not has_function:
                    # ä¼˜å…ˆä½¿ç”¨ parametersï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ input_schemaï¼ˆCursor å¯èƒ½ä½¿ç”¨ input_schemaï¼‰
                    parameters = tool.get("parameters")
                    if parameters is None:
                        # Cursor å¯èƒ½ä½¿ç”¨ input_schema è€Œä¸æ˜¯ parameters
                        input_schema = tool.get("input_schema")
                        if input_schema is not None:
                            # ä½¿ç”¨ clean_json_schema æ¸…ç† input_schema
                            from src.anthropic_converter import clean_json_schema
                            if isinstance(input_schema, dict):
                                parameters = clean_json_schema(input_schema)
                                # ç¡®ä¿æœ‰ type å­—æ®µ
                                if "type" not in parameters:
                                    parameters["type"] = "object"
                            else:
                                log.warning(f"[ANTIGRAVITY] Tool '{tool.get('name')}' has non-dict input_schema: {type(input_schema)}, converting to empty dict")
                                parameters = {}
                        else:
                            parameters = {}
                    
                    # ç¡®ä¿ parameters æ˜¯å­—å…¸
                    if not isinstance(parameters, dict):
                        log.warning(f"[ANTIGRAVITY] Tool '{tool.get('name')}' has non-dict parameters: {type(parameters)}, converting to dict")
                        parameters = {}
                    
                    cleaned_tools.append({
                        "type": "function",
                        "function": {
                            "name": tool.get("name", ""),
                            "description": tool.get("description", ""),
                            "parameters": parameters
                        }
                    })
                    log.warning(f"[ANTIGRAVITY] Converted flat format tool '{tool.get('name')}' to standard format (should have been normalized by gateway). Tool keys: {list(tool.keys())}, used_input_schema={'input_schema' in tool}")
                # Case 4: å…¶ä»–æ ¼å¼ - å°è¯•ä¿®å¤æˆ–è·³è¿‡
                else:
                    log.warning(f"[ANTIGRAVITY] Unknown tool format: type={tool.get('type')}, has_function={has_function}, has_name={has_name}, keys={list(tool.keys())}")
                    # å°è¯•ä¿®å¤ï¼šå¦‚æœæœ‰ nameï¼Œå°±åŒ…è£…æˆæ ‡å‡†æ ¼å¼
                    if has_name:
                        # ä¼˜å…ˆä½¿ç”¨ parametersï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ input_schema
                        parameters = tool.get("parameters")
                        if parameters is None:
                            input_schema = tool.get("input_schema")
                            if input_schema is not None:
                                from src.anthropic_converter import clean_json_schema
                                if isinstance(input_schema, dict):
                                    parameters = clean_json_schema(input_schema)
                                    if "type" not in parameters:
                                        parameters["type"] = "object"
                                else:
                                    parameters = {}
                            else:
                                parameters = {}
                        
                        if not isinstance(parameters, dict):
                            parameters = {}
                        
                        cleaned_tools.append({
                            "type": "function",
                            "function": {
                                "name": tool.get("name", ""),
                                "description": tool.get("description", ""),
                                "parameters": parameters
                            }
                        })
                    else:
                        log.warning(f"[ANTIGRAVITY] Skipping tool without name field")
            else:
                # éå­—å…¸ç±»å‹ï¼Œç›´æ¥ä½¿ç”¨ï¼ˆå¯èƒ½æ˜¯ Pydantic æ¨¡å‹ï¼Œç¨åå¤„ç†ï¼‰
                cleaned_tools.append(tool)
        raw_data["tools"] = cleaned_tools

    # åˆ›å»ºè¯·æ±‚å¯¹è±¡
    try:
        request_data = ChatCompletionRequest(**raw_data)
    except Exception as e:
        log.error(f"Request validation failed: {e}")
        raise HTTPException(status_code=400, detail=f"Request validation error: {str(e)}")

    # å¥åº·æ£€æŸ¥
    if (
        len(request_data.messages) == 1
        and getattr(request_data.messages[0], "role", None) == "user"
        and getattr(request_data.messages[0], "content", None) == "Hi"
    ):
        return JSONResponse(
            content={
                "choices": [{"message": {"role": "assistant", "content": "antigravity API æ­£å¸¸å·¥ä½œä¸­"}}]
            }
        )

    # è·å–å‡­è¯ç®¡ç†å™¨
    from src.credential_manager import get_credential_manager
    cred_mgr = await get_credential_manager()

    # æå–å‚æ•°
    model = request_data.model
    messages = request_data.messages
    stream = getattr(request_data, "stream", False)
    tools = getattr(request_data, "tools", None)

    # DEBUG: Log tools format received by Antigravity router
    if tools:
        # å°† Pydantic æ¨¡å‹è½¬æ¢ä¸ºå­—å…¸ä»¥ä¾¿æ£€æŸ¥
        tool_dicts = []
        for tool in tools:
            if not isinstance(tool, dict):
                if hasattr(tool, "model_dump"):
                    tool_dicts.append(tool.model_dump())
                elif hasattr(tool, "dict"):
                    tool_dicts.append(tool.dict())
                else:
                    # å°è¯•ä½¿ç”¨ getattr è·å–å±æ€§
                    tool_dict = {}
                    for attr in ["type", "function", "custom"]:
                        if hasattr(tool, attr):
                            value = getattr(tool, attr)
                            if hasattr(value, "model_dump"):
                                tool_dict[attr] = value.model_dump()
                            elif hasattr(value, "dict"):
                                tool_dict[attr] = value.dict()
                            else:
                                tool_dict[attr] = value
                    tool_dicts.append(tool_dict)
            else:
                tool_dicts.append(tool)
        
        tool_types = [tool.get("type", "unknown") for tool in tool_dicts]
        log.info(f"[ANTIGRAVITY] Received {len(tools)} tools, types={tool_types[:10]}... (showing first 10)")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ custom æ ¼å¼çš„å·¥å…·
        has_custom = any(tool.get("type") == "custom" for tool in tool_dicts)
        if has_custom:
            log.warning(f"[ANTIGRAVITY] WARNING: Received custom tool format! This should have been normalized by gateway.")
            first_custom = next((tool for tool in tool_dicts if tool.get("type") == "custom"), None)
            if first_custom:
                custom_tool = first_custom.get("custom", {})
                input_schema = custom_tool.get("input_schema", {})
                schema_type = input_schema.get("type") if isinstance(input_schema, dict) else type(input_schema).__name__
                log.warning(f"[ANTIGRAVITY] First custom tool: name={custom_tool.get('name')}, input_schema_type={schema_type}, has_properties={'properties' in input_schema if isinstance(input_schema, dict) else False}")
        
        # å°†å·¥å…·è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆå¦‚æœè¿˜ä¸æ˜¯å­—å…¸ï¼‰
        if tool_dicts != tools:
            tools = tool_dicts

    # æ£€æµ‹å¹¶å¤„ç†æŠ—æˆªæ–­æ¨¡å¼
    use_anti_truncation = is_anti_truncation_model(model)
    if use_anti_truncation:
        # å»æ‰ "æµå¼æŠ—æˆªæ–­/" å‰ç¼€
        from src.utils import get_base_model_from_feature_model
        model = get_base_model_from_feature_model(model)

    # æ¨¡å‹åç§°æ˜ å°„
    actual_model = model_mapping(model)
    enable_thinking = is_thinking_model(model)

    # æ£€æŸ¥å†å²æ¶ˆæ¯ä¸­æ˜¯å¦æœ‰æœ‰æ•ˆçš„ thinking blockï¼ˆåŒ…å« signatureï¼‰
    # å¦‚æœ thinking å¯ç”¨ä½†å†å²æ¶ˆæ¯æ²¡æœ‰æœ‰æ•ˆçš„ thinking blockï¼Œåˆ™ç¦ç”¨ thinking
    # è¿™å¯ä»¥é¿å… 400 é”™è¯¯ï¼š"thinking.signature: Field required"
    if enable_thinking:
        has_valid_thinking = False
        for msg in messages:
            # æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦æœ‰ thinking å†…å®¹
            content = getattr(msg, "content", None) if hasattr(msg, "content") else (msg.get("content") if isinstance(msg, dict) else None)
            
            if content:
                # æ£€æŸ¥å­—ç¬¦ä¸²æ ¼å¼çš„ content
                if isinstance(content, str):
                    import re
                    # æ£€æŸ¥æ˜¯å¦æœ‰ thinking æ ‡ç­¾ï¼ˆä½†æ— æ³•éªŒè¯ signatureï¼‰
                    if re.search(r'<(?:redacted_)?reasoning>.*?</(?:redacted_)?reasoning>', content, flags=re.DOTALL | re.IGNORECASE):
                        # æœ‰ thinking æ ‡ç­¾ï¼Œä½†æ— æ³•éªŒè¯ signatureï¼Œå‡è®¾æœ‰æ•ˆ
                        has_valid_thinking = True
                        break
                # æ£€æŸ¥æ•°ç»„æ ¼å¼çš„ content
                elif isinstance(content, list):
                    for item in content:
                        if isinstance(item, dict):
                            item_type = item.get("type")
                            if item_type in ("thinking", "redacted_thinking"):
                                # æ£€æŸ¥æ˜¯å¦æœ‰ signature
                                signature = item.get("signature")
                                if signature and signature.strip():
                                    has_valid_thinking = True
                                    break
                    if has_valid_thinking:
                        break
        
        if not has_valid_thinking:
            log.warning(f"[ANTIGRAVITY] Thinking å·²å¯ç”¨ï¼Œä½†å†å²æ¶ˆæ¯ä¸­æ²¡æœ‰æœ‰æ•ˆçš„ thinking blockï¼ˆåŒ…å« signatureï¼‰ï¼Œç¦ç”¨ thinking æ¨¡å¼ä»¥é¿å… 400 é”™è¯¯")
            enable_thinking = False

    log.info(f"[ANTIGRAVITY] Request: model={model} -> {actual_model}, stream={stream}, thinking={enable_thinking}, anti_truncation={use_anti_truncation}")

    # å¦‚æœ thinking è¢«ç¦ç”¨ï¼Œæ¸…ç†æ¶ˆæ¯ä¸­çš„ thinking å†…å®¹å—
    # è¿™å¯ä»¥é¿å… 400 é”™è¯¯ï¼š"When thinking is disabled, an `assistant` message..."
    if not enable_thinking:
        original_count = len(messages)
        # æ£€æŸ¥æ˜¯å¦æœ‰ thinking å†…å®¹éœ€è¦æ¸…ç†
        has_thinking_before = any(
            (hasattr(msg, "role") and getattr(msg, "role", None) == "assistant" and 
             hasattr(msg, "content") and isinstance(getattr(msg, "content", ""), str) and 
             ("<think>" in getattr(msg, "content", "").lower() or 
              "<think>" in getattr(msg, "content", "").lower() or
              "<think>" in getattr(msg, "content", "").lower()))
            or (isinstance(msg, dict) and msg.get("role") == "assistant" and 
                isinstance(msg.get("content"), str) and 
                ("<think>" in msg.get("content", "").lower() or 
                 "<think>" in msg.get("content", "").lower() or
                 "<think>" in msg.get("content", "").lower()))
            or (isinstance(msg, dict) and msg.get("role") == "assistant" and 
                isinstance(msg.get("content"), list) and
                any(isinstance(item, dict) and item.get("type") in ("thinking", "redacted_thinking") 
                    for item in msg.get("content", [])))
            for msg in messages
        )
        if has_thinking_before:
            messages = strip_thinking_from_openai_messages(messages)
            log.info(f"[ANTIGRAVITY] Thinking å·²ç¦ç”¨ï¼Œå·²æ¸…ç†å†å²æ¶ˆæ¯ä¸­çš„ thinking å†…å®¹å— (messages={original_count})")

    # è½¬æ¢æ¶ˆæ¯æ ¼å¼
    try:
        contents = openai_messages_to_antigravity_contents(messages, enable_thinking=enable_thinking, tools=tools)
        
        # å¦‚æœ thinking å¯ç”¨ï¼Œç¡®ä¿æœ€åä¸€æ¡ assistant æ¶ˆæ¯ä»¥ thinking block å¼€å¤´
        if enable_thinking and contents:
            # æ‰¾åˆ°æœ€åä¸€æ¡ assistant æ¶ˆæ¯ï¼ˆrole="model"ï¼‰
            last_model_index = -1
            for i in range(len(contents) - 1, -1, -1):
                if contents[i].get("role") == "model":
                    last_model_index = i
                    break
            
            if last_model_index >= 0:
                last_model = contents[last_model_index]
                parts = last_model.get("parts", [])
                if parts:
                    first_part = parts[0]
                    # æ£€æŸ¥ç¬¬ä¸€ä¸ª part æ˜¯å¦æ˜¯ thinking block
                    if not isinstance(first_part, dict) or first_part.get("thought") is not True:
                        # æ²¡æœ‰ thinking blockï¼Œéœ€è¦æ·»åŠ ä¸€ä¸ª
                        # å°è¯•ä»ä¹‹å‰çš„æ¶ˆæ¯ä¸­æå– thinking block
                        thinking_part = None
                        for i in range(last_model_index - 1, -1, -1):
                            prev_msg = contents[i]
                            if prev_msg.get("role") == "model":
                                prev_parts = prev_msg.get("parts", [])
                                for part in prev_parts:
                                    if isinstance(part, dict) and part.get("thought") is True:
                                        # æ£€æŸ¥ signature æ˜¯å¦æœ‰æ•ˆï¼ˆéç©ºï¼‰
                                        signature = part.get("thoughtSignature", "")
                                        if signature and signature.strip():
                                            # æ‰¾åˆ°æœ‰æ•ˆçš„ thinking blockï¼Œå¤åˆ¶å®ƒ
                                            thinking_part = {
                                                "text": part.get("text", ""),
                                                "thought": True,
                                                "thoughtSignature": signature
                                            }
                                            break
                                if thinking_part:
                                    break
                        
                        if thinking_part:
                            # åœ¨å¼€å¤´æ’å…¥ thinking block
                            parts.insert(0, thinking_part)
                            log.info(f"[ANTIGRAVITY] Added thinking block to last assistant message from previous message")
                        else:
                            # æ— æ³•ä»ä¹‹å‰çš„æ¶ˆæ¯ä¸­æå–æœ‰æ•ˆçš„ thinking blockï¼ˆåŒ…å« signatureï¼‰
                            # æ ¹æ®ä¹‹å‰çš„æ£€æŸ¥ï¼Œå¦‚æœå†å²æ¶ˆæ¯æ²¡æœ‰æœ‰æ•ˆçš„ thinking blockï¼Œenable_thinking åº”è¯¥å·²ç»è¢«ç¦ç”¨
                            # ä½†ä¸ºäº†å®‰å…¨èµ·è§ï¼Œå¦‚æœç¡®å®æ²¡æœ‰ï¼Œç¦ç”¨ thinking å¹¶æ¸…ç†æ¶ˆæ¯
                            log.warning(f"[ANTIGRAVITY] Last assistant message does not start with thinking block, cannot find previous thinking block with valid signature. Disabling thinking mode.")
                            enable_thinking = False
                            # æ¸…ç†æ¶ˆæ¯ä¸­çš„ thinking å†…å®¹å—
                            messages = strip_thinking_from_openai_messages(messages)
                            # é‡æ–°è½¬æ¢æ¶ˆæ¯ï¼ˆä¸ä½¿ç”¨ thinkingï¼‰
                            contents = openai_messages_to_antigravity_contents(messages, enable_thinking=False, tools=tools)
                            # æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦ breakï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»é‡æ–°è½¬æ¢äº†æ¶ˆæ¯
                            # åç»­ä»£ç ä¼šä½¿ç”¨æ–°çš„ contents
                    # æ›´æ–° partsï¼ˆå¦‚æœæœ‰ä¿®æ”¹ï¼‰
                    if thinking_part:
                        last_model["parts"] = parts
    except Exception as e:
        log.error(f"Failed to convert messages: {e}")
        raise HTTPException(status_code=500, detail=f"Message conversion failed: {str(e)}")

    # âœ… æ–°å¢ï¼šéªŒè¯ contents æ˜¯å¦ä¸ºç©ºï¼ˆæ ¹æœ¬åŸå› ä¿®å¤ï¼‰
    # å‚è€ƒï¼šantigravity_anthropic_router.py:570 å’Œ gemini_generate_content çš„éªŒè¯é€»è¾‘
    if not contents:
        log.error(f"[ANTIGRAVITY] Contents is empty after conversion! Messages count: {len(messages)}")
        # è®°å½•åŸå§‹æ¶ˆæ¯ï¼Œå¸®åŠ©æ’æŸ¥é—®é¢˜
        for i, msg in enumerate(messages[:5]):  # åªè®°å½•å‰5æ¡
            role = getattr(msg, "role", None) if hasattr(msg, "role") else (msg.get("role") if isinstance(msg, dict) else None)
            content_val = getattr(msg, "content", None) if hasattr(msg, "content") else (msg.get("content") if isinstance(msg, dict) else None)
            content_len = len(str(content_val)) if content_val else 0
            log.error(f"[ANTIGRAVITY] Message {i}: role={role}, content_type={type(content_val).__name__}, content_length={content_len}")

        raise HTTPException(
            status_code=400,
            detail="Messages converted to empty contents. Please ensure messages contain valid user content (non-empty text or images). This usually happens when: 1) Only system messages are provided, 2) User message content is empty, 3) Message format is invalid."
        )

    # éªŒè¯æ¯ä¸ª content çš„ parts ä¸ä¸ºç©º
    for i, content_item in enumerate(contents):
        parts = content_item.get("parts", [])
        if not parts:
            log.error(f"[ANTIGRAVITY] Content {i} (role={content_item.get('role')}) has empty parts!")
            raise HTTPException(
                status_code=400,
                detail=f"Content at index {i} (role={content_item.get('role')}) has empty parts. Please ensure all messages contain valid content."
            )

    # è½¬æ¢å·¥å…·å®šä¹‰
    antigravity_tools = convert_openai_tools_to_antigravity(tools)

    # ç”Ÿæˆé…ç½®å‚æ•°
    parameters = {
        "temperature": getattr(request_data, "temperature", None),
        "top_p": getattr(request_data, "top_p", None),
        "max_tokens": getattr(request_data, "max_tokens", None),
    }
    # è¿‡æ»¤ None å€¼
    parameters = {k: v for k, v in parameters.items() if v is not None}

    # ä½¿ç”¨æ›´æ–°åçš„ enable_thinkingï¼ˆå¯èƒ½å·²è¢«ç¦ç”¨ï¼‰
    generation_config = generate_generation_config(parameters, enable_thinking, actual_model)

    # è·å–å‡­è¯ä¿¡æ¯ï¼ˆç”¨äº project_id å’Œ session_idï¼‰
    cred_result = await cred_mgr.get_valid_credential(is_antigravity=True)
    if not cred_result:
        log.error("å½“å‰æ— å¯ç”¨ antigravity å‡­è¯")
        raise HTTPException(status_code=500, detail="å½“å‰æ— å¯ç”¨ antigravity å‡­è¯")

    _, credential_data = cred_result
    project_id = credential_data.get("project_id", "default-project")
    session_id = f"session-{uuid.uuid4().hex}"

    # âœ… æ–°å¢ï¼šä¼°ç®—è¾“å…¥ token æ•°ï¼ˆç²—ç•¥ä¼°ç®—ï¼Œç”¨äºæ£€æµ‹ä¸Šä¸‹æ–‡è¿‡é•¿ï¼‰
    def estimate_input_tokens(contents: List[Dict[str, Any]]) -> int:
        """ç²—ç•¥ä¼°ç®—è¾“å…¥ token æ•°"""
        total_chars = 0
        for content in contents:
            parts = content.get("parts", [])
            for part in parts:
                if "text" in part:
                    total_chars += len(str(part["text"]))
                elif "functionResponse" in part:
                    # å·¥å…·ç»“æœå¯èƒ½å¾ˆå¤§
                    response = part.get("functionResponse", {}).get("response", {})
                    output = response.get("output", "")
                    total_chars += len(str(output))
        # ç²—ç•¥ä¼°ç®—ï¼š1 token â‰ˆ 4 å­—ç¬¦
        return total_chars // 4
    
    estimated_tokens = estimate_input_tokens(contents)
    if estimated_tokens > 50000:  # 50K tokens é˜ˆå€¼
        log.warning(f"[ANTIGRAVITY] Large input context detected: ~{estimated_tokens} tokens. "
                   f"This may cause API to return empty response or stream truncation. "
                   f"Consider: 1) Reducing tool result size, 2) Using non-streaming request, 3) Splitting context")

    # æ„å»º Antigravity è¯·æ±‚ä½“
    request_body = build_antigravity_request_body(
        contents=contents,
        model=actual_model,
        project_id=project_id,
        session_id=session_id,
        tools=antigravity_tools,
        generation_config=generation_config,
    )

    # âœ… æ–°å¢ï¼šå¢å¼ºè¯Šæ–­æ—¥å¿—ï¼ˆç‰¹åˆ«æ˜¯å·¥å…·è°ƒç”¨åœºæ™¯ï¼‰
    log.info(f"[ANTIGRAVITY DEBUG] Request summary: contents_count={len(contents)}, "
             f"has_tools={bool(antigravity_tools)}, model={actual_model}, "
             f"enable_thinking={enable_thinking}")
    
    # âœ… æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯å·¥å…·è°ƒç”¨åçš„è¯·æ±‚
    has_tool_messages = any(
        content.get("role") == "user" and any(
            part.get("functionResponse") for part in content.get("parts", [])
        )
        for content in contents
    )
    # âœ… æ–°å¢ï¼šç»Ÿè®¡å·¥å…·ç»“æœä¿¡æ¯ï¼Œç”¨äºä¼ é€’ç»™é”™è¯¯æ¶ˆæ¯
    tool_result_count = 0
    total_tool_results_length = 0
    if has_tool_messages:
        log.info(f"[ANTIGRAVITY DEBUG] This is a tool-call-follow-up request")
        # è®°å½•å·¥å…·æ¶ˆæ¯çš„è¯¦ç»†ä¿¡æ¯å¹¶ç»Ÿè®¡
        for i, content in enumerate(contents):
            if content.get("role") == "user":
                parts = content.get("parts", [])
                for j, part in enumerate(parts):
                    if "functionResponse" in part:
                        fr = part["functionResponse"]
                        output = str(fr.get("response", {}).get("output", ""))
                        tool_result_count += 1
                        total_tool_results_length += len(output)
                        log.info(f"[ANTIGRAVITY DEBUG] Tool result {i}-{j}: "
                               f"id={fr.get('id')}, name={fr.get('name')}, "
                               f"output_type={type(fr.get('response', {}).get('output')).__name__}, "
                               f"output_length={len(output)}")
    
    # âœ… æ–°å¢ï¼šæ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”¨äºä¼ é€’ç»™é”™è¯¯æ¶ˆæ¯
    context_info = {
        "estimated_tokens": estimated_tokens,
        "tool_result_count": tool_result_count,
        "total_tool_results_length": total_tool_results_length,
        "has_tool_messages": has_tool_messages
    }

    # ç”Ÿæˆè¯·æ±‚ ID
    request_id = f"chatcmpl-{int(time.time() * 1000)}"

    # âœ… ä¼˜åŒ–ï¼šæ£€æµ‹ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå¦‚æœè¿‡é•¿ä¸”æ˜¯æµå¼è¯·æ±‚ï¼Œè€ƒè™‘ä½¿ç”¨éæµå¼ fallback
    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ estimated_tokens ä½œä¸ºåˆæ­¥åˆ¤æ–­ï¼Œå®é™…å¤„ç†çš„ tokens å¯èƒ½åœ¨æµå¼è¯·æ±‚ä¸­åŠ¨æ€æ£€æµ‹
    # éæµå¼è¯·æ±‚é€šå¸¸å¯¹é•¿ä¸Šä¸‹æ–‡æ›´ç¨³å®šï¼Œä½†ä¼šå¤±å»å®æ—¶åé¦ˆçš„ä¼˜åŠ¿
    USE_NON_STREAMING_FALLBACK_THRESHOLD = 60000  # 60K tokens é˜ˆå€¼ï¼ˆåŸºäº estimated_tokensï¼‰
    # æ³¨æ„ï¼šå®é™…å¤„ç†çš„ tokens é˜ˆå€¼ï¼ˆ50Kï¼‰åœ¨æµå¼è½¬æ¢å™¨ä¸­åŠ¨æ€æ£€æµ‹
    should_use_non_streaming_fallback = (
        stream and
        estimated_tokens > USE_NON_STREAMING_FALLBACK_THRESHOLD and
        has_tool_messages  # åªåœ¨å·¥å…·è°ƒç”¨åœºæ™¯ä¸‹å¯ç”¨ fallback
    )

    if should_use_non_streaming_fallback:
        log.warning(f"[ANTIGRAVITY] Large estimated context ({estimated_tokens:,} tokens) with tool calls detected. "
                   f"Using non-streaming request as fallback for better stability. "
                   f"Note: If API caches content, actual processed tokens may be lower. "
                   f"Streaming request will dynamically check actual processed tokens and fallback if needed.")
        # å¯ä»¥é€‰æ‹©ç›´æ¥åˆ‡æ¢åˆ°éæµå¼ï¼Œæˆ–è€…è®©æµå¼è¯·æ±‚è‡ªå·±å¤„ç†
        # ä¸ºäº†ä¿æŒå®æ—¶åé¦ˆï¼Œæˆ‘ä»¬è®©æµå¼è¯·æ±‚è‡ªå·±å¤„ç†ï¼Œåªåœ¨æç«¯æƒ…å†µä¸‹æ‰ç›´æ¥åˆ‡æ¢
        # stream = False  # å–æ¶ˆç›´æ¥åˆ‡æ¢ï¼Œè®©æµå¼è¯·æ±‚åŠ¨æ€å¤„ç†

    # ====================== å¸¦æ¨¡å‹é™çº§çš„è¯·æ±‚å‘é€ ======================
    # è·å–å½“å‰æ¨¡å‹çš„é™çº§é“¾
    fallback_models = get_fallback_models(actual_model)
    models_to_try = [actual_model] + fallback_models

    # å‘é€è¯·æ±‚ï¼ˆå¸¦æ¨¡å‹é™çº§ï¼‰
    last_error = None
    for attempt_idx, attempt_model in enumerate(models_to_try):
        try:
            # å¦‚æœä¸æ˜¯ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼Œæ›´æ–°è¯·æ±‚ä½“ä¸­çš„æ¨¡å‹
            if attempt_idx > 0:
                log.warning(f"[ANTIGRAVITY FALLBACK] æ¨¡å‹é™çº§: {actual_model} -> {attempt_model} (å°è¯• {attempt_idx + 1}/{len(models_to_try)})")
                request_body["model"] = attempt_model

            if stream:
                # å¤„ç†æŠ—æˆªæ–­åŠŸèƒ½ï¼ˆä»…æµå¼ä¼ è¾“æ—¶æœ‰æ•ˆï¼‰
                if use_anti_truncation:
                    log.info("[ANTIGRAVITY] å¯ç”¨æµå¼æŠ—æˆªæ–­åŠŸèƒ½")
                    max_attempts = await get_anti_truncation_max_attempts()

                    # åŒ…è£…è¯·æ±‚å‡½æ•°ä»¥é€‚é…æŠ—æˆªæ–­å¤„ç†å™¨
                    async def antigravity_request_func(payload):
                        resources, cred_name, cred_data = await send_antigravity_request_stream(
                            payload, cred_mgr, enable_cross_pool_fallback=enable_cross_pool_fallback
                        )
                        response, stream_ctx, client = resources
                        return StreamingResponse(
                            convert_antigravity_stream_to_openai(
                                response, stream_ctx, client, model, request_id, cred_mgr, cred_name,
                                request_body=request_body,  # ä¼ é€’è¯·æ±‚ä½“ç”¨äº fallback
                                cred_mgr=cred_mgr,  # ä¼ é€’å‡­è¯ç®¡ç†å™¨ç”¨äº fallback
                                context_info=context_info  # âœ… æ–°å¢ï¼šä¼ é€’ä¸Šä¸‹æ–‡ä¿¡æ¯ç”¨äºé”™è¯¯æ¶ˆæ¯
                            ),
                            media_type="text/event-stream"
                        )

                    return await apply_anti_truncation_to_stream(
                        antigravity_request_func, request_body, max_attempts
                    )

                # æµå¼è¯·æ±‚ï¼ˆæ— æŠ—æˆªæ–­ï¼‰
                resources, cred_name, cred_data = await send_antigravity_request_stream(
                    request_body, cred_mgr, enable_cross_pool_fallback=enable_cross_pool_fallback
                )
                # resources æ˜¯ä¸€ä¸ªå…ƒç»„: (response, stream_ctx, client)
                response, stream_ctx, client = resources

                # è½¬æ¢å¹¶è¿”å›æµå¼å“åº”,ä¼ é€’èµ„æºç®¡ç†å¯¹è±¡
                # response ç°åœ¨æ˜¯ filtered_lines ç”Ÿæˆå™¨
                # âœ… æ–°å¢ï¼šä¼ é€’è¯·æ±‚ä½“å’Œå‡­è¯ç®¡ç†å™¨ç”¨äº fallbackï¼Œä»¥åŠä¸Šä¸‹æ–‡ä¿¡æ¯ç”¨äºé”™è¯¯æ¶ˆæ¯
                return StreamingResponse(
                    convert_antigravity_stream_to_openai(
                        response, stream_ctx, client, model, request_id, cred_mgr, cred_name,
                        request_body=request_body,  # ä¼ é€’è¯·æ±‚ä½“ç”¨äº fallback
                        cred_mgr=cred_mgr,  # ä¼ é€’å‡­è¯ç®¡ç†å™¨ç”¨äº fallback
                        context_info=context_info  # âœ… æ–°å¢ï¼šä¼ é€’ä¸Šä¸‹æ–‡ä¿¡æ¯ç”¨äºé”™è¯¯æ¶ˆæ¯
                    ),
                    media_type="text/event-stream"
                )
            else:
                # éæµå¼è¯·æ±‚
                response_data, cred_name, cred_data = await send_antigravity_request_no_stream(
                    request_body, cred_mgr
                )

                # è½¬æ¢å¹¶è¿”å›å“åº”
                openai_response = convert_antigravity_response_to_openai(response_data, model, request_id)
                return JSONResponse(content=openai_response)

        except Exception as e:
            last_error = e
            error_msg = str(e)
            log.error(f"[ANTIGRAVITY] Request failed with model {attempt_model}: {error_msg}")

            # ====================== ä½¿ç”¨æ–°çš„æ™ºèƒ½é™çº§é€»è¾‘ ======================
            from .fallback_manager import (
                is_quota_exhausted_error, is_retryable_error, is_403_error,
                is_credential_unavailable_error,
                get_cross_pool_fallback, is_haiku_model, HAIKU_FALLBACK_TARGET
            )

            # 1. 403 é”™è¯¯ - éœ€è¦è§¦å‘å‡­è¯éªŒè¯ï¼ˆæš‚æ—¶ç›´æ¥å¤±è´¥ï¼Œåç»­å¯æ·»åŠ éªŒè¯é€»è¾‘ï¼‰
            if is_403_error(error_msg):
                log.warning(f"[ANTIGRAVITY FALLBACK] æ£€æµ‹åˆ° 403 é”™è¯¯ï¼Œéœ€è¦éªŒè¯å‡­è¯")
                raise HTTPException(status_code=403, detail=f"Credential verification required: {error_msg}")

            # 2. å¯é‡è¯•é”™è¯¯ï¼ˆ400, æ™®é€š429, 5xxï¼‰- ä¸é™çº§ï¼Œç›´æ¥å¤±è´¥è®©å®¢æˆ·ç«¯é‡è¯•
            if is_retryable_error(error_msg):
                log.info(f"[ANTIGRAVITY] å¯é‡è¯•é”™è¯¯ï¼Œä¸è§¦å‘é™çº§ï¼Œè¿”å›é”™è¯¯è®©å®¢æˆ·ç«¯é‡è¯•")
                raise HTTPException(status_code=500, detail=f"Antigravity API request failed (retryable): {error_msg}")

            # 3. é¢åº¦ç”¨å°½é”™è¯¯ - è§¦å‘è·¨æ± é™çº§
            if is_quota_exhausted_error(error_msg):
                log.warning(f"[ANTIGRAVITY FALLBACK] æ£€æµ‹åˆ°é¢åº¦ç”¨å°½é”™è¯¯")

                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰é™çº§æ¨¡å‹å¯ç”¨
                if attempt_idx < len(models_to_try) - 1:
                    log.warning(f"[ANTIGRAVITY FALLBACK] å°†å°è¯•ä¸‹ä¸€ä¸ªé™çº§æ¨¡å‹")
                    continue
                else:
                    # å·²ç»å°è¯•è¿‡é™çº§ï¼Œæ£€æŸ¥æ˜¯å¦å¯ä»¥è·¯ç”±åˆ° Copilot
                    log.error(f"[ANTIGRAVITY FALLBACK] æ‰€æœ‰é™çº§æ¨¡å‹å‡å·²å°è¯•")
                    # TODO: åç»­å¯æ·»åŠ  Copilot è·¯ç”±é€»è¾‘
                    raise HTTPException(
                        status_code=429,
                        detail=f"All quota pools exhausted. Original error: {error_msg}"
                    )

            # 4. å‡­è¯ä¸å¯ç”¨é”™è¯¯ - è¿”å› 503 è®© Gateway è·¯ç”±åˆ° Copilot
            if is_credential_unavailable_error(error_msg):
                log.warning(f"[ANTIGRAVITY] å‡­è¯ä¸å¯ç”¨ï¼Œè¿”å› 503 è®© Gateway è·¯ç”±åˆ°å¤‡ç”¨åç«¯")
                raise HTTPException(status_code=503, detail=f"Antigravity credentials unavailable: {error_msg}")

            # 5. å…¶ä»–é”™è¯¯ - ç›´æ¥å¤±è´¥
            log.info(f"[ANTIGRAVITY] æœªçŸ¥é”™è¯¯ç±»å‹ï¼Œä¸è§¦å‘é™çº§")
            raise HTTPException(status_code=500, detail=f"Antigravity API request failed: {error_msg}")

    # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥äº†
    log.error(f"[ANTIGRAVITY FALLBACK] æ‰€æœ‰æ¨¡å‹å‡å¤±è´¥: {last_error}")
    raise HTTPException(status_code=500, detail=f"Antigravity API request failed (å·²å°è¯•æ‰€æœ‰é™çº§æ¨¡å‹): {str(last_error)}")


# ==================== Gemini æ ¼å¼ API ç«¯ç‚¹ ====================

@router.get("/antigravity/v1beta/models")
@router.get("/antigravity/v1/models")
async def gemini_list_models(api_key: str = Depends(authenticate_gemini_flexible)):
    """è¿”å› Gemini æ ¼å¼çš„æ¨¡å‹åˆ—è¡¨ - åŠ¨æ€ä» Antigravity API è·å–"""

    try:
        # è·å–å‡­è¯ç®¡ç†å™¨
        cred_mgr = await get_credential_manager()

        # ä» Antigravity API è·å–æ¨¡å‹åˆ—è¡¨ï¼ˆè¿”å› OpenAI æ ¼å¼çš„å­—å…¸åˆ—è¡¨ï¼‰
        models = await fetch_available_models(cred_mgr)

        if not models:
            # å¦‚æœè·å–å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
            log.warning("[ANTIGRAVITY GEMINI] Failed to fetch models from API, returning empty list")
            return JSONResponse(content={"models": []})

        # å°† OpenAI æ ¼å¼è½¬æ¢ä¸º Gemini æ ¼å¼ï¼ŒåŒæ—¶æ·»åŠ æŠ—æˆªæ–­ç‰ˆæœ¬
        gemini_models = []
        for model in models:
            model_id = model.get("id", "")

            # æ·»åŠ åŸå§‹æ¨¡å‹
            gemini_models.append({
                "name": f"models/{model_id}",
                "version": "001",
                "displayName": model_id,
                "description": f"Antigravity API - {model_id}",
                "supportedGenerationMethods": ["generateContent", "streamGenerateContent"],
            })

            # æ·»åŠ æµå¼æŠ—æˆªæ–­ç‰ˆæœ¬
            anti_truncation_id = f"æµå¼æŠ—æˆªæ–­/{model_id}"
            gemini_models.append({
                "name": f"models/{anti_truncation_id}",
                "version": "001",
                "displayName": anti_truncation_id,
                "description": f"Antigravity API - {anti_truncation_id} (å¸¦æµå¼æŠ—æˆªæ–­åŠŸèƒ½)",
                "supportedGenerationMethods": ["generateContent", "streamGenerateContent"],
            })

        return JSONResponse(content={"models": gemini_models})

    except Exception as e:
        log.error(f"[ANTIGRAVITY GEMINI] Error fetching models: {e}")
        # è¿”å›ç©ºåˆ—è¡¨
        return JSONResponse(content={"models": []})


@router.post("/antigravity/v1beta/models/{model:path}:generateContent")
@router.post("/antigravity/v1/models/{model:path}:generateContent")
async def gemini_generate_content(
    model: str = Path(..., description="Model name"),
    request: Request = None,
    api_key: str = Depends(authenticate_gemini_flexible),
):
    """å¤„ç† Gemini æ ¼å¼çš„éæµå¼å†…å®¹ç”Ÿæˆè¯·æ±‚ï¼ˆé€šè¿‡ Antigravity APIï¼‰"""
    log.debug(f"[ANTIGRAVITY GEMINI] Non-streaming request for model: {model}")

    # è·å–åŸå§‹è¯·æ±‚æ•°æ®
    try:
        request_data = await request.json()
    except Exception as e:
        log.error(f"Failed to parse JSON request: {e}")
        raise HTTPException(status_code=400, detail=f"Invalid JSON: {str(e)}")

    # éªŒè¯å¿…è¦å­—æ®µ
    if "contents" not in request_data or not request_data["contents"]:
        raise HTTPException(status_code=400, detail="Missing required field: contents")

    # å¥åº·æ£€æŸ¥
    if (
        len(request_data["contents"]) == 1
        and request_data["contents"][0].get("role") == "user"
        and request_data["contents"][0].get("parts", [{}])[0].get("text") == "Hi"
    ):
        return JSONResponse(
            content={
                "candidates": [
                    {
                        "content": {"parts": [{"text": "antigravity API æ­£å¸¸å·¥ä½œä¸­"}], "role": "model"},
                        "finishReason": "STOP",
                        "index": 0,
                    }
                ]
            }
        )

    # è·å–å‡­è¯ç®¡ç†å™¨
    from src.credential_manager import get_credential_manager
    cred_mgr = await get_credential_manager()

    # æå–æ¨¡å‹åç§°ï¼ˆç§»é™¤ "models/" å‰ç¼€ï¼‰
    if model.startswith("models/"):
        model = model[7:]

    # æ£€æµ‹å¹¶å¤„ç†æŠ—æˆªæ–­æ¨¡å¼ï¼ˆè™½ç„¶éæµå¼ä¸ä¼šä½¿ç”¨ï¼Œä½†è¦å¤„ç†æ¨¡å‹åï¼‰
    use_anti_truncation = is_anti_truncation_model(model)
    if use_anti_truncation:
        # å»æ‰ "æµå¼æŠ—æˆªæ–­/" å‰ç¼€
        from src.utils import get_base_model_from_feature_model
        model = get_base_model_from_feature_model(model)

    # æ¨¡å‹åç§°æ˜ å°„
    actual_model = model_mapping(model)
    enable_thinking = is_thinking_model(model)

    log.info(f"[ANTIGRAVITY GEMINI] Request: model={model} -> {actual_model}, thinking={enable_thinking}")

    # è½¬æ¢ Gemini contents ä¸º Antigravity contents
    try:
        contents = gemini_contents_to_antigravity_contents(request_data["contents"])
    except Exception as e:
        log.error(f"Failed to convert Gemini contents: {e}")
        raise HTTPException(status_code=500, detail=f"Message conversion failed: {str(e)}")

    # æå– Gemini generationConfig
    gemini_config = request_data.get("generationConfig", {})

    # è½¬æ¢ä¸º Antigravity generation_config
    parameters = {
        "temperature": gemini_config.get("temperature"),
        "top_p": gemini_config.get("topP"),
        "top_k": gemini_config.get("topK"),
        "max_tokens": gemini_config.get("maxOutputTokens"),
        # å›¾ç‰‡ç”Ÿæˆç›¸å…³å‚æ•°
        "response_modalities": gemini_config.get("response_modalities"),
        "image_config": gemini_config.get("image_config"),
    }
    # è¿‡æ»¤ None å€¼
    parameters = {k: v for k, v in parameters.items() if v is not None}

    generation_config = generate_generation_config(parameters, enable_thinking, actual_model)

    # è·å–å‡­è¯ä¿¡æ¯ï¼ˆç”¨äº project_id å’Œ session_idï¼‰
    cred_result = await cred_mgr.get_valid_credential(is_antigravity=True)
    if not cred_result:
        log.error("å½“å‰æ— å¯ç”¨ antigravity å‡­è¯")
        raise HTTPException(status_code=500, detail="å½“å‰æ— å¯ç”¨ antigravity å‡­è¯")

    _, credential_data = cred_result
    project_id = credential_data.get("project_id", "default-project")
    session_id = credential_data.get("session_id", f"session-{uuid.uuid4().hex}")

    # å¤„ç† systemInstruction
    system_instruction = None
    if "systemInstruction" in request_data:
        system_instruction = request_data["systemInstruction"]

    # å¤„ç† tools
    antigravity_tools = None
    if "tools" in request_data:
        # Gemini å’Œ Antigravity çš„ tools æ ¼å¼åŸºæœ¬ä¸€è‡´
        antigravity_tools = request_data["tools"]

    # æ„å»º Antigravity è¯·æ±‚ä½“
    request_body = build_antigravity_request_body(
        contents=contents,
        model=actual_model,
        project_id=project_id,
        session_id=session_id,
        system_instruction=system_instruction,
        tools=antigravity_tools,
        generation_config=generation_config,
    )

    # å‘é€éæµå¼è¯·æ±‚
    try:
        response_data, cred_name, cred_data = await send_antigravity_request_no_stream(
            request_body, cred_mgr
        )

        # è½¬æ¢å¹¶è¿”å› Gemini æ ¼å¼å“åº”
        gemini_response = convert_antigravity_response_to_gemini(response_data)
        return JSONResponse(content=gemini_response)

    except Exception as e:
        log.error(f"[ANTIGRAVITY GEMINI] Request failed: {e}")
        raise HTTPException(status_code=500, detail=f"Antigravity API request failed: {str(e)}")


@router.post("/antigravity/v1beta/models/{model:path}:streamGenerateContent")
@router.post("/antigravity/v1/models/{model:path}:streamGenerateContent")
async def gemini_stream_generate_content(
    model: str = Path(..., description="Model name"),
    request: Request = None,
    api_key: str = Depends(authenticate_gemini_flexible),
):
    """å¤„ç† Gemini æ ¼å¼çš„æµå¼å†…å®¹ç”Ÿæˆè¯·æ±‚ï¼ˆé€šè¿‡ Antigravity APIï¼‰"""
    log.debug(f"[ANTIGRAVITY GEMINI] Streaming request for model: {model}")

    # è·å–åŸå§‹è¯·æ±‚æ•°æ®
    try:
        request_data = await request.json()
    except Exception as e:
        log.error(f"Failed to parse JSON request: {e}")
        raise HTTPException(status_code=400, detail=f"Invalid JSON: {str(e)}")

    # éªŒè¯å¿…è¦å­—æ®µ
    if "contents" not in request_data or not request_data["contents"]:
        raise HTTPException(status_code=400, detail="Missing required field: contents")

    # è·å–å‡­è¯ç®¡ç†å™¨
    from src.credential_manager import get_credential_manager
    cred_mgr = await get_credential_manager()

    # æå–æ¨¡å‹åç§°ï¼ˆç§»é™¤ "models/" å‰ç¼€ï¼‰
    if model.startswith("models/"):
        model = model[7:]

    # æ£€æµ‹å¹¶å¤„ç†æŠ—æˆªæ–­æ¨¡å¼
    use_anti_truncation = is_anti_truncation_model(model)
    if use_anti_truncation:
        # å»æ‰ "æµå¼æŠ—æˆªæ–­/" å‰ç¼€
        from src.utils import get_base_model_from_feature_model
        model = get_base_model_from_feature_model(model)

    # æ¨¡å‹åç§°æ˜ å°„
    actual_model = model_mapping(model)
    enable_thinking = is_thinking_model(model)

    log.info(f"[ANTIGRAVITY GEMINI] Stream request: model={model} -> {actual_model}, thinking={enable_thinking}, anti_truncation={use_anti_truncation}")

    # è½¬æ¢ Gemini contents ä¸º Antigravity contents
    try:
        contents = gemini_contents_to_antigravity_contents(request_data["contents"])
    except Exception as e:
        log.error(f"Failed to convert Gemini contents: {e}")
        raise HTTPException(status_code=500, detail=f"Message conversion failed: {str(e)}")

    # æå– Gemini generationConfig
    gemini_config = request_data.get("generationConfig", {})

    # è½¬æ¢ä¸º Antigravity generation_config
    parameters = {
        "temperature": gemini_config.get("temperature"),
        "top_p": gemini_config.get("topP"),
        "top_k": gemini_config.get("topK"),
        "max_tokens": gemini_config.get("maxOutputTokens"),
        # å›¾ç‰‡ç”Ÿæˆç›¸å…³å‚æ•°
        "response_modalities": gemini_config.get("response_modalities"),
        "image_config": gemini_config.get("image_config"),
    }
    # è¿‡æ»¤ None å€¼
    parameters = {k: v for k, v in parameters.items() if v is not None}

    generation_config = generate_generation_config(parameters, enable_thinking, actual_model)

    # è·å–å‡­è¯ä¿¡æ¯ï¼ˆç”¨äº project_id å’Œ session_idï¼‰
    cred_result = await cred_mgr.get_valid_credential(is_antigravity=True)
    if not cred_result:
        log.error("å½“å‰æ— å¯ç”¨ antigravity å‡­è¯")
        raise HTTPException(status_code=500, detail="å½“å‰æ— å¯ç”¨ antigravity å‡­è¯")

    _, credential_data = cred_result
    project_id = credential_data.get("project_id", "default-project")
    session_id = credential_data.get("session_id", f"session-{uuid.uuid4().hex}")

    # å¤„ç† systemInstruction
    system_instruction = None
    if "systemInstruction" in request_data:
        system_instruction = request_data["systemInstruction"]

    # å¤„ç† tools
    antigravity_tools = None
    if "tools" in request_data:
        # Gemini å’Œ Antigravity çš„ tools æ ¼å¼åŸºæœ¬ä¸€è‡´
        antigravity_tools = request_data["tools"]

    # æ„å»º Antigravity è¯·æ±‚ä½“
    request_body = build_antigravity_request_body(
        contents=contents,
        model=actual_model,
        project_id=project_id,
        session_id=session_id,
        system_instruction=system_instruction,
        tools=antigravity_tools,
        generation_config=generation_config,
    )

    # å‘é€æµå¼è¯·æ±‚
    try:
        # å¤„ç†æŠ—æˆªæ–­åŠŸèƒ½ï¼ˆä»…æµå¼ä¼ è¾“æ—¶æœ‰æ•ˆï¼‰
        if use_anti_truncation:
            log.info("[ANTIGRAVITY GEMINI] å¯ç”¨æµå¼æŠ—æˆªæ–­åŠŸèƒ½")
            max_attempts = await get_anti_truncation_max_attempts()

            # åŒ…è£…è¯·æ±‚å‡½æ•°ä»¥é€‚é…æŠ—æˆªæ–­å¤„ç†å™¨
            async def antigravity_gemini_request_func(payload):
                resources, cred_name, cred_data = await send_antigravity_request_stream(
                    payload, cred_mgr
                )
                response, stream_ctx, client = resources
                return StreamingResponse(
                    convert_antigravity_stream_to_gemini(
                        response, stream_ctx, client, cred_mgr, cred_name
                    ),
                    media_type="text/event-stream"
                )

            return await apply_anti_truncation_to_stream(
                antigravity_gemini_request_func, request_body, max_attempts
            )

        # æµå¼è¯·æ±‚ï¼ˆæ— æŠ—æˆªæ–­ï¼‰
        resources, cred_name, cred_data = await send_antigravity_request_stream(
            request_body, cred_mgr
        )
        # resources æ˜¯ä¸€ä¸ªå…ƒç»„: (response, stream_ctx, client)
        response, stream_ctx, client = resources

        # è½¬æ¢å¹¶è¿”å›æµå¼å“åº”
        # response ç°åœ¨æ˜¯ filtered_lines ç”Ÿæˆå™¨
        return StreamingResponse(
            convert_antigravity_stream_to_gemini(
                response, stream_ctx, client, cred_mgr, cred_name
            ),
            media_type="text/event-stream"
        )

    except Exception as e:
        log.error(f"[ANTIGRAVITY GEMINI] Stream request failed: {e}")
        raise HTTPException(status_code=500, detail=f"Antigravity API request failed: {str(e)}")


# ==================== SD-WebUI æ ¼å¼ API ç«¯ç‚¹ ====================

@router.get("/sdapi/v1/options")
@router.get("/antigravity/sdapi/v1/options")
async def sdwebui_get_options(_: str = Depends(authenticate_sdwebui_flexible)):
    """è¿”å› SD-WebUI æ ¼å¼çš„é…ç½®é€‰é¡¹"""
    log.info("[ANTIGRAVITY SD-WebUI] Received options request")
    # è¿”å›åŸºæœ¬çš„é…ç½®é€‰é¡¹
    return {
        "sd_model_checkpoint": "gemini-3-pro-image",
        "sd_checkpoint_hash": None,
        "samples_save": True,
        "samples_format": "png",
        "save_images_add_number": True,
        "grid_save": True,
        "return_grid": True,
        "enable_pnginfo": True,
        "save_txt": False,
        "CLIP_stop_at_last_layers": 1,
    }


@router.get("/sdapi/v1/sd-models")
@router.get("/antigravity/sdapi/v1/sd-models")
async def sdwebui_list_models(_: str = Depends(authenticate_sdwebui_flexible)):
    """è¿”å› SD-WebUI æ ¼å¼çš„æ¨¡å‹åˆ—è¡¨ - åªåŒ…å«å¸¦ image å…³é”®è¯çš„æ¨¡å‹"""

    try:
        # è·å–å‡­è¯ç®¡ç†å™¨
        cred_mgr = await get_credential_manager()

        # ä» Antigravity API è·å–æ¨¡å‹åˆ—è¡¨
        models = await fetch_available_models(cred_mgr)

        if not models:
            log.warning("[ANTIGRAVITY SD-WebUI] Failed to fetch models from API, returning empty list")
            return []

        # è¿‡æ»¤åªåŒ…å« "image" å…³é”®è¯çš„æ¨¡å‹
        image_models = []
        for model in models:
            model_id = model.get("id", "")
            if "image" in model_id.lower():
                # SD-WebUI æ ¼å¼: {"title": "model_name", "model_name": "model_name", "hash": null}
                image_models.append({
                    "title": model_id,
                    "model_name": model_id,
                    "hash": None,
                    "sha256": None,
                    "filename": model_id,
                    "config": None
                })

        return image_models

    except Exception as e:
        log.error(f"[ANTIGRAVITY SD-WebUI] Error fetching models: {e}")
        return []


@router.post("/sdapi/v1/txt2img")
@router.post("/antigravity/sdapi/v1/txt2img")
async def sdwebui_txt2img(request: Request, _: str = Depends(authenticate_sdwebui_flexible)):
    """å¤„ç† SD-WebUI æ ¼å¼çš„ txt2img è¯·æ±‚ï¼Œè½¬æ¢ä¸º Antigravity API"""
    # è·å–åŸå§‹è¯·æ±‚æ•°æ®
    try:
        request_data = await request.json()
    except Exception as e:
        log.error(f"Failed to parse JSON request: {e}")
        raise HTTPException(status_code=400, detail=f"Invalid JSON: {str(e)}")

    # æå–åŸºæœ¬å‚æ•°
    prompt = request_data.get("prompt", "")
    if not prompt:
        raise HTTPException(status_code=400, detail="Missing required field: prompt")

    negative_prompt = request_data.get("negative_prompt", "")

    # æå–å›¾ç‰‡ç”Ÿæˆç›¸å…³å‚æ•°
    width = request_data.get("width", 1024)
    height = request_data.get("height", 1024)

    # è®¡ç®— aspect_ratio - æ˜ å°„åˆ°æ”¯æŒçš„æ¯”ä¾‹
    # æ”¯æŒçš„æ¯”ä¾‹: "1:1", "2:3", "3:2", "3:4", "4:3", "4:5", "5:4", "9:16", "16:9", "21:9"
    def find_closest_aspect_ratio(w: int, h: int) -> str:
        ratio = w / h
        supported_ratios = {
            "1:1": 1.0,
            "2:3": 0.667,
            "3:2": 1.5,
            "3:4": 0.75,
            "4:3": 1.333,
            "4:5": 0.8,
            "5:4": 1.25,
            "9:16": 0.5625,
            "16:9": 1.778,
            "21:9": 2.333
        }
        closest = min(supported_ratios.items(), key=lambda x: abs(x[1] - ratio))
        return closest[0]

    aspect_ratio = find_closest_aspect_ratio(width, height)

    # ç®€åŒ–çš„å°ºå¯¸æ˜ å°„åˆ° image_size
    max_dimension = max(width, height)
    if max_dimension <= 1024:
        image_size = "1K"
    elif max_dimension <= 2048:
        image_size = "2K"
    else:
        image_size = "4K"

    # æå–æ¨¡å‹ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    model = request_data.get("override_settings", {}).get("sd_model_checkpoint", "gemini-3-pro-image")
    if not model or "image" not in model.lower():
        model = "gemini-3-pro-image"

    log.info(f"[ANTIGRAVITY SD-WebUI] txt2img request: model={model}, prompt={prompt[:50]}..., aspect_ratio={aspect_ratio}, image_size={image_size}")

    cred_mgr = await get_credential_manager()

    # æ„å»º Gemini æ ¼å¼çš„ contents
    full_prompt = prompt
    if negative_prompt:
        full_prompt = f"{prompt}\n\nNegative prompt: {negative_prompt}"

    contents = [{
        "role": "user",
        "parts": [{"text": full_prompt}]
    }]

    # æ„å»º generation_configï¼ŒåŒ…å«å›¾ç‰‡ç”Ÿæˆå‚æ•°
    parameters = {
        "response_modalities": ["TEXT", "IMAGE"],
        "image_config": {
            "aspect_ratio": aspect_ratio,
            "image_size": image_size
        }
    }

    # æ¨¡å‹åç§°æ˜ å°„
    actual_model = model_mapping(model)
    enable_thinking = is_thinking_model(model)

    generation_config = generate_generation_config(parameters, enable_thinking, actual_model)

    # è·å–å‡­è¯ä¿¡æ¯
    cred_result = await cred_mgr.get_valid_credential(is_antigravity=True)
    if not cred_result:
        log.error("å½“å‰æ— å¯ç”¨ antigravity å‡­è¯")
        raise HTTPException(status_code=500, detail="å½“å‰æ— å¯ç”¨ antigravity å‡­è¯")

    _, credential_data = cred_result
    project_id = credential_data.get("project_id", "default-project")
    session_id = f"session-{uuid.uuid4().hex}"

    # æ„å»º Antigravity è¯·æ±‚ä½“
    request_body = build_antigravity_request_body(
        contents=contents,
        model=actual_model,
        project_id=project_id,
        session_id=session_id,
        tools=None,
        generation_config=generation_config,
    )

    # å‘é€éæµå¼è¯·æ±‚
    try:
        response_data, cred_name, cred_data = await send_antigravity_request_no_stream(
            request_body, cred_mgr
        )

        # æå–ç”Ÿæˆçš„å›¾ç‰‡
        parts = response_data.get("response", {}).get("candidates", [{}])[0].get("content", {}).get("parts", [])

        images = []
        info_text = ""

        for part in parts:
            if "inlineData" in part:
                inline_data = part["inlineData"]
                base64_data = inline_data.get("data", "")
                images.append(base64_data)
            elif "text" in part:
                info_text += part.get("text", "")

        if not images:
            raise HTTPException(status_code=500, detail="No images generated")

        # æ„å»º SD-WebUI æ ¼å¼çš„å“åº”
        sdwebui_response = {
            "images": images,
            "parameters": request_data,
            "info": info_text or f"Generated by {model}"
        }

        return JSONResponse(content=sdwebui_response)

    except Exception as e:
        log.error(f"[ANTIGRAVITY SD-WebUI] txt2img request failed: {e}")
        raise HTTPException(status_code=500, detail=f"Image generation failed: {str(e)}")

