"""
Antigravity Router - Handles OpenAI and Gemini format requests and converts to Antigravity API
处理 OpenAI 和 Gemini 格式请求并转换为 Antigravity API 格式
"""

import json
import time
import uuid
from typing import Any, Dict, List, Optional

from fastapi import APIRouter, Depends, HTTPException, Path, Request
from fastapi.responses import JSONResponse, StreamingResponse

from config import get_anti_truncation_max_attempts
from log import log
from src.utils import is_anti_truncation_model, authenticate_bearer, authenticate_gemini_flexible, authenticate_sdwebui_flexible

from .antigravity_api import (
    build_antigravity_request_body,
    send_antigravity_request_no_stream,
    send_antigravity_request_stream,
    fetch_available_models,
)
from .credential_manager import CredentialManager
from .models import (
    ChatCompletionRequest,
    GeminiGenerationConfig,
    Model,
    ModelList,
    model_to_dict,
    OpenAIChatCompletionChoice,
    OpenAIChatCompletionResponse,
    OpenAIChatMessage,
    OpenAIToolCall,
    OpenAIToolFunction,
)
from .anti_truncation import (
    apply_anti_truncation_to_stream,
)

# 创建路由器
router = APIRouter()

# 全局凭证管理器实例
credential_manager = None


async def get_credential_manager():
    """获取全局凭证管理器实例"""
    global credential_manager
    if not credential_manager:
        credential_manager = CredentialManager()
        await credential_manager.initialize()
    return credential_manager


# 模型名称映射
def model_mapping(model_name: str) -> str:
    """
    OpenAI 模型名映射到 Antigravity 实际模型名

    参考文档:
    - claude-sonnet-4-5-thinking -> claude-sonnet-4-5
    - claude-opus-4-5 -> claude-opus-4-5-thinking
    - gemini-2.5-flash-thinking -> gemini-2.5-flash
    """
    mapping = {
        "claude-sonnet-4-5-thinking": "claude-sonnet-4-5",
        "claude-opus-4-5": "claude-opus-4-5-thinking",
        "gemini-2.5-flash-thinking": "gemini-2.5-flash",
        # Cursor 客户端模型名映射
        "claude-4.5-opus-high-thinking": "claude-opus-4-5-thinking",
        "claude-4.5-opus-high": "claude-opus-4-5",
        "claude-4.5-opus": "claude-opus-4-5",
        "claude-4.5-opus-thinking": "claude-opus-4-5-thinking",
        "claude-4.5-sonnet-high-thinking": "claude-sonnet-4-5-thinking",
        "claude-4.5-sonnet-high": "claude-sonnet-4-5",
        "claude-4.5-sonnet": "claude-sonnet-4-5",
        "claude-4.5-sonnet-thinking": "claude-sonnet-4-5-thinking",
        "claude-opus-4-5-high-thinking": "claude-opus-4-5-thinking",
        "claude-opus-4-5-high": "claude-opus-4-5",
        "claude-sonnet-4-5-high-thinking": "claude-sonnet-4-5-thinking",
        "claude-sonnet-4-5-high": "claude-sonnet-4-5",
        # OpenAI/Anthropic 标准模型名映射
        "claude-3-5-sonnet-20241022": "claude-sonnet-4-5",
        "claude-3-opus-20240229": "claude-opus-4-5",
        "claude-3-5-sonnet": "claude-sonnet-4-5",
        "claude-3-opus": "claude-opus-4-5",
        "gpt-4": "claude-opus-4-5",
        "gpt-4-turbo": "claude-opus-4-5",
        "gpt-4o": "claude-sonnet-4-5",
    }
    return mapping.get(model_name, model_name)


# ====================== 导入智能降级管理器 ======================
from .fallback_manager import (
    decide_fallback_action,
    is_quota_exhausted_error,
    is_retryable_error,
    is_403_error,
    trigger_credential_verification,
    get_cross_pool_fallback,
    is_model_supported,
    FallbackDecision,
    COPILOT_URL,
)

# Copilot API 地址
COPILOT_URL = "http://localhost:4141/"

# ====================== 降级辅助函数 ======================

def get_fallback_models(model_name: str) -> List[str]:
    """
    获取模型的降级链（使用新的跨池降级逻辑）

    注意：此函数用于预计算降级目标，使用 debug 级别日志避免噪音。
    实际降级时会使用 info 级别日志。

    Args:
        model_name: 当前模型名

    Returns:
        降级模型列表（按优先级排序）
    """
    from .fallback_manager import get_cross_pool_fallback, is_haiku_model, HAIKU_FALLBACK_TARGET

    fallback_list = []

    # Haiku 模型特殊处理
    if is_haiku_model(model_name):
        fallback_list.append(HAIKU_FALLBACK_TARGET)
        return fallback_list

    # 获取跨池降级目标 - 预计算使用 debug 级别日志
    cross_pool_fallback = get_cross_pool_fallback(model_name, log_level="debug")
    if cross_pool_fallback:
        fallback_list.append(cross_pool_fallback)

    return fallback_list


def should_fallback_on_error(error_msg: str) -> bool:
    """
    判断是否应该触发模型降级

    只有额度用尽错误才触发降级，其他错误（400/429普通限流/5xx）应该重试

    Args:
        error_msg: 错误消息

    Returns:
        True 如果应该降级，False 如果应该重试或失败
    """
    from .fallback_manager import is_quota_exhausted_error

    # 只有额度用尽才触发降级
    return is_quota_exhausted_error(error_msg)




def is_thinking_model(model_name: str) -> bool:
    """检测是否是思考模型"""
    # 检查是否包含 -thinking 后缀
    if "-thinking" in model_name:
        return True

    # 检查是否包含 pro 关键词
    if "pro" in model_name.lower():
        return True

    return False


def extract_images_from_content(content: Any) -> Dict[str, Any]:
    """
    从 OpenAI content 中提取文本和图片
    """
    result = {"text": "", "images": []}

    if isinstance(content, str):
        result["text"] = content
    elif isinstance(content, list):
        for item in content:
            if isinstance(item, dict):
                if item.get("type") == "text":
                    result["text"] += item.get("text", "")
                elif item.get("type") == "image_url":
                    image_url = item.get("image_url", {}).get("url", "")
                    # 解析 data:image/png;base64,xxx 格式
                    if image_url.startswith("data:image/"):
                        import re
                        match = re.match(r"^data:image/(\w+);base64,(.+)$", image_url)
                        if match:
                            mime_type = match.group(1)
                            base64_data = match.group(2)
                            result["images"].append({
                                "inlineData": {
                                    "mimeType": f"image/{mime_type}",
                                    "data": base64_data
                                }
                            })

    return result


def strip_thinking_from_openai_messages(messages: List[Any]) -> List[Any]:
    """
    从 OpenAI 格式消息中移除 thinking 内容块。
    
    当 thinking 被禁用时，历史消息中的 thinking 内容块会导致 400 错误：
    "When thinking is disabled, an `assistant` message..."
    
    此函数会：
    1. 遍历所有消息
    2. 对于 assistant 消息，移除 content 中的 thinking 相关内容
    3. 处理字符串格式的 content（移除 <think>...</think> 或 <think>...</think> 标签）
    4. 处理数组格式的 content（移除 type="thinking" 的项）
    """
    if not messages:
        return messages
    
    import re
    cleaned_messages = []
    
    for msg in messages:
        # 处理 Pydantic 模型对象
        if hasattr(msg, "role") and hasattr(msg, "content"):
            role = getattr(msg, "role", None)
            content = getattr(msg, "content", None)
            
            # 只处理 assistant 消息
            if role == "assistant" and content:
                # 处理字符串格式的 content
                if isinstance(content, str):
                    # 移除各种 thinking 标签格式
                    # <think>...</think> 或 <think>...</think>
                    content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL | re.IGNORECASE)
                    content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL | re.IGNORECASE)
                    # 清理多余的空白行
                    content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)
                    # 如果内容为空，保留一个占位符
                    if not content.strip():
                        content = "..."
                    # 创建新消息对象
                    from src.models import OpenAIChatMessage
                    cleaned_msg = OpenAIChatMessage(role=role, content=content)
                    cleaned_messages.append(cleaned_msg)
                    continue
                
                # 处理数组格式的 content
                elif isinstance(content, list):
                    cleaned_content = []
                    for item in content:
                        if isinstance(item, dict):
                            item_type = item.get("type")
                            # 跳过 thinking 类型的内容块
                            if item_type in ("thinking", "redacted_thinking"):
                                continue
                        cleaned_content.append(item)
                    
                    # 如果清理后为空，添加一个空文本块
                    if not cleaned_content:
                        cleaned_content = [{"type": "text", "text": "..."}]
                    
                    # 创建新消息对象
                    from src.models import OpenAIChatMessage
                    cleaned_msg = OpenAIChatMessage(role=role, content=cleaned_content)
                    cleaned_messages.append(cleaned_msg)
                    continue
        
        # 处理字典格式的消息
        elif isinstance(msg, dict):
            role = msg.get("role")
            content = msg.get("content")
            
            # 只处理 assistant 消息
            if role == "assistant" and content:
                # 处理字符串格式的 content
                if isinstance(content, str):
                    # 移除各种 thinking 标签格式
                    # <think>...</think> 或 <think>...</think>
                    content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL | re.IGNORECASE)
                    content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL | re.IGNORECASE)
                    # 清理多余的空白行
                    content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)
                    # 如果内容为空，保留一个占位符
                    if not content.strip():
                        content = "..."
                    # 创建新消息对象
                    cleaned_msg = msg.copy()
                    cleaned_msg["content"] = content
                    cleaned_messages.append(cleaned_msg)
                    continue
                
                # 处理数组格式的 content
                elif isinstance(content, list):
                    cleaned_content = []
                    for item in content:
                        if isinstance(item, dict):
                            item_type = item.get("type")
                            # 跳过 thinking 类型的内容块
                            if item_type in ("thinking", "redacted_thinking"):
                                continue
                        cleaned_content.append(item)
                    
                    # 如果清理后为空，添加一个空文本块
                    if not cleaned_content:
                        cleaned_content = [{"type": "text", "text": "..."}]
                    
                    # 创建新消息对象
                    cleaned_msg = msg.copy()
                    cleaned_msg["content"] = cleaned_content
                    cleaned_messages.append(cleaned_msg)
                    continue
        
        # 其他情况直接保留
        cleaned_messages.append(msg)
    
    return cleaned_messages


def openai_messages_to_antigravity_contents(messages: List[Any], enable_thinking: bool = False) -> List[Dict[str, Any]]:
    """
    将 OpenAI 消息格式转换为 Antigravity contents 格式
    
    Args:
        messages: OpenAI 格式的消息列表
        enable_thinking: 是否启用 thinking（当启用时，最后一条 assistant 消息必须以 thinking block 开头）
    """
    contents = []
    system_messages = []

    # ✅ 方案2+4：检测历史消息中是否有工具调用错误，决定是否注入强化提示
    # 工具格式提示常量（内联定义）
    TOOL_FORMAT_REMINDER_TEMPLATE = """

[IMPORTANT - Tool Call Format Rules]
When calling tools, you MUST follow these rules strictly:
1. Always use the EXACT parameter names as defined in the current tool schema
2. Do NOT use parameter names from previous conversations - schemas may have changed
3. For terminal/command tools: the parameter name varies - check the tool definition
4. When in doubt: re-read the tool definition and use ONLY the parameters listed there

{tool_params_section}
"""

    TOOL_FORMAT_REMINDER_AFTER_ERROR_TEMPLATE = """

[CRITICAL - Tool Call Error Detected]
Previous tool calls failed due to invalid arguments. You MUST:
1. STOP using parameter names from previous conversations
2. Use ONLY the exact parameter names shown below
3. Do NOT guess parameter names

{tool_params_section}

IMPORTANT: If a tool call fails, check the parameter names above and try again with the EXACT names listed.
"""

    has_tool_error = False
    has_tools = False  # 检测是否有工具调用

    for msg in messages:
        msg_content = getattr(msg, "content", "")
        msg_tool_calls = getattr(msg, "tool_calls", None)

        # 检测是否有工具调用
        if msg_tool_calls:
            has_tools = True

        # 检测错误模式
        if msg_content and isinstance(msg_content, str):
            error_patterns = [
                "invalid arguments",
                "Invalid arguments",
                "invalid parameters",
                "Invalid parameters",
                "Unexpected parameters",
                "unexpected parameters",
                "model provided invalid",
                "Tool call arguments",
                "were invalid",
            ]
            for pattern in error_patterns:
                if pattern in msg_content:
                    has_tool_error = True
                    log.info(f"[ANTIGRAVITY] Detected tool error pattern in message: '{pattern}'")
                    break
        if has_tool_error:
            break

    for i, msg in enumerate(messages):
        role = getattr(msg, "role", "user")
        content = getattr(msg, "content", "")
        tool_calls = getattr(msg, "tool_calls", None)
        tool_call_id = getattr(msg, "tool_call_id", None)

        # 处理 system 消息 - 合并到第一条用户消息
        if role == "system":
            # ✅ 方案2+3：在 system 消息末尾注入工具格式提示（包含动态参数）
            if has_tools:
                # 提取工具参数摘要
                tool_params = extract_tool_params_summary(messages[0].tool_calls if hasattr(messages[0], 'tool_calls') else [])

                # 如果没有从 messages 中提取到，尝试使用全局 tools（如果可用）
                if not tool_params:
                    tool_params_section = "Check the tool definitions in your context for exact parameter names."
                else:
                    tool_params_section = f"Current tool parameters (use EXACTLY these names):\n{tool_params}"

                if has_tool_error:
                    # 检测到错误，注入强化提示
                    reminder = TOOL_FORMAT_REMINDER_AFTER_ERROR_TEMPLATE.format(tool_params_section=tool_params_section)
                    content = content + reminder
                    log.info(f"[ANTIGRAVITY] Injected TOOL_FORMAT_REMINDER_AFTER_ERROR with params into system message")
                else:
                    # 预防性注入基础提示
                    reminder = TOOL_FORMAT_REMINDER_TEMPLATE.format(tool_params_section=tool_params_section)
                    content = content + reminder
                    log.debug("[ANTIGRAVITY] Injected TOOL_FORMAT_REMINDER with params into system message")
            system_messages.append(content)
            continue

        # 处理 user 消息
        elif role == "user":
            parts = []

            # 如果有系统消息，添加到第一条用户消息
            if system_messages:
                for sys_msg in system_messages:
                    parts.append({"text": sys_msg})
                system_messages = []

            # 提取文本和图片
            extracted = extract_images_from_content(content)
            if extracted["text"]:
                parts.append({"text": extracted["text"]})
            parts.extend(extracted["images"])

            if parts:
                contents.append({"role": "user", "parts": parts})

        # 处理 assistant 消息
        elif role == "assistant":
            # 处理 content：可能是字符串或数组
            content_parts = []
            if content:
                if isinstance(content, str):
                    # 字符串格式：检查是否包含 thinking 标签
                    import re
                    # 匹配 <think>...</think> 或 <think>...</think>
                    thinking_match = re.search(r'<(?:redacted_)?reasoning>.*?</(?:redacted_)?reasoning>', content, flags=re.DOTALL | re.IGNORECASE)
                    if not thinking_match:
                        thinking_match = re.search(r'<think>.*?</think>', content, flags=re.DOTALL | re.IGNORECASE)
                    
                    if thinking_match:
                        # 提取 thinking 内容
                        thinking_text = thinking_match.group(0)
                        # 移除标签，保留内容
                        thinking_content = re.sub(r'</?(?:redacted_)?reasoning>', '', thinking_text, flags=re.IGNORECASE)
                        thinking_content = re.sub(r'</?think>', '', thinking_content, flags=re.IGNORECASE)
                        # 注意：Claude API 要求 thoughtSignature 必须是有效的非空字符串
                        # 从字符串格式的 content 中无法提取 signature，因此不添加 thinking block
                        # 这样可以避免 400 错误："thinking.signature: Field required"
                        # thinking 内容会被移除（因为无法正确传递给 Claude）
                        log.debug(f"[ANTIGRAVITY] Found thinking tag in string content, but no signature available. Removing thinking content.")
                        # 移除 thinking 标签后的剩余内容
                        content = re.sub(r'<(?:redacted_)?reasoning>.*?</(?:redacted_)?reasoning>', '', content, flags=re.DOTALL | re.IGNORECASE)
                        content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL | re.IGNORECASE)
                        content = content.strip()
                    
                    extracted = extract_images_from_content(content)
                    if extracted["text"]:
                        content_parts.append({"text": extracted["text"]})
                    content_parts.extend(extracted["images"])
                elif isinstance(content, list):
                    # 数组格式：检查是否有 thinking 类型的内容块
                    for item in content:
                        if isinstance(item, dict):
                            item_type = item.get("type")
                            if item_type == "thinking":
                                # 提取 thinking 内容
                                thinking_text = item.get("thinking", "")
                                signature = item.get("signature", "")
                                # 只有当 signature 有效时才添加 thinking block
                                # Claude API 要求 signature 必须是有效的非空字符串
                                if signature and signature.strip():
                                    content_parts.append({
                                        "text": str(thinking_text),
                                        "thought": True,
                                        "thoughtSignature": signature
                                    })
                                else:
                                    # signature 无效，将 thinking 内容作为普通文本处理
                                    if thinking_text:
                                        content_parts.append({"text": str(thinking_text)})
                                        log.debug(f"[ANTIGRAVITY] Thinking block has no valid signature, treating as regular text")
                            elif item_type == "redacted_thinking":
                                # 提取 redacted_thinking 内容
                                thinking_text = item.get("thinking") or item.get("data", "")
                                signature = item.get("signature", "")
                                # 只有当 signature 有效时才添加 thinking block
                                # Claude API 要求 signature 必须是有效的非空字符串
                                if signature and signature.strip():
                                    content_parts.append({
                                        "text": str(thinking_text),
                                        "thought": True,
                                        "thoughtSignature": signature
                                    })
                                else:
                                    # signature 无效，将 thinking 内容作为普通文本处理
                                    if thinking_text:
                                        content_parts.append({"text": str(thinking_text)})
                                        log.debug(f"[ANTIGRAVITY] Redacted thinking block has no valid signature, treating as regular text")
                            elif item_type == "text":
                                content_parts.append({"text": item.get("text", "")})
                            elif item_type == "image_url":
                                # 处理图片
                                image_url = item.get("image_url", {}).get("url", "")
                                if image_url.startswith("data:image/"):
                                    import re
                                    match = re.match(r"^data:image/(\w+);base64,(.+)$", image_url)
                                    if match:
                                        mime_type = match.group(1)
                                        base64_data = match.group(2)
                                        content_parts.append({
                                            "inlineData": {
                                                "mimeType": f"image/{mime_type}",
                                                "data": base64_data
                                            }
                                        })
                        else:
                            # 非字典项，转换为文本
                            if item:
                                content_parts.append({"text": str(item)})
                else:
                    # 其他格式，尝试提取文本
                    extracted = extract_images_from_content(content)
                    if extracted["text"]:
                        content_parts.append({"text": extracted["text"]})
                    content_parts.extend(extracted["images"])
            
            # 添加工具调用
            if tool_calls:
                for tool_call in tool_calls:
                    tc_id = getattr(tool_call, "id", None)
                    tc_type = getattr(tool_call, "type", "function")
                    tc_function = getattr(tool_call, "function", None)

                    if tc_function:
                        func_name = getattr(tc_function, "name", "")
                        func_args = getattr(tc_function, "arguments", "{}")

                        # 解析 arguments（可能是字符串）
                        if isinstance(func_args, str):
                            try:
                                args_dict = json.loads(func_args)
                            except:
                                args_dict = {"query": func_args}
                        else:
                            args_dict = func_args

                        content_parts.append({
                            "functionCall": {
                                "id": tc_id,
                                "name": func_name,
                                "args": args_dict
                            }
                        })
            
            # 注意：不在这里检查是否是最后一条 assistant 消息，因为消息顺序可能会改变
            # 将在转换后统一检查
            
            if content_parts:
                contents.append({"role": "model", "parts": content_parts})

        # 处理 tool 消息
        elif role == "tool":
            tool_call_id = getattr(msg, "tool_call_id", None)
            tool_name = getattr(msg, "name", "unknown")
            content = getattr(msg, "content", "")
            
            # ✅ 验证必要字段
            if not tool_call_id:
                log.warning(f"[ANTIGRAVITY] Tool message missing tool_call_id at index {i}, skipping")
                continue  # 跳过无效的工具消息
            
            # ✅ 处理 content 为 None 的情况
            if content is None:
                content = ""
                log.debug(f"[ANTIGRAVITY] Tool message content is None, converting to empty string")
            
            # ✅ 记录工具消息信息（用于诊断）
            if not content:
                log.warning(f"[ANTIGRAVITY] Tool message has empty content: tool_call_id={tool_call_id}, name={tool_name}")
            else:
                content_preview = str(content)[:100] if content else ""
                log.debug(f"[ANTIGRAVITY] Tool message: tool_call_id={tool_call_id}, name={tool_name}, content_length={len(str(content))}, preview={content_preview}")
            
            # ✅ 确保 response.output 是有效的 JSON 可序列化值
            # 如果 content 是复杂对象，需要序列化
            if not isinstance(content, (str, int, float, bool, type(None))):
                try:
                    import json
                    content = json.dumps(content) if content else ""
                except Exception as e:
                    log.warning(f"[ANTIGRAVITY] Failed to serialize tool content: {e}, using str()")
                    content = str(content) if content else ""
            
            parts = [{
                "functionResponse": {
                    "id": tool_call_id,
                    "name": tool_name,
                    "response": {"output": content}
                }
            }]
            contents.append({"role": "user", "parts": parts})

    return contents


def gemini_contents_to_antigravity_contents(gemini_contents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    将 Gemini 原生 contents 格式转换为 Antigravity contents 格式
    Gemini 和 Antigravity 的 contents 格式基本一致，只需要做少量调整
    """
    contents = []

    for content in gemini_contents:
        role = content.get("role", "user")
        parts = content.get("parts", [])

        contents.append({
            "role": role,
            "parts": parts
        })

    return contents



def extract_tool_params_summary(tools: Optional[List[Any]]) -> str:
    """
    从工具定义中提取参数摘要，用于注入到System Prompt
    帮助模型了解当前正确的工具参数名
    """
    if not tools:
        return ""

    # 重点关注的常用工具
    important_tools = ["read", "read_file", "terminal", "run_terminal_command",
                       "write", "edit", "bash", "str_replace_editor", "execute_command"]

    summaries = []

    for tool in tools:
        try:
            # 获取工具名
            if hasattr(tool, "function"):
                func = tool.function
                tool_name = getattr(func, "name", None) or (func.get("name") if isinstance(func, dict) else None)
                params = getattr(func, "parameters", None) or (func.get("parameters") if isinstance(func, dict) else None)
            elif isinstance(tool, dict) and "function" in tool:
                func = tool["function"]
                tool_name = func.get("name")
                params = func.get("parameters")
            else:
                continue

            if not tool_name:
                continue

            # 只处理重要工具或名称中包含关键词的工具
            is_important = any(imp in tool_name.lower() for imp in important_tools)
            if not is_important:
                continue

            # 提取参数名
            if params and isinstance(params, dict):
                properties = params.get("properties", {})
                required = params.get("required", [])

                if properties:
                    param_list = []
                    for param_name in properties.keys():
                        if param_name in required:
                            param_list.append(f"`{param_name}` (required)")
                        else:
                            param_list.append(f"`{param_name}`")

                    if param_list:
                        summaries.append(f"- {tool_name}: {', '.join(param_list)}")
        except Exception:
            continue

    if summaries:
        return "\n\nCurrent tool parameters (use ONLY these exact names):\n" + "\n".join(summaries)
    return ""


def convert_openai_tools_to_antigravity(tools: Optional[List[Any]]) -> Optional[List[Dict[str, Any]]]:
    """
    将 OpenAI 工具定义转换为 Antigravity 格式

    支持两种输入格式：
    1. Pydantic 模型对象（使用 getattr）
    2. 普通字典（使用 .get()）
    """
    if not tools:
        return None

    # 需要排除的字段
    EXCLUDED_KEYS = {'$schema', 'additionalProperties', 'minLength', 'maxLength',
                     'minItems', 'maxItems', 'uniqueItems'}

    def clean_parameters(obj):
        """递归清理参数对象"""
        if isinstance(obj, dict):
            cleaned = {}
            for key, value in obj.items():
                if key in EXCLUDED_KEYS:
                    continue
                cleaned[key] = clean_parameters(value)
            return cleaned
        elif isinstance(obj, list):
            return [clean_parameters(item) for item in obj]
        else:
            return obj

    def get_value(obj, key, default=None):
        """从对象或字典中获取值"""
        if isinstance(obj, dict):
            return obj.get(key, default)
        return getattr(obj, key, default)

    # 导入 clean_json_schema 函数用于清理 custom 工具的 input_schema
    from src.anthropic_converter import clean_json_schema

    function_declarations = []

    for tool in tools:
        # 首先将 Pydantic 模型转换为字典（如果需要）
        if not isinstance(tool, dict):
            # 处理 Pydantic 模型对象
            if hasattr(tool, "model_dump"):
                tool = tool.model_dump()
            elif hasattr(tool, "dict"):
                tool = tool.dict()
            else:
                # 尝试使用 getattr 获取属性
                tool_dict = {}
                for attr in ["type", "function", "custom"]:
                    if hasattr(tool, attr):
                        value = getattr(tool, attr)
                        if hasattr(value, "model_dump"):
                            tool_dict[attr] = value.model_dump()
                        elif hasattr(value, "dict"):
                            tool_dict[attr] = value.dict()
                        else:
                            tool_dict[attr] = value
                tool = tool_dict
        
        # 支持字典和对象两种格式
        tool_type = get_value(tool, "type", "function")
        
        # DEBUG: Log tool structure
        if isinstance(tool, dict):
            tool_keys = list(tool.keys())
            log.debug(f"[ANTIGRAVITY] Processing tool: type={tool_type}, keys={tool_keys}")

        if tool_type == "function":
            function = get_value(tool, "function", None)
            
            # 如果 function 是 Pydantic 模型，转换为字典
            if function and not isinstance(function, dict):
                if hasattr(function, "model_dump"):
                    function = function.model_dump()
                elif hasattr(function, "dict"):
                    function = function.dict()
                else:
                    function = {k: getattr(function, k) for k in ["name", "description", "parameters"] if hasattr(function, k)}

            if function:
                func_name = get_value(function, "name")
                if not func_name:
                    log.warning(f"[ANTIGRAVITY] Skipping tool without function name")
                    continue

                func_desc = get_value(function, "description", "")
                func_params = get_value(function, "parameters", {})

                # 转换为字典（如果是 Pydantic 模型）
                if hasattr(func_params, "dict"):
                    func_params = func_params.dict()
                elif hasattr(func_params, "model_dump"):
                    func_params = func_params.model_dump()

                # 使用 clean_json_schema 清理参数（确保嵌套 object 类型正确处理）
                cleaned_params = clean_json_schema(func_params) if isinstance(func_params, dict) else clean_parameters(func_params)

                # 确保 parameters 有 type 字段（Antigravity 要求）
                if cleaned_params and "type" not in cleaned_params:
                    cleaned_params["type"] = "object"

                function_declarations.append({
                    "name": func_name,
                    "description": func_desc,
                    "parameters": cleaned_params
                })
        elif tool_type == "custom":
            # 处理 custom 类型的工具（Cursor 使用）
            # 格式: {"type": "custom", "custom": {"name": "...", "input_schema": {...}}}
            # 注意：这不应该发生，因为网关应该已经将 custom 工具转换为 function 格式
            # 但如果确实收到了，我们仍然处理它
            log.warning(f"[ANTIGRAVITY] Received custom tool format (should have been normalized by gateway): {list(tool.keys()) if isinstance(tool, dict) else 'not a dict'}")
            
            custom_tool = get_value(tool, "custom", None)
            
            # 如果 custom_tool 是 Pydantic 模型，转换为字典
            if custom_tool and not isinstance(custom_tool, dict):
                if hasattr(custom_tool, "model_dump"):
                    custom_tool = custom_tool.model_dump()
                elif hasattr(custom_tool, "dict"):
                    custom_tool = custom_tool.dict()
                else:
                    custom_tool = {k: getattr(custom_tool, k) for k in ["name", "description", "input_schema"] if hasattr(custom_tool, k)}
            
            if custom_tool:
                func_name = get_value(custom_tool, "name")
                if not func_name:
                    log.warning(f"[ANTIGRAVITY] Skipping custom tool without name")
                    continue

                func_desc = get_value(custom_tool, "description", "")
                input_schema = get_value(custom_tool, "input_schema", {}) or {}
                
                # 如果 input_schema 是 Pydantic 模型，转换为字典
                if input_schema and not isinstance(input_schema, dict):
                    if hasattr(input_schema, "model_dump"):
                        input_schema = input_schema.model_dump()
                    elif hasattr(input_schema, "dict"):
                        input_schema = input_schema.dict()
                    else:
                        log.warning(f"[ANTIGRAVITY] input_schema is not a dict or Pydantic model: {type(input_schema)}")
                        input_schema = {}

                # 使用 clean_json_schema 清理 input_schema，确保有 type 字段
                cleaned_params = clean_json_schema(input_schema) if isinstance(input_schema, dict) else {}

                # 确保 parameters 有 type 字段（Antigravity 要求）
                if cleaned_params and "type" not in cleaned_params:
                    cleaned_params["type"] = "object"

                function_declarations.append({
                    "name": func_name,
                    "description": func_desc,
                    "parameters": cleaned_params
                })
            else:
                log.warning(f"[ANTIGRAVITY] Skipping custom tool without 'custom' field")

    if function_declarations:
        return [{"functionDeclarations": function_declarations}]

    return None


def generate_generation_config(
    parameters: Dict[str, Any],
    enable_thinking: bool,
    model_name: str
) -> Dict[str, Any]:
    """
    生成 Antigravity generationConfig，使用 GeminiGenerationConfig 模型
    """
    # 构建基础配置
    config_dict = {
        "candidateCount": 1,
        "stopSequences": [
            "<|user|>",
            "<|bot|>",
            "<|context_request|>",
            "<|endoftext|>",
            "<|end_of_turn|>"
        ],
        "topK": parameters.get("top_k", 50),  # 默认值 50
    }

    # 添加可选参数
    if "temperature" in parameters:
        config_dict["temperature"] = parameters["temperature"]

    if "top_p" in parameters:
        config_dict["topP"] = parameters["top_p"]

    if "max_tokens" in parameters:
        config_dict["maxOutputTokens"] = parameters["max_tokens"]

    # 图片生成相关参数
    if "response_modalities" in parameters:
        config_dict["response_modalities"] = parameters["response_modalities"]

    if "image_config" in parameters:
        config_dict["image_config"] = parameters["image_config"]

    # 思考模型配置
    if enable_thinking:
        config_dict["thinkingConfig"] = {
            "includeThoughts": True,
            "thinkingBudget": 1024
        }

        # Claude 思考模型：删除 topP 参数
        if "claude" in model_name.lower():
            config_dict.pop("topP", None)

    # 使用 GeminiGenerationConfig 模型进行验证
    try:
        config = GeminiGenerationConfig(**config_dict)
        return config.model_dump(exclude_none=True)
    except Exception as e:
        log.warning(f"[ANTIGRAVITY] Failed to validate generation config: {e}, using dict directly")
        return config_dict


def convert_to_openai_tool_call(function_call: Dict[str, Any], index: int = None) -> Dict[str, Any]:
    """
    将 Antigravity functionCall 转换为 OpenAI tool_call，使用 OpenAIToolCall 模型

    Args:
        function_call: Antigravity 格式的函数调用
        index: 工具调用索引（流式响应必需）
    """
    tool_call = OpenAIToolCall(
        index=index,
        id=function_call.get("id", f"call_{uuid.uuid4().hex[:24]}"),
        type="function",
        function=OpenAIToolFunction(
            name=function_call.get("name", ""),
            arguments=json.dumps(function_call.get("args", {}))
        )
    )
    result = model_to_dict(tool_call)
    # Remove None values for cleaner output (non-streaming doesn't need index)
    if result.get("index") is None:
        del result["index"]
    return result


async def convert_antigravity_stream_to_openai(
    lines_generator: Any,
    stream_ctx: Any,
    client: Any,
    model: str,
    request_id: str,
    credential_manager: Any,
    credential_name: str,
    request_body: Optional[Dict[str, Any]] = None,  # ✅ 新增：用于获取上下文信息
    cred_mgr: Optional[Any] = None,  # ✅ 新增：用于 fallback（未来可能使用）
    context_info: Optional[Dict[str, Any]] = None  # ✅ 新增：上下文信息（token 数、工具结果数量等）
):
    """
    将 Antigravity 流式响应转换为 OpenAI 格式的 SSE 流

    Args:
        lines_generator: 行生成器 (已经过滤的 SSE 行)
    """
    state = {
        "thinking_started": False,
        "tool_calls": [],
        "content_buffer": "",
        "thinking_buffer": "",
        "success_recorded": False,
        "sse_lines_received": 0,  # 记录收到的 SSE 行数
        "chunks_sent": 0,  # 记录发送的 chunk 数
        "tool_calls_sent": False,  # 标记工具调用是否已发送
        "finish_reason_sent": False,  # 标记 finish_reason 是否已发送
        "has_valid_content": False,  # 标记是否收到了有效内容（text 或 functionCall）
        "empty_parts_count": 0,  # 记录空 parts 的次数
    }

    created = int(time.time())

    try:
        def build_content_chunk(content: str) -> str:
            chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {"content": content},
                    "finish_reason": None
                }]
            }
            return f"data: {json.dumps(chunk)}\n\n"

        def flush_thinking_buffer() -> Optional[str]:
            if not state["thinking_started"]:
                return None
            state["thinking_buffer"] += "\n</think>\n"
            thinking_block = state["thinking_buffer"]
            state["content_buffer"] += thinking_block
            state["thinking_buffer"] = ""
            state["thinking_started"] = False
            return thinking_block

        async for line in lines_generator:
            if not line or not line.startswith("data: "):
                continue

            # 记录第一次成功响应
            if not state["success_recorded"]:
                if credential_name and credential_manager:
                    await credential_manager.record_api_call_result(credential_name, True, is_antigravity=True)
                state["success_recorded"] = True

            # 解析 SSE 数据
            try:
                data = json.loads(line[6:])  # 去掉 "data: " 前缀
            except:
                continue

            state["sse_lines_received"] += 1

            # DEBUG: 记录收到的原始数据（用于诊断空响应问题）
            log.info(f"[ANTIGRAVITY STREAM] SSE line {state['sse_lines_received']}: {json.dumps(data)[:500]}")
            log.debug(f"[ANTIGRAVITY STREAM] Received data keys: {list(data.keys())}")

            # ✅ 新增：提取 cachedContentTokenCount（如果可用）
            # 这个信息可能在第一个 chunk 就出现，用于判断实际处理的 tokens
            usage_metadata = data.get("response", {}).get("usageMetadata", {})
            if usage_metadata:
                cached_content_token_count = usage_metadata.get("cachedContentTokenCount", 0)
                if cached_content_token_count > 0 and "cached_content_token_count" not in state:
                    # 只在第一次提取时保存（避免覆盖）
                    state["cached_content_token_count"] = cached_content_token_count
                    prompt_token_count = usage_metadata.get("promptTokenCount", 0)
                    if prompt_token_count > 0:
                        state["prompt_token_count"] = prompt_token_count
                        actual_processed_tokens = prompt_token_count - cached_content_token_count
                        state["actual_processed_tokens"] = actual_processed_tokens
                        log.info(f"[ANTIGRAVITY STREAM] Cache detected: {cached_content_token_count:,} tokens cached, "
                               f"{actual_processed_tokens:,} tokens actually processed (out of {prompt_token_count:,} total)")

            # 检查是否有错误
            if "error" in data:
                log.error(f"[ANTIGRAVITY STREAM] Error in response: {data.get('error')}")

            # 提取 candidates 和 parts
            response = data.get("response", {})
            candidates = response.get("candidates", [])
            log.info(f"[ANTIGRAVITY STREAM] Response has {len(candidates)} candidates")

            if candidates:
                candidate = candidates[0]
                content_obj = candidate.get("content", {})
                finish_reason_raw = candidate.get("finishReason")
                log.info(f"[ANTIGRAVITY STREAM] Candidate 0: finishReason={finish_reason_raw}, content_keys={list(content_obj.keys()) if content_obj else 'None'}")
                parts = content_obj.get("parts", [])

                # 检测空 parts 情况
                if not parts and finish_reason_raw:
                    state["empty_parts_count"] += 1
                    log.warning(f"[ANTIGRAVITY STREAM] Empty parts with finishReason={finish_reason_raw}! This may cause empty response.")
            else:
                parts = []
                log.warning(f"[ANTIGRAVITY STREAM] No candidates in response!")

            # DEBUG: 记录 parts 信息
            if parts:
                part_types = [list(p.keys()) if isinstance(p, dict) else type(p).__name__ for p in parts]
                log.debug(f"[ANTIGRAVITY STREAM] Parts count: {len(parts)}, types: {part_types}")

            # 检查 finishReason（提前检查以便记录）
            candidates = data.get("response", {}).get("candidates", [])
            if candidates:
                candidate = candidates[0]
                fr = candidate.get("finishReason")
                if fr:
                    log.info(f"[ANTIGRAVITY STREAM] finishReason detected: {fr}")
                    # 检查是否有内容
                    content_obj = candidate.get("content", {})
                    log.debug(f"[ANTIGRAVITY STREAM] Final content keys: {list(content_obj.keys()) if content_obj else 'None'}")

            for part in parts:
                # 处理思考内容
                if part.get("thought") is True:
                    if not state["thinking_started"]:
                        state["thinking_buffer"] = "<think>\n"
                        state["thinking_started"] = True
                    state["thinking_buffer"] += part.get("text", "")

                # 处理图片数据 (inlineData)
                elif "inlineData" in part:
                    # 如果之前在思考，先结束思考
                    thinking_block = flush_thinking_buffer()
                    if thinking_block:
                        yield build_content_chunk(thinking_block)

                    # 提取图片数据
                    inline_data = part["inlineData"]
                    mime_type = inline_data.get("mimeType", "image/png")
                    base64_data = inline_data.get("data", "")

                    # 转换为 Markdown 格式的图片
                    image_markdown = f"\n\n![生成的图片](data:{mime_type};base64,{base64_data})\n\n"
                    state["content_buffer"] += image_markdown

                    # 发送图片块
                    chunk = {
                        "id": request_id,
                        "object": "chat.completion.chunk",
                        "created": created,
                        "model": model,
                        "choices": [{
                            "index": 0,
                            "delta": {"content": image_markdown},
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(chunk)}\n\n"

                # 处理普通文本
                elif "text" in part:
                    # 如果之前在思考，先结束思考
                    thinking_block = flush_thinking_buffer()
                    if thinking_block:
                        yield build_content_chunk(thinking_block)

                    # 添加文本内容
                    text = part.get("text", "")

                    # 只有当 text 非空时才发送和标记为有效内容
                    # 修复：Antigravity 有时返回 {"text": ""} 空字符串，不应视为有效内容
                    if text:  # 非空字符串才处理
                        state["content_buffer"] += text

                        # 发送文本块
                        chunk = {
                            "id": request_id,
                            "object": "chat.completion.chunk",
                            "created": created,
                            "model": model,
                            "choices": [{
                                "index": 0,
                                "delta": {"content": text},
                                "finish_reason": None
                            }]
                        }
                        yield f"data: {json.dumps(chunk)}\n\n"
                        state["chunks_sent"] += 1
                        state["has_valid_content"] = True  # 收到了有效的文本内容
                    else:
                        # ✅ 新增：记录空文本，但如果是第一个 chunk 且无 finishReason，继续等待
                        state["empty_parts_count"] = state.get("empty_parts_count", 0) + 1
                        if state["sse_lines_received"] == 1 and not finish_reason_raw:
                            log.info(f"[ANTIGRAVITY STREAM] First chunk has empty text and no finishReason, "
                                   f"waiting for more chunks (this may indicate stream is starting or context is too long)")
                            # 不立即标记为无效，继续等待后续 chunk
                        else:
                            log.debug(f"[ANTIGRAVITY STREAM] Skipping empty text in part")

                # 处理工具调用
                elif "functionCall" in part:
                    tool_index = len(state["tool_calls"])
                    fc = part["functionCall"]
                    log.info(f"[ANTIGRAVITY STREAM] Tool call detected: name={fc.get('name')}, id={fc.get('id')}")
                    tool_call = convert_to_openai_tool_call(fc, index=tool_index)
                    state["tool_calls"].append(tool_call)
                    state["has_valid_content"] = True  # 收到了有效的工具调用
                    log.debug(f"[ANTIGRAVITY STREAM] Converted tool_call: {json.dumps(tool_call)[:200]}")

            # 检查是否结束
            finish_reason = data.get("response", {}).get("candidates", [{}])[0].get("finishReason")

            # ✅ 新增：处理 SAFETY/RECITATION finishReason
            if finish_reason in ("SAFETY", "RECITATION"):
                log.warning(f"[ANTIGRAVITY STREAM] Response blocked by {finish_reason} filter")

                # ✅ 构建包含工具提示的错误消息
                error_msg_parts = []
                error_msg_parts.append(f"[Response blocked by {finish_reason} filter. The content may have triggered safety policies.]")
                error_msg_parts.append("")
                error_msg_parts.append("⚠️ **Tool Call Format Reminder**:")
                error_msg_parts.append("If you encounter 'invalid arguments' errors when calling tools:")
                error_msg_parts.append("- Use EXACT parameter names from the current tool schema")
                error_msg_parts.append("- Do NOT use parameters from previous conversations")
                error_msg_parts.append("- For terminal/command tools: verify parameter name (may be `command`, `input`, or `cmd`)")

                error_chunk = {
                    "id": request_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": model,
                    "choices": [{
                        "index": 0,
                        "delta": {"content": "\n".join(error_msg_parts)},
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(error_chunk)}\n\n"
                state["chunks_sent"] += 1
                state["has_valid_content"] = True  # 标记为有内容（错误消息）

            if finish_reason:
                thinking_block = flush_thinking_buffer()
                if thinking_block:
                    yield build_content_chunk(thinking_block)

                # 发送工具调用
                if state["tool_calls"]:
                    log.info(f"[ANTIGRAVITY STREAM] Sending {len(state['tool_calls'])} tool calls")
                    chunk = {
                        "id": request_id,
                        "object": "chat.completion.chunk",
                        "created": created,
                        "model": model,
                        "choices": [{
                            "index": 0,
                            "delta": {"tool_calls": state["tool_calls"]},
                            "finish_reason": None
                        }]
                    }
                    log.debug(f"[ANTIGRAVITY STREAM] Tool calls chunk: {json.dumps(chunk)[:500]}")
                    yield f"data: {json.dumps(chunk)}\n\n"
                    state["tool_calls_sent"] = True  # 标记工具调用已发送

                # 发送使用统计
                usage_metadata = data.get("response", {}).get("usageMetadata", {})
                prompt_token_count = usage_metadata.get("promptTokenCount", 0)
                cached_content_token_count = usage_metadata.get("cachedContentTokenCount", 0)
                # ✅ 新增：保存 promptTokenCount 和 cachedContentTokenCount 到 state，用于错误消息
                state["prompt_token_count"] = prompt_token_count
                state["cached_content_token_count"] = cached_content_token_count
                # ✅ 计算实际处理的 tokens（排除缓存的部分）
                actual_processed_tokens = prompt_token_count - cached_content_token_count
                state["actual_processed_tokens"] = actual_processed_tokens
                usage = {
                    "prompt_tokens": prompt_token_count,
                    "completion_tokens": usage_metadata.get("candidatesTokenCount", 0),
                    "total_tokens": usage_metadata.get("totalTokenCount", 0)
                }

                # 确定 finish_reason
                openai_finish_reason = "stop"
                if state["tool_calls"]:
                    openai_finish_reason = "tool_calls"
                elif finish_reason == "MAX_TOKENS":
                    openai_finish_reason = "length"

                chunk = {
                    "id": request_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": model,
                    "choices": [{
                        "index": 0,
                        "delta": {},
                        "finish_reason": openai_finish_reason
                    }],
                    "usage": usage
                }
                yield f"data: {json.dumps(chunk)}\n\n"

        # 在流结束前，检查是否有未发送的工具调用
        # 这是一个保底逻辑，用于处理 finishReason 没有被正确检测到的情况
        if state["tool_calls"] and not state.get("tool_calls_sent", False):
            log.warning(f"[ANTIGRAVITY STREAM] Tool calls not sent yet, sending now (fallback logic)")
            log.info(f"[ANTIGRAVITY STREAM] Sending {len(state['tool_calls'])} tool calls (fallback)")

            # 发送工具调用
            chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {"tool_calls": state["tool_calls"]},
                    "finish_reason": None
                }]
            }
            yield f"data: {json.dumps(chunk)}\n\n"
            state["chunks_sent"] += 1

            # 发送 finish_reason
            finish_chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {},
                    "finish_reason": "tool_calls"
                }]
            }
            yield f"data: {json.dumps(finish_chunk)}\n\n"
            state["chunks_sent"] += 1
            state["finish_reason_sent"] = True

        # 检查是否收到了任何有效数据但没有发送任何内容
        # 这可能表示 Antigravity 后端返回了空响应
        if state["sse_lines_received"] == 0:
            log.warning(f"[ANTIGRAVITY STREAM] No SSE data received from Antigravity backend!")

            # ✅ 构建包含工具提示的错误消息
            error_msg_parts = []
            error_msg_parts.append("[Error: No response from backend. Please try again.]")
            error_msg_parts.append("")
            error_msg_parts.append("⚠️ **Tool Call Format Reminder**:")
            error_msg_parts.append("If you encounter 'invalid arguments' errors when calling tools:")
            error_msg_parts.append("- Use EXACT parameter names from the current tool schema")
            error_msg_parts.append("- Do NOT use parameters from previous conversations")
            error_msg_parts.append("- For terminal/command tools: verify parameter name (may be `command`, `input`, or `cmd`)")
            error_msg_parts.append("- When in doubt: re-read the tool definition")

            error_chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {"content": "\n".join(error_msg_parts)},
                    "finish_reason": None
                }]
            }
            yield f"data: {json.dumps(error_chunk)}\n\n"

            # 发送 finish_reason
            finish_chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {},
                    "finish_reason": "stop"
                }]
            }
            yield f"data: {json.dumps(finish_chunk)}\n\n"
            state["finish_reason_sent"] = True

        elif not state.get("has_valid_content", False):
            # 没有收到任何有效内容（text 或 functionCall）
            # 这是后端异常，应该明确告知前端
            log.error(f"[ANTIGRAVITY STREAM] No valid content received! "
                      f"SSE lines: {state['sse_lines_received']}, "
                      f"empty_parts_count: {state.get('empty_parts_count', 0)}, "
                      f"finish_reason: {finish_reason}")
            
            # ✅ 新增：检查是否只有空文本 chunk（可能是上下文过长或流式响应被截断）
            if state.get("empty_parts_count", 0) > 0 and state["chunks_sent"] == 0:
                log.warning(f"[ANTIGRAVITY STREAM] Stream ended with only empty text chunks. "
                           f"This may indicate: 1) Context too long (check promptTokenCount), "
                           f"2) API capacity issue, 3) Stream truncated before finishReason")

            # ✅ 新增：构建明确的、可操作的错误消息，让 Cursor 的 AI agent 知道需要压缩上下文
            # 从 state 中获取 promptTokenCount 和缓存信息（如果可用）
            prompt_token_count = state.get("prompt_token_count", 0)
            cached_content_token_count = state.get("cached_content_token_count", 0)
            actual_processed_tokens = state.get("actual_processed_tokens", 0)
            
            # ✅ 新增：基于实际处理的 tokens 进行动态 fallback
            # 如果实际处理的 tokens 超过阈值（50K），且还没有收到有效内容，尝试非流式 fallback
            ACTUAL_PROCESSED_TOKENS_THRESHOLD = 50000  # 50K tokens 阈值
            should_try_non_streaming_fallback = (
                request_body and 
                cred_mgr and 
                actual_processed_tokens > ACTUAL_PROCESSED_TOKENS_THRESHOLD and
                not state.get("fallback_attempted", False)  # 避免重复尝试
            )
            
            if should_try_non_streaming_fallback:
                log.warning(f"[ANTIGRAVITY STREAM] Actual processed tokens ({actual_processed_tokens:,}) exceed threshold ({ACTUAL_PROCESSED_TOKENS_THRESHOLD:,}). "
                           f"Attempting non-streaming fallback...")
                state["fallback_attempted"] = True
                
                try:
                    # 尝试非流式请求
                    from .antigravity_api import send_antigravity_request_no_stream
                    response_data, _, _ = await send_antigravity_request_no_stream(
                        request_body, cred_mgr
                    )
                    
                    # 转换非流式响应为 OpenAI 格式
                    openai_response_dict = convert_antigravity_response_to_openai(response_data, model, request_id)
                    
                    # 提取内容和工具调用
                    fallback_content = ""
                    fallback_tool_calls = []
                    if openai_response_dict and openai_response_dict.get("choices"):
                        choice = openai_response_dict["choices"][0]
                        message = choice.get("message", {})
                        if message and message.get("content"):
                            fallback_content = message["content"]
                        if message and message.get("tool_calls"):
                            fallback_tool_calls = message["tool_calls"]
                    
                    if fallback_content or fallback_tool_calls:
                        log.info(f"[ANTIGRAVITY STREAM] Non-streaming fallback successful! "
                               f"Content length: {len(fallback_content)}, Tool calls: {len(fallback_tool_calls)}")
                        
                        # 发送工具调用（如果存在）
                        if fallback_tool_calls:
                            tool_chunk = {
                                "id": request_id,
                                "object": "chat.completion.chunk",
                                "created": created,
                                "model": model,
                                "choices": [{
                                    "index": 0,
                                    "delta": {"tool_calls": fallback_tool_calls},
                                    "finish_reason": None
                                }]
                            }
                            yield f"data: {json.dumps(tool_chunk)}\n\n"
                            state["chunks_sent"] += 1
                            state["has_valid_content"] = True
                        
                        # 发送内容（如果存在，分块发送以保持流式体验）
                        if fallback_content:
                            # 将内容分成小块发送，模拟流式响应
                            chunk_size = 100  # 每块约100字符
                            for i in range(0, len(fallback_content), chunk_size):
                                chunk_text = fallback_content[i:i + chunk_size]
                                content_chunk = {
                                    "id": request_id,
                                    "object": "chat.completion.chunk",
                                    "created": created,
                                    "model": model,
                                    "choices": [{
                                        "index": 0,
                                        "delta": {"content": chunk_text},
                                        "finish_reason": None
                                    }]
                                }
                                yield f"data: {json.dumps(content_chunk)}\n\n"
                                state["chunks_sent"] += 1
                            state["has_valid_content"] = True
                        
                        # 发送 finish_reason
                        finish_reason_fallback = "stop"
                        if fallback_tool_calls:
                            finish_reason_fallback = "tool_calls"
                        
                        finish_chunk = {
                            "id": request_id,
                            "object": "chat.completion.chunk",
                            "created": created,
                            "model": model,
                            "choices": [{
                                "index": 0,
                                "delta": {},
                                "finish_reason": finish_reason_fallback
                            }]
                        }
                        yield f"data: {json.dumps(finish_chunk)}\n\n"
                        state["chunks_sent"] += 1
                        state["finish_reason_sent"] = True
                        state["has_valid_content"] = True
                        return  # 成功 fallback，结束流
                    else:
                        log.warning("[ANTIGRAVITY STREAM] Non-streaming fallback returned empty response.")
                except Exception as fallback_e:
                    log.error(f"[ANTIGRAVITY STREAM] Non-streaming fallback failed: {fallback_e}")
                    # Fallback 失败，继续执行错误消息逻辑
            
            # 从 context_info 获取上下文信息（如果可用）
            estimated_tokens = context_info.get("estimated_tokens", 0) if context_info else 0
            tool_result_count = context_info.get("tool_result_count", 0) if context_info else 0
            total_tool_results_length = context_info.get("total_tool_results_length", 0) if context_info else 0
            
            # 构建详细的错误消息，告诉 Cursor 的 AI agent 需要做什么
            error_parts = []
            error_parts.append("❌ **Context Length Limit Exceeded**: Backend returned empty response due to context being too long.")
            error_parts.append("")
            
            # 添加上下文统计信息（包含缓存信息）
            if prompt_token_count > 0:
                error_parts.append(f"📊 **Input tokens**: {prompt_token_count:,}")
                if cached_content_token_count > 0:
                    error_parts.append(f"   - **Cached tokens**: {cached_content_token_count:,} (reused from previous requests)")
                    error_parts.append(f"   - **Actual processed tokens**: {actual_processed_tokens:,}")
                    if actual_processed_tokens < 50000:
                        error_parts.append(f"   ⚠️ Note: Actual processed tokens ({actual_processed_tokens:,}) are within API limit, but API still returned empty response.")
                        error_parts.append(f"   This may indicate: 1) API capacity issue, 2) Stream truncation, 3) Safety filter")
                else:
                    error_parts.append(f"   ⚠️ **No cache available**: All {prompt_token_count:,} tokens need to be processed")
            elif estimated_tokens > 0:
                error_parts.append(f"📊 **Estimated input tokens**: {estimated_tokens:,} (exceeds API limit)")
            
            if tool_result_count > 0:
                error_parts.append(f"🔧 **Tool results**: {tool_result_count} results, {total_tool_results_length:,} characters")
            
            error_parts.append("")
            error_parts.append("💡 **Action Required**: You need to compress the context before retrying:")
            error_parts.append("")
            error_parts.append("1. **Summarize tool results**: Extract only essential information (errors, summaries, key findings)")
            error_parts.append("2. **Remove old tool results**: Keep only the most recent and relevant tool results")
            error_parts.append("3. **Truncate large results**: For large tool results, keep only the beginning and end, or extract key sections")
            error_parts.append("4. **Reduce tool calls**: Use fewer tool calls in the next request if possible")
            error_parts.append("")

            # ✅ 工具调用格式提示，帮助 agent 自我纠正参数格式问题
            error_parts.append("⚠️ **Tool Call Format Reminder** (IMPORTANT - Read carefully before making tool calls):")
            error_parts.append("")
            error_parts.append("If you encounter 'invalid arguments' errors when calling tools, please note:")
            error_parts.append("- **Always use the EXACT parameter names** as defined in the current tool schema")
            error_parts.append("- **Do NOT use parameters from previous conversations** - tool schemas may have changed")
            error_parts.append("- **Common mistakes to avoid**:")
            error_parts.append("  - `should_read_entire_file` → Use `target_file` with `offset`/`limit` instead")
            error_parts.append("  - `start_line_one_indexed` / `end_line_one_indexed` → Use `offset` / `limit` instead")
            error_parts.append("  - `command` for terminal → Check if it should be `input`, `cmd`, or other name")
            error_parts.append("  - Unknown parameters → Check the tool definition in current context")
            error_parts.append("- **Terminal/Command tools**: Parameter name varies - could be `command`, `input`, `cmd`, or `shell_command`")
            error_parts.append("- **When in doubt**: Re-read the tool definition and use only the parameters listed there")
            error_parts.append("")

            if cached_content_token_count > 0:
                error_parts.append(f"💡 **Cache Hint**: API cached {cached_content_token_count:,} tokens from previous requests.")
                error_parts.append(f"   If you keep similar context in subsequent requests, API may cache more tokens and reduce processing load.")
            error_parts.append("")
            error_parts.append("After compressing the context, retry the request with the reduced context.")
            
            error_msg = "\n".join(error_parts)
            error_chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {"content": error_msg},
                    "finish_reason": None
                }]
            }
            yield f"data: {json.dumps(error_chunk)}\n\n"
            state["chunks_sent"] += 1

            # 发送 finish_reason
            finish_chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {},
                    "finish_reason": "stop"
                }]
            }
            yield f"data: {json.dumps(finish_chunk)}\n\n"
            state["chunks_sent"] += 1
            state["finish_reason_sent"] = True

        # 检查是否已发送 finish_reason
        # 这是最后的保底逻辑，确保 Cursor 总是收到 finish_reason
        if not state.get("finish_reason_sent", False):
            log.warning(f"[ANTIGRAVITY STREAM] finish_reason not sent yet, sending now (final fallback)")
            # 确定 finish_reason
            final_finish_reason = "tool_calls" if state["tool_calls"] else "stop"
            finish_chunk = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {},
                    "finish_reason": final_finish_reason
                }]
            }
            yield f"data: {json.dumps(finish_chunk)}\n\n"
            state["finish_reason_sent"] = True
            state["chunks_sent"] += 1

        # 发送结束标记
        log.info(f"[ANTIGRAVITY STREAM] Stream ending. SSE lines: {state['sse_lines_received']}, Chunks sent: {state['chunks_sent']}, Content buffer: {len(state['content_buffer'])}, Tool calls: {len(state['tool_calls'])}, has_valid_content: {state.get('has_valid_content', False)}, empty_parts_count: {state.get('empty_parts_count', 0)}, finish_reason_sent: {state.get('finish_reason_sent', False)}")
        yield "data: [DONE]\n\n"

    except Exception as e:
        log.error(f"[ANTIGRAVITY] Streaming error: {e}")

        # ✅ 构建包含工具提示的错误响应
        error_msg_parts = []
        error_msg_parts.append(f"Streaming error: {str(e)}")
        error_msg_parts.append("")
        error_msg_parts.append("⚠️ **Tool Call Format Reminder**:")
        error_msg_parts.append("If you encounter 'invalid arguments' errors when calling tools:")
        error_msg_parts.append("- Use EXACT parameter names from the current tool schema")
        error_msg_parts.append("- Do NOT use parameters from previous conversations")
        error_msg_parts.append("- For terminal/command tools: verify parameter name (may be `command`, `input`, or `cmd`)")

        error_response = {
            "error": {
                "message": "\n".join(error_msg_parts),
                "type": "api_error",
                "code": 500
            }
        }
        yield f"data: {json.dumps(error_response)}\n\n"
    finally:
        # 确保清理所有资源
        try:
            await stream_ctx.__aexit__(None, None, None)
        except Exception as e:
            log.debug(f"[ANTIGRAVITY] Error closing stream context: {e}")
        try:
            await client.aclose()
        except Exception as e:
            log.debug(f"[ANTIGRAVITY] Error closing client: {e}")


def convert_antigravity_response_to_openai(
    response_data: Dict[str, Any],
    model: str,
    request_id: str
) -> Dict[str, Any]:
    """
    将 Antigravity 非流式响应转换为 OpenAI 格式
    """
    # 提取 parts
    parts = response_data.get("response", {}).get("candidates", [{}])[0].get("content", {}).get("parts", [])

    content = ""
    thinking_content = ""
    tool_calls_list = []

    for part in parts:
        # 处理思考内容
        if part.get("thought") is True:
            thinking_content += part.get("text", "")

        # 处理图片数据 (inlineData)
        elif "inlineData" in part:
            inline_data = part["inlineData"]
            mime_type = inline_data.get("mimeType", "image/png")
            base64_data = inline_data.get("data", "")
            # 转换为 Markdown 格式的图片
            content += f"\n\n![生成的图片](data:{mime_type};base64,{base64_data})\n\n"

        # 处理普通文本
        elif "text" in part:
            content += part.get("text", "")

        # 处理工具调用
        elif "functionCall" in part:
            tool_index = len(tool_calls_list)
            tool_calls_list.append(convert_to_openai_tool_call(part["functionCall"], index=tool_index))

    # 拼接思考内容
    if thinking_content:
        content = f"<think>\n{thinking_content}\n</think>\n{content}"

    # 使用 OpenAIChatMessage 模型构建消息
    message = OpenAIChatMessage(
        role="assistant",
        content=content,
        tool_calls=tool_calls_list if tool_calls_list else None
    )

    # 确定 finish_reason
    finish_reason = "stop"
    if tool_calls_list:
        finish_reason = "tool_calls"

    finish_reason_raw = response_data.get("response", {}).get("candidates", [{}])[0].get("finishReason")
    if finish_reason_raw == "MAX_TOKENS":
        finish_reason = "length"

    # 提取使用统计
    usage_metadata = response_data.get("response", {}).get("usageMetadata", {})
    usage = {
        "prompt_tokens": usage_metadata.get("promptTokenCount", 0),
        "completion_tokens": usage_metadata.get("candidatesTokenCount", 0),
        "total_tokens": usage_metadata.get("totalTokenCount", 0)
    }

    # 使用 OpenAIChatCompletionChoice 模型
    choice = OpenAIChatCompletionChoice(
        index=0,
        message=message,
        finish_reason=finish_reason
    )

    # 使用 OpenAIChatCompletionResponse 模型
    response = OpenAIChatCompletionResponse(
        id=request_id,
        object="chat.completion",
        created=int(time.time()),
        model=model,
        choices=[choice],
        usage=usage
    )

    return model_to_dict(response)


def convert_antigravity_response_to_gemini(
    response_data: Dict[str, Any]
) -> Dict[str, Any]:
    """
    将 Antigravity 非流式响应转换为 Gemini 格式
    Antigravity 的响应格式与 Gemini 非常相似，只需要提取 response 字段
    """
    # Antigravity 响应格式: {"response": {...}}
    # Gemini 响应格式: {...}
    return response_data.get("response", response_data)


async def convert_antigravity_stream_to_gemini(
    lines_generator: Any,
    stream_ctx: Any,
    client: Any,
    credential_manager: Any,
    credential_name: str
):
    """
    将 Antigravity 流式响应转换为 Gemini 格式的 SSE 流

    Args:
        lines_generator: 行生成器 (已经过滤的 SSE 行)
    """
    success_recorded = False

    try:
        async for line in lines_generator:
            if not line or not line.startswith("data: "):
                continue

            # 记录第一次成功响应
            if not success_recorded:
                if credential_name and credential_manager:
                    await credential_manager.record_api_call_result(credential_name, True, is_antigravity=True)
                success_recorded = True

            # 解析 SSE 数据
            try:
                data = json.loads(line[6:])  # 去掉 "data: " 前缀
            except:
                continue

            # Antigravity 流式响应格式: {"response": {...}}
            # Gemini 流式响应格式: {...}
            gemini_data = data.get("response", data)

            # 发送 Gemini 格式的数据
            yield f"data: {json.dumps(gemini_data)}\n\n"

    except Exception as e:
        log.error(f"[ANTIGRAVITY GEMINI] Streaming error: {e}")
        error_response = {
            "error": {
                "message": str(e),
                "code": 500,
                "status": "INTERNAL"
            }
        }
        yield f"data: {json.dumps(error_response)}\n\n"
    finally:
        # 确保清理所有资源
        try:
            await stream_ctx.__aexit__(None, None, None)
        except Exception as e:
            log.debug(f"[ANTIGRAVITY GEMINI] Error closing stream context: {e}")
        try:
            await client.aclose()
        except Exception as e:
            log.debug(f"[ANTIGRAVITY GEMINI] Error closing client: {e}")


@router.get("/antigravity/v1/models", response_model=ModelList)
async def list_models():
    """返回 OpenAI 格式的模型列表 - 动态从 Antigravity API 获取"""

    try:
        # 获取凭证管理器
        cred_mgr = await get_credential_manager()

        # 从 Antigravity API 获取模型列表（返回 OpenAI 格式的字典列表）
        models = await fetch_available_models(cred_mgr)

        if not models:
            # 如果获取失败，直接返回空列表
            log.warning("[ANTIGRAVITY] Failed to fetch models from API, returning empty list")
            return ModelList(data=[])

        # models 已经是 OpenAI 格式的字典列表，扩展为包含抗截断版本
        expanded_models = []
        for model in models:
            # 添加原始模型
            expanded_models.append(Model(**model))

            # 添加流式抗截断版本
            anti_truncation_model = model.copy()
            anti_truncation_model["id"] = f"流式抗截断/{model['id']}"
            expanded_models.append(Model(**anti_truncation_model))

        return ModelList(data=expanded_models)

    except Exception as e:
        log.error(f"[ANTIGRAVITY] Error fetching models: {e}")
        # 返回空列表
        return ModelList(data=[])


@router.post("/antigravity/v1/chat/completions")
async def chat_completions(
    request: Request,
    token: str = Depends(authenticate_bearer)
):
    """
    处理 OpenAI 格式的聊天完成请求，转换为 Antigravity API
    """
    # 获取原始请求数据
    try:
        raw_data = await request.json()
    except Exception as e:
        log.error(f"Failed to parse JSON request: {e}")
        raise HTTPException(status_code=400, detail=f"Invalid JSON: {str(e)}")

    # 清理工具格式：确保所有工具都有正确的结构
    # 网关应该已经规范化了工具，但为了安全起见，我们再次清理和验证
    if "tools" in raw_data and raw_data["tools"]:
        cleaned_tools = []
        for tool in raw_data["tools"]:
            if isinstance(tool, dict):
                # 检查工具是否有正确的结构
                has_type = "type" in tool
                has_function = "function" in tool
                has_name = "name" in tool
                
                # Case 1: 标准格式 - 已经有 type 和 function
                if has_type and has_function:
                    cleaned_tools.append(tool)
                # Case 2: Custom 类型 - 需要转换
                elif tool.get("type") == "custom":
                    custom_tool = tool.get("custom", {})
                    if isinstance(custom_tool, dict) and "name" in custom_tool:
                        from src.anthropic_converter import clean_json_schema
                        input_schema = custom_tool.get("input_schema", {}) or {}
                        if isinstance(input_schema, dict):
                            cleaned_schema = clean_json_schema(input_schema)
                            if "type" not in cleaned_schema:
                                cleaned_schema["type"] = "object"
                            input_schema = cleaned_schema
                        elif not input_schema:
                            input_schema = {"type": "object", "properties": {}}
                        
                        cleaned_tools.append({
                            "type": "function",
                            "function": {
                                "name": custom_tool.get("name", ""),
                                "description": custom_tool.get("description", ""),
                                "parameters": input_schema
                            }
                        })
                        log.warning(f"[ANTIGRAVITY] Converted custom tool '{custom_tool.get('name')}' to function format (should have been normalized by gateway)")
                    else:
                        log.warning(f"[ANTIGRAVITY] Skipping invalid custom tool: {list(custom_tool.keys()) if isinstance(custom_tool, dict) else 'not a dict'}")
                # Case 3: 扁平格式 - 只有 name，没有 type 和 function 包装
                # Cursor 可能使用 input_schema 而不是 parameters
                elif has_name and not has_function:
                    # 优先使用 parameters，如果没有则使用 input_schema（Cursor 可能使用 input_schema）
                    parameters = tool.get("parameters")
                    if parameters is None:
                        # Cursor 可能使用 input_schema 而不是 parameters
                        input_schema = tool.get("input_schema")
                        if input_schema is not None:
                            # 使用 clean_json_schema 清理 input_schema
                            from src.anthropic_converter import clean_json_schema
                            if isinstance(input_schema, dict):
                                parameters = clean_json_schema(input_schema)
                                # 确保有 type 字段
                                if "type" not in parameters:
                                    parameters["type"] = "object"
                            else:
                                log.warning(f"[ANTIGRAVITY] Tool '{tool.get('name')}' has non-dict input_schema: {type(input_schema)}, converting to empty dict")
                                parameters = {}
                        else:
                            parameters = {}
                    
                    # 确保 parameters 是字典
                    if not isinstance(parameters, dict):
                        log.warning(f"[ANTIGRAVITY] Tool '{tool.get('name')}' has non-dict parameters: {type(parameters)}, converting to dict")
                        parameters = {}
                    
                    cleaned_tools.append({
                        "type": "function",
                        "function": {
                            "name": tool.get("name", ""),
                            "description": tool.get("description", ""),
                            "parameters": parameters
                        }
                    })
                    log.warning(f"[ANTIGRAVITY] Converted flat format tool '{tool.get('name')}' to standard format (should have been normalized by gateway). Tool keys: {list(tool.keys())}, used_input_schema={'input_schema' in tool}")
                # Case 4: 其他格式 - 尝试修复或跳过
                else:
                    log.warning(f"[ANTIGRAVITY] Unknown tool format: type={tool.get('type')}, has_function={has_function}, has_name={has_name}, keys={list(tool.keys())}")
                    # 尝试修复：如果有 name，就包装成标准格式
                    if has_name:
                        # 优先使用 parameters，如果没有则使用 input_schema
                        parameters = tool.get("parameters")
                        if parameters is None:
                            input_schema = tool.get("input_schema")
                            if input_schema is not None:
                                from src.anthropic_converter import clean_json_schema
                                if isinstance(input_schema, dict):
                                    parameters = clean_json_schema(input_schema)
                                    if "type" not in parameters:
                                        parameters["type"] = "object"
                                else:
                                    parameters = {}
                            else:
                                parameters = {}
                        
                        if not isinstance(parameters, dict):
                            parameters = {}
                        
                        cleaned_tools.append({
                            "type": "function",
                            "function": {
                                "name": tool.get("name", ""),
                                "description": tool.get("description", ""),
                                "parameters": parameters
                            }
                        })
                    else:
                        log.warning(f"[ANTIGRAVITY] Skipping tool without name field")
            else:
                # 非字典类型，直接使用（可能是 Pydantic 模型，稍后处理）
                cleaned_tools.append(tool)
        raw_data["tools"] = cleaned_tools

    # 创建请求对象
    try:
        request_data = ChatCompletionRequest(**raw_data)
    except Exception as e:
        log.error(f"Request validation failed: {e}")
        raise HTTPException(status_code=400, detail=f"Request validation error: {str(e)}")

    # 健康检查
    if (
        len(request_data.messages) == 1
        and getattr(request_data.messages[0], "role", None) == "user"
        and getattr(request_data.messages[0], "content", None) == "Hi"
    ):
        return JSONResponse(
            content={
                "choices": [{"message": {"role": "assistant", "content": "antigravity API 正常工作中"}}]
            }
        )

    # 获取凭证管理器
    from src.credential_manager import get_credential_manager
    cred_mgr = await get_credential_manager()

    # 提取参数
    model = request_data.model
    messages = request_data.messages
    stream = getattr(request_data, "stream", False)
    tools = getattr(request_data, "tools", None)

    # DEBUG: Log tools format received by Antigravity router
    if tools:
        # 将 Pydantic 模型转换为字典以便检查
        tool_dicts = []
        for tool in tools:
            if not isinstance(tool, dict):
                if hasattr(tool, "model_dump"):
                    tool_dicts.append(tool.model_dump())
                elif hasattr(tool, "dict"):
                    tool_dicts.append(tool.dict())
                else:
                    # 尝试使用 getattr 获取属性
                    tool_dict = {}
                    for attr in ["type", "function", "custom"]:
                        if hasattr(tool, attr):
                            value = getattr(tool, attr)
                            if hasattr(value, "model_dump"):
                                tool_dict[attr] = value.model_dump()
                            elif hasattr(value, "dict"):
                                tool_dict[attr] = value.dict()
                            else:
                                tool_dict[attr] = value
                    tool_dicts.append(tool_dict)
            else:
                tool_dicts.append(tool)
        
        tool_types = [tool.get("type", "unknown") for tool in tool_dicts]
        log.info(f"[ANTIGRAVITY] Received {len(tools)} tools, types={tool_types[:10]}... (showing first 10)")
        
        # 检查是否有 custom 格式的工具
        has_custom = any(tool.get("type") == "custom" for tool in tool_dicts)
        if has_custom:
            log.warning(f"[ANTIGRAVITY] WARNING: Received custom tool format! This should have been normalized by gateway.")
            first_custom = next((tool for tool in tool_dicts if tool.get("type") == "custom"), None)
            if first_custom:
                custom_tool = first_custom.get("custom", {})
                input_schema = custom_tool.get("input_schema", {})
                schema_type = input_schema.get("type") if isinstance(input_schema, dict) else type(input_schema).__name__
                log.warning(f"[ANTIGRAVITY] First custom tool: name={custom_tool.get('name')}, input_schema_type={schema_type}, has_properties={'properties' in input_schema if isinstance(input_schema, dict) else False}")
        
        # 将工具转换为字典格式（如果还不是字典）
        if tool_dicts != tools:
            tools = tool_dicts

    # 检测并处理抗截断模式
    use_anti_truncation = is_anti_truncation_model(model)
    if use_anti_truncation:
        # 去掉 "流式抗截断/" 前缀
        from src.utils import get_base_model_from_feature_model
        model = get_base_model_from_feature_model(model)

    # 模型名称映射
    actual_model = model_mapping(model)
    enable_thinking = is_thinking_model(model)

    # 检查历史消息中是否有有效的 thinking block（包含 signature）
    # 如果 thinking 启用但历史消息没有有效的 thinking block，则禁用 thinking
    # 这可以避免 400 错误："thinking.signature: Field required"
    if enable_thinking:
        has_valid_thinking = False
        for msg in messages:
            # 检查消息是否有 thinking 内容
            content = getattr(msg, "content", None) if hasattr(msg, "content") else (msg.get("content") if isinstance(msg, dict) else None)
            
            if content:
                # 检查字符串格式的 content
                if isinstance(content, str):
                    import re
                    # 检查是否有 thinking 标签（但无法验证 signature）
                    if re.search(r'<(?:redacted_)?reasoning>.*?</(?:redacted_)?reasoning>', content, flags=re.DOTALL | re.IGNORECASE):
                        # 有 thinking 标签，但无法验证 signature，假设有效
                        has_valid_thinking = True
                        break
                # 检查数组格式的 content
                elif isinstance(content, list):
                    for item in content:
                        if isinstance(item, dict):
                            item_type = item.get("type")
                            if item_type in ("thinking", "redacted_thinking"):
                                # 检查是否有 signature
                                signature = item.get("signature")
                                if signature and signature.strip():
                                    has_valid_thinking = True
                                    break
                    if has_valid_thinking:
                        break
        
        if not has_valid_thinking:
            log.warning(f"[ANTIGRAVITY] Thinking 已启用，但历史消息中没有有效的 thinking block（包含 signature），禁用 thinking 模式以避免 400 错误")
            enable_thinking = False

    log.info(f"[ANTIGRAVITY] Request: model={model} -> {actual_model}, stream={stream}, thinking={enable_thinking}, anti_truncation={use_anti_truncation}")

    # 如果 thinking 被禁用，清理消息中的 thinking 内容块
    # 这可以避免 400 错误："When thinking is disabled, an `assistant` message..."
    if not enable_thinking:
        original_count = len(messages)
        # 检查是否有 thinking 内容需要清理
        has_thinking_before = any(
            (hasattr(msg, "role") and getattr(msg, "role", None) == "assistant" and 
             hasattr(msg, "content") and isinstance(getattr(msg, "content", ""), str) and 
             ("<think>" in getattr(msg, "content", "").lower() or 
              "<think>" in getattr(msg, "content", "").lower() or
              "<think>" in getattr(msg, "content", "").lower()))
            or (isinstance(msg, dict) and msg.get("role") == "assistant" and 
                isinstance(msg.get("content"), str) and 
                ("<think>" in msg.get("content", "").lower() or 
                 "<think>" in msg.get("content", "").lower() or
                 "<think>" in msg.get("content", "").lower()))
            or (isinstance(msg, dict) and msg.get("role") == "assistant" and 
                isinstance(msg.get("content"), list) and
                any(isinstance(item, dict) and item.get("type") in ("thinking", "redacted_thinking") 
                    for item in msg.get("content", [])))
            for msg in messages
        )
        if has_thinking_before:
            messages = strip_thinking_from_openai_messages(messages)
            log.info(f"[ANTIGRAVITY] Thinking 已禁用，已清理历史消息中的 thinking 内容块 (messages={original_count})")

    # 转换消息格式
    try:
        contents = openai_messages_to_antigravity_contents(messages, enable_thinking=enable_thinking)
        
        # 如果 thinking 启用，确保最后一条 assistant 消息以 thinking block 开头
        if enable_thinking and contents:
            # 找到最后一条 assistant 消息（role="model"）
            last_model_index = -1
            for i in range(len(contents) - 1, -1, -1):
                if contents[i].get("role") == "model":
                    last_model_index = i
                    break
            
            if last_model_index >= 0:
                last_model = contents[last_model_index]
                parts = last_model.get("parts", [])
                if parts:
                    first_part = parts[0]
                    # 检查第一个 part 是否是 thinking block
                    if not isinstance(first_part, dict) or first_part.get("thought") is not True:
                        # 没有 thinking block，需要添加一个
                        # 尝试从之前的消息中提取 thinking block
                        thinking_part = None
                        for i in range(last_model_index - 1, -1, -1):
                            prev_msg = contents[i]
                            if prev_msg.get("role") == "model":
                                prev_parts = prev_msg.get("parts", [])
                                for part in prev_parts:
                                    if isinstance(part, dict) and part.get("thought") is True:
                                        # 检查 signature 是否有效（非空）
                                        signature = part.get("thoughtSignature", "")
                                        if signature and signature.strip():
                                            # 找到有效的 thinking block，复制它
                                            thinking_part = {
                                                "text": part.get("text", ""),
                                                "thought": True,
                                                "thoughtSignature": signature
                                            }
                                            break
                                if thinking_part:
                                    break
                        
                        if thinking_part:
                            # 在开头插入 thinking block
                            parts.insert(0, thinking_part)
                            log.info(f"[ANTIGRAVITY] Added thinking block to last assistant message from previous message")
                        else:
                            # 无法从之前的消息中提取有效的 thinking block（包含 signature）
                            # 根据之前的检查，如果历史消息没有有效的 thinking block，enable_thinking 应该已经被禁用
                            # 但为了安全起见，如果确实没有，禁用 thinking 并清理消息
                            log.warning(f"[ANTIGRAVITY] Last assistant message does not start with thinking block, cannot find previous thinking block with valid signature. Disabling thinking mode.")
                            enable_thinking = False
                            # 清理消息中的 thinking 内容块
                            messages = strip_thinking_from_openai_messages(messages)
                            # 重新转换消息（不使用 thinking）
                            contents = openai_messages_to_antigravity_contents(messages, enable_thinking=False)
                            # 注意：这里不需要 break，因为我们已经重新转换了消息
                            # 后续代码会使用新的 contents
                    # 更新 parts（如果有修改）
                    if thinking_part:
                        last_model["parts"] = parts
    except Exception as e:
        log.error(f"Failed to convert messages: {e}")
        raise HTTPException(status_code=500, detail=f"Message conversion failed: {str(e)}")

    # ✅ 新增：验证 contents 是否为空（根本原因修复）
    # 参考：antigravity_anthropic_router.py:570 和 gemini_generate_content 的验证逻辑
    if not contents:
        log.error(f"[ANTIGRAVITY] Contents is empty after conversion! Messages count: {len(messages)}")
        # 记录原始消息，帮助排查问题
        for i, msg in enumerate(messages[:5]):  # 只记录前5条
            role = getattr(msg, "role", None) if hasattr(msg, "role") else (msg.get("role") if isinstance(msg, dict) else None)
            content_val = getattr(msg, "content", None) if hasattr(msg, "content") else (msg.get("content") if isinstance(msg, dict) else None)
            content_len = len(str(content_val)) if content_val else 0
            log.error(f"[ANTIGRAVITY] Message {i}: role={role}, content_type={type(content_val).__name__}, content_length={content_len}")

        raise HTTPException(
            status_code=400,
            detail="Messages converted to empty contents. Please ensure messages contain valid user content (non-empty text or images). This usually happens when: 1) Only system messages are provided, 2) User message content is empty, 3) Message format is invalid."
        )

    # 验证每个 content 的 parts 不为空
    for i, content_item in enumerate(contents):
        parts = content_item.get("parts", [])
        if not parts:
            log.error(f"[ANTIGRAVITY] Content {i} (role={content_item.get('role')}) has empty parts!")
            raise HTTPException(
                status_code=400,
                detail=f"Content at index {i} (role={content_item.get('role')}) has empty parts. Please ensure all messages contain valid content."
            )

    # 转换工具定义
    antigravity_tools = convert_openai_tools_to_antigravity(tools)

    # 生成配置参数
    parameters = {
        "temperature": getattr(request_data, "temperature", None),
        "top_p": getattr(request_data, "top_p", None),
        "max_tokens": getattr(request_data, "max_tokens", None),
    }
    # 过滤 None 值
    parameters = {k: v for k, v in parameters.items() if v is not None}

    # 使用更新后的 enable_thinking（可能已被禁用）
    generation_config = generate_generation_config(parameters, enable_thinking, actual_model)

    # 获取凭证信息（用于 project_id 和 session_id）
    cred_result = await cred_mgr.get_valid_credential(is_antigravity=True)
    if not cred_result:
        log.error("当前无可用 antigravity 凭证")
        raise HTTPException(status_code=500, detail="当前无可用 antigravity 凭证")

    _, credential_data = cred_result
    project_id = credential_data.get("project_id", "default-project")
    session_id = f"session-{uuid.uuid4().hex}"

    # ✅ 新增：估算输入 token 数（粗略估算，用于检测上下文过长）
    def estimate_input_tokens(contents: List[Dict[str, Any]]) -> int:
        """粗略估算输入 token 数"""
        total_chars = 0
        for content in contents:
            parts = content.get("parts", [])
            for part in parts:
                if "text" in part:
                    total_chars += len(str(part["text"]))
                elif "functionResponse" in part:
                    # 工具结果可能很大
                    response = part.get("functionResponse", {}).get("response", {})
                    output = response.get("output", "")
                    total_chars += len(str(output))
        # 粗略估算：1 token ≈ 4 字符
        return total_chars // 4
    
    estimated_tokens = estimate_input_tokens(contents)
    if estimated_tokens > 50000:  # 50K tokens 阈值
        log.warning(f"[ANTIGRAVITY] Large input context detected: ~{estimated_tokens} tokens. "
                   f"This may cause API to return empty response or stream truncation. "
                   f"Consider: 1) Reducing tool result size, 2) Using non-streaming request, 3) Splitting context")

    # 构建 Antigravity 请求体
    request_body = build_antigravity_request_body(
        contents=contents,
        model=actual_model,
        project_id=project_id,
        session_id=session_id,
        tools=antigravity_tools,
        generation_config=generation_config,
    )

    # ✅ 新增：增强诊断日志（特别是工具调用场景）
    log.info(f"[ANTIGRAVITY DEBUG] Request summary: contents_count={len(contents)}, "
             f"has_tools={bool(antigravity_tools)}, model={actual_model}, "
             f"enable_thinking={enable_thinking}")
    
    # ✅ 新增：检查是否是工具调用后的请求
    has_tool_messages = any(
        content.get("role") == "user" and any(
            part.get("functionResponse") for part in content.get("parts", [])
        )
        for content in contents
    )
    # ✅ 新增：统计工具结果信息，用于传递给错误消息
    tool_result_count = 0
    total_tool_results_length = 0
    if has_tool_messages:
        log.info(f"[ANTIGRAVITY DEBUG] This is a tool-call-follow-up request")
        # 记录工具消息的详细信息并统计
        for i, content in enumerate(contents):
            if content.get("role") == "user":
                parts = content.get("parts", [])
                for j, part in enumerate(parts):
                    if "functionResponse" in part:
                        fr = part["functionResponse"]
                        output = str(fr.get("response", {}).get("output", ""))
                        tool_result_count += 1
                        total_tool_results_length += len(output)
                        log.info(f"[ANTIGRAVITY DEBUG] Tool result {i}-{j}: "
                               f"id={fr.get('id')}, name={fr.get('name')}, "
                               f"output_type={type(fr.get('response', {}).get('output')).__name__}, "
                               f"output_length={len(output)}")
    
    # ✅ 新增：构建上下文信息，用于传递给错误消息
    context_info = {
        "estimated_tokens": estimated_tokens,
        "tool_result_count": tool_result_count,
        "total_tool_results_length": total_tool_results_length,
        "has_tool_messages": has_tool_messages
    }

    # 生成请求 ID
    request_id = f"chatcmpl-{int(time.time() * 1000)}"

    # ✅ 优化：检测上下文长度，如果过长且是流式请求，考虑使用非流式 fallback
    # 注意：这里使用 estimated_tokens 作为初步判断，实际处理的 tokens 可能在流式请求中动态检测
    # 非流式请求通常对长上下文更稳定，但会失去实时反馈的优势
    USE_NON_STREAMING_FALLBACK_THRESHOLD = 60000  # 60K tokens 阈值（基于 estimated_tokens）
    # 注意：实际处理的 tokens 阈值（50K）在流式转换器中动态检测
    should_use_non_streaming_fallback = (
        stream and
        estimated_tokens > USE_NON_STREAMING_FALLBACK_THRESHOLD and
        has_tool_messages  # 只在工具调用场景下启用 fallback
    )

    if should_use_non_streaming_fallback:
        log.warning(f"[ANTIGRAVITY] Large estimated context ({estimated_tokens:,} tokens) with tool calls detected. "
                   f"Using non-streaming request as fallback for better stability. "
                   f"Note: If API caches content, actual processed tokens may be lower. "
                   f"Streaming request will dynamically check actual processed tokens and fallback if needed.")
        # 可以选择直接切换到非流式，或者让流式请求自己处理
        # 为了保持实时反馈，我们让流式请求自己处理，只在极端情况下才直接切换
        # stream = False  # 取消直接切换，让流式请求动态处理

    # ====================== 带模型降级的请求发送 ======================
    # 获取当前模型的降级链
    fallback_models = get_fallback_models(actual_model)
    models_to_try = [actual_model] + fallback_models

    # 发送请求（带模型降级）
    last_error = None
    for attempt_idx, attempt_model in enumerate(models_to_try):
        try:
            # 如果不是第一个模型，更新请求体中的模型
            if attempt_idx > 0:
                log.warning(f"[ANTIGRAVITY FALLBACK] 模型降级: {actual_model} -> {attempt_model} (尝试 {attempt_idx + 1}/{len(models_to_try)})")
                request_body["model"] = attempt_model

            if stream:
                # 处理抗截断功能（仅流式传输时有效）
                if use_anti_truncation:
                    log.info("[ANTIGRAVITY] 启用流式抗截断功能")
                    max_attempts = await get_anti_truncation_max_attempts()

                    # 包装请求函数以适配抗截断处理器
                    async def antigravity_request_func(payload):
                        resources, cred_name, cred_data = await send_antigravity_request_stream(
                            payload, cred_mgr
                        )
                        response, stream_ctx, client = resources
                        return StreamingResponse(
                            convert_antigravity_stream_to_openai(
                                response, stream_ctx, client, model, request_id, cred_mgr, cred_name,
                                request_body=request_body,  # 传递请求体用于 fallback
                                cred_mgr=cred_mgr,  # 传递凭证管理器用于 fallback
                                context_info=context_info  # ✅ 新增：传递上下文信息用于错误消息
                            ),
                            media_type="text/event-stream"
                        )

                    return await apply_anti_truncation_to_stream(
                        antigravity_request_func, request_body, max_attempts
                    )

                # 流式请求（无抗截断）
                resources, cred_name, cred_data = await send_antigravity_request_stream(
                    request_body, cred_mgr
                )
                # resources 是一个元组: (response, stream_ctx, client)
                response, stream_ctx, client = resources

                # 转换并返回流式响应,传递资源管理对象
                # response 现在是 filtered_lines 生成器
                # ✅ 新增：传递请求体和凭证管理器用于 fallback，以及上下文信息用于错误消息
                return StreamingResponse(
                    convert_antigravity_stream_to_openai(
                        response, stream_ctx, client, model, request_id, cred_mgr, cred_name,
                        request_body=request_body,  # 传递请求体用于 fallback
                        cred_mgr=cred_mgr,  # 传递凭证管理器用于 fallback
                        context_info=context_info  # ✅ 新增：传递上下文信息用于错误消息
                    ),
                    media_type="text/event-stream"
                )
            else:
                # 非流式请求
                response_data, cred_name, cred_data = await send_antigravity_request_no_stream(
                    request_body, cred_mgr
                )

                # 转换并返回响应
                openai_response = convert_antigravity_response_to_openai(response_data, model, request_id)
                return JSONResponse(content=openai_response)

        except Exception as e:
            last_error = e
            error_msg = str(e)
            log.error(f"[ANTIGRAVITY] Request failed with model {attempt_model}: {error_msg}")

            # ====================== 使用新的智能降级逻辑 ======================
            from .fallback_manager import (
                is_quota_exhausted_error, is_retryable_error, is_403_error,
                get_cross_pool_fallback, is_haiku_model, HAIKU_FALLBACK_TARGET
            )

            # 1. 403 错误 - 需要触发凭证验证（暂时直接失败，后续可添加验证逻辑）
            if is_403_error(error_msg):
                log.warning(f"[ANTIGRAVITY FALLBACK] 检测到 403 错误，需要验证凭证")
                raise HTTPException(status_code=403, detail=f"Credential verification required: {error_msg}")

            # 2. 可重试错误（400, 普通429, 5xx）- 不降级，直接失败让客户端重试
            if is_retryable_error(error_msg):
                log.info(f"[ANTIGRAVITY] 可重试错误，不触发降级，返回错误让客户端重试")
                raise HTTPException(status_code=500, detail=f"Antigravity API request failed (retryable): {error_msg}")

            # 3. 额度用尽错误 - 触发跨池降级
            if is_quota_exhausted_error(error_msg):
                log.warning(f"[ANTIGRAVITY FALLBACK] 检测到额度用尽错误")

                # 检查是否还有降级模型可用
                if attempt_idx < len(models_to_try) - 1:
                    log.warning(f"[ANTIGRAVITY FALLBACK] 将尝试下一个降级模型")
                    continue
                else:
                    # 已经尝试过降级，检查是否可以路由到 Copilot
                    log.error(f"[ANTIGRAVITY FALLBACK] 所有降级模型均已尝试")
                    # TODO: 后续可添加 Copilot 路由逻辑
                    raise HTTPException(
                        status_code=429,
                        detail=f"All quota pools exhausted. Original error: {error_msg}"
                    )

            # 4. 其他错误 - 直接失败
            log.info(f"[ANTIGRAVITY] 未知错误类型，不触发降级")
            raise HTTPException(status_code=500, detail=f"Antigravity API request failed: {error_msg}")

    # 所有模型都失败了
    log.error(f"[ANTIGRAVITY FALLBACK] 所有模型均失败: {last_error}")
    raise HTTPException(status_code=500, detail=f"Antigravity API request failed (已尝试所有降级模型): {str(last_error)}")


# ==================== Gemini 格式 API 端点 ====================

@router.get("/antigravity/v1beta/models")
@router.get("/antigravity/v1/models")
async def gemini_list_models(api_key: str = Depends(authenticate_gemini_flexible)):
    """返回 Gemini 格式的模型列表 - 动态从 Antigravity API 获取"""

    try:
        # 获取凭证管理器
        cred_mgr = await get_credential_manager()

        # 从 Antigravity API 获取模型列表（返回 OpenAI 格式的字典列表）
        models = await fetch_available_models(cred_mgr)

        if not models:
            # 如果获取失败，返回空列表
            log.warning("[ANTIGRAVITY GEMINI] Failed to fetch models from API, returning empty list")
            return JSONResponse(content={"models": []})

        # 将 OpenAI 格式转换为 Gemini 格式，同时添加抗截断版本
        gemini_models = []
        for model in models:
            model_id = model.get("id", "")

            # 添加原始模型
            gemini_models.append({
                "name": f"models/{model_id}",
                "version": "001",
                "displayName": model_id,
                "description": f"Antigravity API - {model_id}",
                "supportedGenerationMethods": ["generateContent", "streamGenerateContent"],
            })

            # 添加流式抗截断版本
            anti_truncation_id = f"流式抗截断/{model_id}"
            gemini_models.append({
                "name": f"models/{anti_truncation_id}",
                "version": "001",
                "displayName": anti_truncation_id,
                "description": f"Antigravity API - {anti_truncation_id} (带流式抗截断功能)",
                "supportedGenerationMethods": ["generateContent", "streamGenerateContent"],
            })

        return JSONResponse(content={"models": gemini_models})

    except Exception as e:
        log.error(f"[ANTIGRAVITY GEMINI] Error fetching models: {e}")
        # 返回空列表
        return JSONResponse(content={"models": []})


@router.post("/antigravity/v1beta/models/{model:path}:generateContent")
@router.post("/antigravity/v1/models/{model:path}:generateContent")
async def gemini_generate_content(
    model: str = Path(..., description="Model name"),
    request: Request = None,
    api_key: str = Depends(authenticate_gemini_flexible),
):
    """处理 Gemini 格式的非流式内容生成请求（通过 Antigravity API）"""
    log.debug(f"[ANTIGRAVITY GEMINI] Non-streaming request for model: {model}")

    # 获取原始请求数据
    try:
        request_data = await request.json()
    except Exception as e:
        log.error(f"Failed to parse JSON request: {e}")
        raise HTTPException(status_code=400, detail=f"Invalid JSON: {str(e)}")

    # 验证必要字段
    if "contents" not in request_data or not request_data["contents"]:
        raise HTTPException(status_code=400, detail="Missing required field: contents")

    # 健康检查
    if (
        len(request_data["contents"]) == 1
        and request_data["contents"][0].get("role") == "user"
        and request_data["contents"][0].get("parts", [{}])[0].get("text") == "Hi"
    ):
        return JSONResponse(
            content={
                "candidates": [
                    {
                        "content": {"parts": [{"text": "antigravity API 正常工作中"}], "role": "model"},
                        "finishReason": "STOP",
                        "index": 0,
                    }
                ]
            }
        )

    # 获取凭证管理器
    from src.credential_manager import get_credential_manager
    cred_mgr = await get_credential_manager()

    # 提取模型名称（移除 "models/" 前缀）
    if model.startswith("models/"):
        model = model[7:]

    # 检测并处理抗截断模式（虽然非流式不会使用，但要处理模型名）
    use_anti_truncation = is_anti_truncation_model(model)
    if use_anti_truncation:
        # 去掉 "流式抗截断/" 前缀
        from src.utils import get_base_model_from_feature_model
        model = get_base_model_from_feature_model(model)

    # 模型名称映射
    actual_model = model_mapping(model)
    enable_thinking = is_thinking_model(model)

    log.info(f"[ANTIGRAVITY GEMINI] Request: model={model} -> {actual_model}, thinking={enable_thinking}")

    # 转换 Gemini contents 为 Antigravity contents
    try:
        contents = gemini_contents_to_antigravity_contents(request_data["contents"])
    except Exception as e:
        log.error(f"Failed to convert Gemini contents: {e}")
        raise HTTPException(status_code=500, detail=f"Message conversion failed: {str(e)}")

    # 提取 Gemini generationConfig
    gemini_config = request_data.get("generationConfig", {})

    # 转换为 Antigravity generation_config
    parameters = {
        "temperature": gemini_config.get("temperature"),
        "top_p": gemini_config.get("topP"),
        "top_k": gemini_config.get("topK"),
        "max_tokens": gemini_config.get("maxOutputTokens"),
        # 图片生成相关参数
        "response_modalities": gemini_config.get("response_modalities"),
        "image_config": gemini_config.get("image_config"),
    }
    # 过滤 None 值
    parameters = {k: v for k, v in parameters.items() if v is not None}

    generation_config = generate_generation_config(parameters, enable_thinking, actual_model)

    # 获取凭证信息（用于 project_id 和 session_id）
    cred_result = await cred_mgr.get_valid_credential(is_antigravity=True)
    if not cred_result:
        log.error("当前无可用 antigravity 凭证")
        raise HTTPException(status_code=500, detail="当前无可用 antigravity 凭证")

    _, credential_data = cred_result
    project_id = credential_data.get("project_id", "default-project")
    session_id = credential_data.get("session_id", f"session-{uuid.uuid4().hex}")

    # 处理 systemInstruction
    system_instruction = None
    if "systemInstruction" in request_data:
        system_instruction = request_data["systemInstruction"]

    # 处理 tools
    antigravity_tools = None
    if "tools" in request_data:
        # Gemini 和 Antigravity 的 tools 格式基本一致
        antigravity_tools = request_data["tools"]

    # 构建 Antigravity 请求体
    request_body = build_antigravity_request_body(
        contents=contents,
        model=actual_model,
        project_id=project_id,
        session_id=session_id,
        system_instruction=system_instruction,
        tools=antigravity_tools,
        generation_config=generation_config,
    )

    # 发送非流式请求
    try:
        response_data, cred_name, cred_data = await send_antigravity_request_no_stream(
            request_body, cred_mgr
        )

        # 转换并返回 Gemini 格式响应
        gemini_response = convert_antigravity_response_to_gemini(response_data)
        return JSONResponse(content=gemini_response)

    except Exception as e:
        log.error(f"[ANTIGRAVITY GEMINI] Request failed: {e}")
        raise HTTPException(status_code=500, detail=f"Antigravity API request failed: {str(e)}")


@router.post("/antigravity/v1beta/models/{model:path}:streamGenerateContent")
@router.post("/antigravity/v1/models/{model:path}:streamGenerateContent")
async def gemini_stream_generate_content(
    model: str = Path(..., description="Model name"),
    request: Request = None,
    api_key: str = Depends(authenticate_gemini_flexible),
):
    """处理 Gemini 格式的流式内容生成请求（通过 Antigravity API）"""
    log.debug(f"[ANTIGRAVITY GEMINI] Streaming request for model: {model}")

    # 获取原始请求数据
    try:
        request_data = await request.json()
    except Exception as e:
        log.error(f"Failed to parse JSON request: {e}")
        raise HTTPException(status_code=400, detail=f"Invalid JSON: {str(e)}")

    # 验证必要字段
    if "contents" not in request_data or not request_data["contents"]:
        raise HTTPException(status_code=400, detail="Missing required field: contents")

    # 获取凭证管理器
    from src.credential_manager import get_credential_manager
    cred_mgr = await get_credential_manager()

    # 提取模型名称（移除 "models/" 前缀）
    if model.startswith("models/"):
        model = model[7:]

    # 检测并处理抗截断模式
    use_anti_truncation = is_anti_truncation_model(model)
    if use_anti_truncation:
        # 去掉 "流式抗截断/" 前缀
        from src.utils import get_base_model_from_feature_model
        model = get_base_model_from_feature_model(model)

    # 模型名称映射
    actual_model = model_mapping(model)
    enable_thinking = is_thinking_model(model)

    log.info(f"[ANTIGRAVITY GEMINI] Stream request: model={model} -> {actual_model}, thinking={enable_thinking}, anti_truncation={use_anti_truncation}")

    # 转换 Gemini contents 为 Antigravity contents
    try:
        contents = gemini_contents_to_antigravity_contents(request_data["contents"])
    except Exception as e:
        log.error(f"Failed to convert Gemini contents: {e}")
        raise HTTPException(status_code=500, detail=f"Message conversion failed: {str(e)}")

    # 提取 Gemini generationConfig
    gemini_config = request_data.get("generationConfig", {})

    # 转换为 Antigravity generation_config
    parameters = {
        "temperature": gemini_config.get("temperature"),
        "top_p": gemini_config.get("topP"),
        "top_k": gemini_config.get("topK"),
        "max_tokens": gemini_config.get("maxOutputTokens"),
        # 图片生成相关参数
        "response_modalities": gemini_config.get("response_modalities"),
        "image_config": gemini_config.get("image_config"),
    }
    # 过滤 None 值
    parameters = {k: v for k, v in parameters.items() if v is not None}

    generation_config = generate_generation_config(parameters, enable_thinking, actual_model)

    # 获取凭证信息（用于 project_id 和 session_id）
    cred_result = await cred_mgr.get_valid_credential(is_antigravity=True)
    if not cred_result:
        log.error("当前无可用 antigravity 凭证")
        raise HTTPException(status_code=500, detail="当前无可用 antigravity 凭证")

    _, credential_data = cred_result
    project_id = credential_data.get("project_id", "default-project")
    session_id = credential_data.get("session_id", f"session-{uuid.uuid4().hex}")

    # 处理 systemInstruction
    system_instruction = None
    if "systemInstruction" in request_data:
        system_instruction = request_data["systemInstruction"]

    # 处理 tools
    antigravity_tools = None
    if "tools" in request_data:
        # Gemini 和 Antigravity 的 tools 格式基本一致
        antigravity_tools = request_data["tools"]

    # 构建 Antigravity 请求体
    request_body = build_antigravity_request_body(
        contents=contents,
        model=actual_model,
        project_id=project_id,
        session_id=session_id,
        system_instruction=system_instruction,
        tools=antigravity_tools,
        generation_config=generation_config,
    )

    # 发送流式请求
    try:
        # 处理抗截断功能（仅流式传输时有效）
        if use_anti_truncation:
            log.info("[ANTIGRAVITY GEMINI] 启用流式抗截断功能")
            max_attempts = await get_anti_truncation_max_attempts()

            # 包装请求函数以适配抗截断处理器
            async def antigravity_gemini_request_func(payload):
                resources, cred_name, cred_data = await send_antigravity_request_stream(
                    payload, cred_mgr
                )
                response, stream_ctx, client = resources
                return StreamingResponse(
                    convert_antigravity_stream_to_gemini(
                        response, stream_ctx, client, cred_mgr, cred_name
                    ),
                    media_type="text/event-stream"
                )

            return await apply_anti_truncation_to_stream(
                antigravity_gemini_request_func, request_body, max_attempts
            )

        # 流式请求（无抗截断）
        resources, cred_name, cred_data = await send_antigravity_request_stream(
            request_body, cred_mgr
        )
        # resources 是一个元组: (response, stream_ctx, client)
        response, stream_ctx, client = resources

        # 转换并返回流式响应
        # response 现在是 filtered_lines 生成器
        return StreamingResponse(
            convert_antigravity_stream_to_gemini(
                response, stream_ctx, client, cred_mgr, cred_name
            ),
            media_type="text/event-stream"
        )

    except Exception as e:
        log.error(f"[ANTIGRAVITY GEMINI] Stream request failed: {e}")
        raise HTTPException(status_code=500, detail=f"Antigravity API request failed: {str(e)}")


# ==================== SD-WebUI 格式 API 端点 ====================

@router.get("/sdapi/v1/options")
@router.get("/antigravity/sdapi/v1/options")
async def sdwebui_get_options(_: str = Depends(authenticate_sdwebui_flexible)):
    """返回 SD-WebUI 格式的配置选项"""
    log.info("[ANTIGRAVITY SD-WebUI] Received options request")
    # 返回基本的配置选项
    return {
        "sd_model_checkpoint": "gemini-3-pro-image",
        "sd_checkpoint_hash": None,
        "samples_save": True,
        "samples_format": "png",
        "save_images_add_number": True,
        "grid_save": True,
        "return_grid": True,
        "enable_pnginfo": True,
        "save_txt": False,
        "CLIP_stop_at_last_layers": 1,
    }


@router.get("/sdapi/v1/sd-models")
@router.get("/antigravity/sdapi/v1/sd-models")
async def sdwebui_list_models(_: str = Depends(authenticate_sdwebui_flexible)):
    """返回 SD-WebUI 格式的模型列表 - 只包含带 image 关键词的模型"""

    try:
        # 获取凭证管理器
        cred_mgr = await get_credential_manager()

        # 从 Antigravity API 获取模型列表
        models = await fetch_available_models(cred_mgr)

        if not models:
            log.warning("[ANTIGRAVITY SD-WebUI] Failed to fetch models from API, returning empty list")
            return []

        # 过滤只包含 "image" 关键词的模型
        image_models = []
        for model in models:
            model_id = model.get("id", "")
            if "image" in model_id.lower():
                # SD-WebUI 格式: {"title": "model_name", "model_name": "model_name", "hash": null}
                image_models.append({
                    "title": model_id,
                    "model_name": model_id,
                    "hash": None,
                    "sha256": None,
                    "filename": model_id,
                    "config": None
                })

        return image_models

    except Exception as e:
        log.error(f"[ANTIGRAVITY SD-WebUI] Error fetching models: {e}")
        return []


@router.post("/sdapi/v1/txt2img")
@router.post("/antigravity/sdapi/v1/txt2img")
async def sdwebui_txt2img(request: Request, _: str = Depends(authenticate_sdwebui_flexible)):
    """处理 SD-WebUI 格式的 txt2img 请求，转换为 Antigravity API"""
    # 获取原始请求数据
    try:
        request_data = await request.json()
    except Exception as e:
        log.error(f"Failed to parse JSON request: {e}")
        raise HTTPException(status_code=400, detail=f"Invalid JSON: {str(e)}")

    # 提取基本参数
    prompt = request_data.get("prompt", "")
    if not prompt:
        raise HTTPException(status_code=400, detail="Missing required field: prompt")

    negative_prompt = request_data.get("negative_prompt", "")

    # 提取图片生成相关参数
    width = request_data.get("width", 1024)
    height = request_data.get("height", 1024)

    # 计算 aspect_ratio - 映射到支持的比例
    # 支持的比例: "1:1", "2:3", "3:2", "3:4", "4:3", "4:5", "5:4", "9:16", "16:9", "21:9"
    def find_closest_aspect_ratio(w: int, h: int) -> str:
        ratio = w / h
        supported_ratios = {
            "1:1": 1.0,
            "2:3": 0.667,
            "3:2": 1.5,
            "3:4": 0.75,
            "4:3": 1.333,
            "4:5": 0.8,
            "5:4": 1.25,
            "9:16": 0.5625,
            "16:9": 1.778,
            "21:9": 2.333
        }
        closest = min(supported_ratios.items(), key=lambda x: abs(x[1] - ratio))
        return closest[0]

    aspect_ratio = find_closest_aspect_ratio(width, height)

    # 简化的尺寸映射到 image_size
    max_dimension = max(width, height)
    if max_dimension <= 1024:
        image_size = "1K"
    elif max_dimension <= 2048:
        image_size = "2K"
    else:
        image_size = "4K"

    # 提取模型（如果指定）
    model = request_data.get("override_settings", {}).get("sd_model_checkpoint", "gemini-3-pro-image")
    if not model or "image" not in model.lower():
        model = "gemini-3-pro-image"

    log.info(f"[ANTIGRAVITY SD-WebUI] txt2img request: model={model}, prompt={prompt[:50]}..., aspect_ratio={aspect_ratio}, image_size={image_size}")

    cred_mgr = await get_credential_manager()

    # 构建 Gemini 格式的 contents
    full_prompt = prompt
    if negative_prompt:
        full_prompt = f"{prompt}\n\nNegative prompt: {negative_prompt}"

    contents = [{
        "role": "user",
        "parts": [{"text": full_prompt}]
    }]

    # 构建 generation_config，包含图片生成参数
    parameters = {
        "response_modalities": ["TEXT", "IMAGE"],
        "image_config": {
            "aspect_ratio": aspect_ratio,
            "image_size": image_size
        }
    }

    # 模型名称映射
    actual_model = model_mapping(model)
    enable_thinking = is_thinking_model(model)

    generation_config = generate_generation_config(parameters, enable_thinking, actual_model)

    # 获取凭证信息
    cred_result = await cred_mgr.get_valid_credential(is_antigravity=True)
    if not cred_result:
        log.error("当前无可用 antigravity 凭证")
        raise HTTPException(status_code=500, detail="当前无可用 antigravity 凭证")

    _, credential_data = cred_result
    project_id = credential_data.get("project_id", "default-project")
    session_id = f"session-{uuid.uuid4().hex}"

    # 构建 Antigravity 请求体
    request_body = build_antigravity_request_body(
        contents=contents,
        model=actual_model,
        project_id=project_id,
        session_id=session_id,
        tools=None,
        generation_config=generation_config,
    )

    # 发送非流式请求
    try:
        response_data, cred_name, cred_data = await send_antigravity_request_no_stream(
            request_body, cred_mgr
        )

        # 提取生成的图片
        parts = response_data.get("response", {}).get("candidates", [{}])[0].get("content", {}).get("parts", [])

        images = []
        info_text = ""

        for part in parts:
            if "inlineData" in part:
                inline_data = part["inlineData"]
                base64_data = inline_data.get("data", "")
                images.append(base64_data)
            elif "text" in part:
                info_text += part.get("text", "")

        if not images:
            raise HTTPException(status_code=500, detail="No images generated")

        # 构建 SD-WebUI 格式的响应
        sdwebui_response = {
            "images": images,
            "parameters": request_data,
            "info": info_text or f"Generated by {model}"
        }

        return JSONResponse(content=sdwebui_response)

    except Exception as e:
        log.error(f"[ANTIGRAVITY SD-WebUI] txt2img request failed: {e}")
        raise HTTPException(status_code=500, detail=f"Image generation failed: {str(e)}")

