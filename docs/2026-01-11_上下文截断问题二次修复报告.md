# 上下文截断问题二次修复报告

**日期**: 2026-01-11 ~ 2026-01-12
**版本**: v1.4 (追加 max_tokens=None 默认值修复)
**作者**: 浮浮酱 (Claude Opus 4.5)

---

## 1. 问题背景

### 1.1 原始优化 (2026-01-10)

在 2026-01-10 实施了上下文截断智能优化，包括：
- tiktoken 精确 Token 计算
- 动态阈值调整
- 监控诊断模块

### 1.2 遗留问题

用户报告优化后仍存在五个问题：

**问题 1**: 启动日志显示 tiktoken 未安装
```
[WARNING] [CONTEXT TRUNCATION] tiktoken 未安装，使用字符估算模式
```

**问题 2**: 长对话仍触发 MAX_TOKENS 错误（输入过长）
```
promptTokenCount: 127,858
candidatesTokenCount: 4,096
finishReason: MAX_TOKENS
```

**问题 3**: 写 MD 文档时输出被截断（输出空间不足）
```
promptTokenCount: 80,097
candidatesTokenCount: 5,120
finishReason: MAX_TOKENS
日志: maxOutputTokens 提升 4096 -> 5120 (thinkingBudget=1024, 实际输出空间=4096)
```

**问题 4**: thinking=False 时输出仍被截断（下限保护缺失）
```
promptTokenCount: 70,203
candidatesTokenCount: 4,096
finishReason: MAX_TOKENS
日志: thinking=False, max_tokens=4096 (客户端原值直接使用，无下限保护)
```

**问题 5**: max_tokens=None 时输出仍被截断（缺少默认值）
```
promptTokenCount: 81,690
candidatesTokenCount: 4,096
finishReason: MAX_TOKENS
日志: thinking=False, 无 "maxOutputTokens 低于下限" 日志（if 块被跳过）
```

---

## 2. 问题分析

### 2.1 问题 1 根因：虚拟环境隔离

| 环境 | Python 版本 | tiktoken 状态 |
|------|-------------|---------------|
| 全局 Python | 3.14 | ✅ 已安装 (0.12.0) |
| .venv 虚拟环境 | 3.12 | ❌ 未安装 |

服务通过 `start.bat` 启动，使用 `.venv` 虚拟环境：
```batch
call .venv\Scripts\activate.bat
python web.py
```

**结论**: tiktoken 安装在全局 Python，但服务运行在 .venv 中，导致导入失败。

### 2.2 问题 2 根因：安全边际过高

原始配置：
```python
MODEL_CONTEXT_LIMITS = {
    "claude-opus": (200000, 0.80),  # 动态阈值 = 143,616 tokens
}
```

计算过程：
- 动态阈值 = 200K × 0.80 - 16384 = **143,616 tokens**
- 用户输入 = **127,858 tokens**
- 127,858 < 143,616 → **截断逻辑未触发！**

**结论**: 安全边际 0.80 过高，留给输出的空间不足（仅 20%），且无法触发截断。

### 2.3 问题 3 根因：输出空间限制过小

原始配置（`tool_converter.py`）：
```python
MAX_ALLOWED_TOKENS = 65535   # max_tokens 的绝对上限
MIN_OUTPUT_TOKENS = 4096     # 实际输出的最小保障空间
DEFAULT_MAX_OUTPUT_TOKENS = 16384  # 默认的 maxOutputTokens
```

问题分析：
- 用户输入 = **80,097 tokens**（输入空间充足，未触发截断）
- 输出限制 = **5,120 tokens**（thinkingBudget=1024 + 实际输出=4096）
- 写 MD 文档需要 **10K-30K tokens** 输出空间
- `MIN_OUTPUT_TOKENS = 4096` 太小，无法完成 MD 文档写入

**结论**: 输出空间分配不足，导致工具调用输出被截断。

### 2.4 问题 4 根因：thinking=False 时无下限保护

原始代码（`anthropic_converter.py:753-763`）：
```python
max_tokens = payload.get("max_tokens")
if max_tokens is not None:
    MAX_OUTPUT_TOKENS_LIMIT = 65535
    if isinstance(max_tokens, int) and max_tokens > MAX_OUTPUT_TOKENS_LIMIT:
        max_tokens = MAX_OUTPUT_TOKENS_LIMIT  # 只有上限保护！
    config["maxOutputTokens"] = max_tokens    # 直接使用客户端值
```

问题分析：
- 当 `thinking=False` 时，双向限制逻辑（第818-867行）不执行
- 第753-763行只有上限保护（65535），**没有下限保护**
- Cursor 客户端默认传 `max_tokens=4096`，被直接使用
- 4096 tokens 远远不够写 MD 文档（需要 10K-30K tokens）

**结论**: thinking=False 场景缺少下限保护，客户端的小 max_tokens 值被直接使用。

### 2.5 问题 5 根因：max_tokens=None 时缺少默认值

**这是之前所有修复都没生效的真正根因！**

原始代码结构（`anthropic_converter.py:753-773`）：
```python
max_tokens = payload.get("max_tokens")  # 返回 None
if max_tokens is not None:              # 条件为 False，整个 if 块被跳过！
    # 上限保护...
    # 下限保护...
    config["maxOutputTokens"] = max_tokens
# 没有 else 分支！config 中没有 maxOutputTokens
```

问题分析：
- 客户端（如 Claude Code）可能**不传** `max_tokens` 参数
- `payload.get("max_tokens")` 返回 `None`
- 整个 `if` 块被跳过，`config["maxOutputTokens"]` 没有被设置
- 下游 API（Antigravity/Gemini）使用默认值 **4096**
- 之前添加的下限保护代码根本没有执行！

**结论**: 代码只处理了 `max_tokens is not None` 的情况，缺少 `else` 分支来设置默认值。

---

## 3. 修复方案

### 3.1 修复问题 1：安装 tiktoken 到虚拟环境

```bash
cd F:/antigravity2api/gcli2api
uv pip install tiktoken
```

验证结果：
```
tiktoken 0.12.0 已安装到 .venv
```

### 3.2 修复问题 2：降低安全边际系数

**文件**: `gcli2api/src/context_truncation.py`

**修改前**:
```python
MODEL_CONTEXT_LIMITS = {
    "claude": (200000, 0.80),
    "claude-opus": (200000, 0.80),
    "claude-sonnet": (200000, 0.80),
    "claude-haiku": (200000, 0.85),
}
```

**修改后**:
```python
MODEL_CONTEXT_LIMITS = {
    # Claude 系列：200K 上下文
    # 思考模式输出可能高达 40K+，需要预留更多输出空间
    "claude": (200000, 0.55),      # 200K * 0.55 = 110K 安全限制
    "claude-opus": (200000, 0.50), # Opus thinking 需要更多输出空间
    "claude-sonnet": (200000, 0.55),
    "claude-haiku": (200000, 0.65),  # Haiku 输出较少
}
```

### 3.3 修复问题 3：提高输出空间限制

**文件**: `gcli2api/src/converters/tool_converter.py`

**修改前**:
```python
MAX_ALLOWED_TOKENS = 65535   # max_tokens 的绝对上限
MIN_OUTPUT_TOKENS = 4096     # 实际输出的最小保障空间
DEFAULT_MAX_OUTPUT_TOKENS = 16384  # 默认的 maxOutputTokens
```

**修改后**:
```python
# [FIX 2026-01-11] 提高输出空间限制，支持写长文档（MD文档可能需要 10K-30K tokens）
MAX_ALLOWED_TOKENS = 65535   # max_tokens 的绝对上限（Claude 最大值）
MIN_OUTPUT_TOKENS = 16384    # 实际输出的最小保障空间（4096 -> 16384，支持长文档输出）
DEFAULT_MAX_OUTPUT_TOKENS = 32768  # 默认的 maxOutputTokens（16384 -> 32768，确保足够的输出空间）
```

### 3.4 修复问题 4：添加 max_tokens 下限保护

**文件**: `gcli2api/src/anthropic_converter.py:753-773`

**修改前**:
```python
max_tokens = payload.get("max_tokens")
if max_tokens is not None:
    MAX_OUTPUT_TOKENS_LIMIT = 65535
    if isinstance(max_tokens, int) and max_tokens > MAX_OUTPUT_TOKENS_LIMIT:
        max_tokens = MAX_OUTPUT_TOKENS_LIMIT
    config["maxOutputTokens"] = max_tokens  # 直接使用，无下限保护
```

**修改后**:
```python
max_tokens = payload.get("max_tokens")
if max_tokens is not None:
    MAX_OUTPUT_TOKENS_LIMIT = 65535
    if isinstance(max_tokens, int) and max_tokens > MAX_OUTPUT_TOKENS_LIMIT:
        max_tokens = MAX_OUTPUT_TOKENS_LIMIT

    # [FIX 2026-01-11] 添加下限保护
    MIN_OUTPUT_TOKENS_FLOOR = 16384
    if isinstance(max_tokens, int) and max_tokens < MIN_OUTPUT_TOKENS_FLOOR:
        log.info(f"[ANTHROPIC CONVERTER] maxOutputTokens 低于下限: {max_tokens} -> {MIN_OUTPUT_TOKENS_FLOOR}")
        max_tokens = MIN_OUTPUT_TOKENS_FLOOR

    config["maxOutputTokens"] = max_tokens
```

### 3.5 修复问题 5：添加 max_tokens 默认值

**文件**: `gcli2api/src/anthropic_converter.py:753-781`

**修改前**:
```python
max_tokens = payload.get("max_tokens")
if max_tokens is not None:
    # 上限保护
    # 下限保护
    config["maxOutputTokens"] = max_tokens
# 没有 else 分支！

stop_sequences = payload.get("stop_sequences")
```

**修改后**:
```python
max_tokens = payload.get("max_tokens")
if max_tokens is not None:
    # 上限保护
    # 下限保护
    config["maxOutputTokens"] = max_tokens
else:
    # [FIX 2026-01-12] 客户端未传 max_tokens 时，使用默认值保证足够输出空间
    DEFAULT_MAX_OUTPUT_TOKENS = 16384
    log.info(f"[ANTHROPIC CONVERTER] max_tokens 未指定，使用默认值: {DEFAULT_MAX_OUTPUT_TOKENS}")
    config["maxOutputTokens"] = DEFAULT_MAX_OUTPUT_TOKENS

stop_sequences = payload.get("stop_sequences")
```

---

## 4. 修复效果

### 4.1 问题 2 - 新的动态阈值

| 模型 | 旧安全边际 | 新安全边际 | 旧阈值 | 新阈值 |
|------|-----------|-----------|--------|--------|
| claude-opus | 0.80 | 0.50 | 143,616 | **83,616** |
| claude-sonnet | 0.80 | 0.55 | 143,616 | **93,616** |
| claude-haiku | 0.85 | 0.65 | 153,616 | **113,616** |
| claude | 0.80 | 0.55 | 143,616 | **93,616** |

用户场景：127,858 tokens 输入

| 模型 | 动态阈值 | 是否触发截断 |
|------|----------|-------------|
| claude-opus | 83,616 | ✅ 会触发 (127,858 > 83,616) |
| claude-sonnet | 93,616 | ✅ 会触发 |
| claude-haiku | 113,616 | ✅ 会触发 |

### 4.2 问题 3 - 新的输出空间

| 配置项 | 旧值 | 新值 | 效果 |
|--------|------|------|------|
| MIN_OUTPUT_TOKENS | 4,096 | 16,384 | **4x 提升** |
| DEFAULT_MAX_OUTPUT_TOKENS | 16,384 | 32,768 | **2x 提升** |

Thinking 模式下的实际输出空间：
- **修复前**: 5,120 - 1,024 = **4,096 tokens**
- **修复后**: 17,408 - 1,024 = **16,384 tokens**

这足以支持写 10K-30K tokens 的 MD 文档。

### 4.3 问题 4 - thinking=False 时的下限保护

| 场景 | 客户端 max_tokens | 修复前 | 修复后 |
|------|-------------------|--------|--------|
| Cursor 默认 | 4,096 | 4,096（被截断）| **16,384** |
| Claude CLI | 8,192 | 8,192 | **16,384** |
| 自定义客户端 | 2,048 | 2,048（严重截断）| **16,384** |

**关键改进**：
- 即使 `thinking=False`，输出空间也保证 >= 16,384 tokens
- 即使客户端不传 `max_tokens`，输出空间也保证 >= 16,384 tokens
- Cursor 默认的 `max_tokens=4096` 会自动提升到 16,384
- 足够完成 MD 文档写入任务

### 4.4 问题 5 - max_tokens=None 时的默认值

| 场景 | 客户端 max_tokens | 修复前 | 修复后 |
|------|-------------------|--------|--------|
| 未传参数 | None | 4,096（API默认）| **16,384** |
| Claude Code | 未传 | 4,096 | **16,384** |
| 自定义客户端 | 未传 | 4,096 | **16,384** |

**关键改进**：
- 无论客户端是否传 `max_tokens`，都保证至少 16,384 tokens 输出空间
- 添加日志：`max_tokens 未指定，使用默认值: 16384`
- 彻底解决了之前所有修复都不生效的问题

---

## 5. 文件变更清单

| 文件 | 变更类型 | 说明 |
|------|----------|------|
| `src/context_truncation.py` | 修改 | 降低 Claude 系列安全边际 |
| `src/converters/tool_converter.py` | 修改 | 提高输出 token 限制 |
| `src/anthropic_converter.py` | 修改 | **关键修复** - 提高 MIN_OUTPUT_TOKENS + 添加下限保护 |
| `scripts/fix_safety_margin.py` | 新增 | 安全边际修复脚本 |
| `scripts/fix_output_tokens.py` | 新增 | 输出限制修复脚本 |
| `scripts/fix_anthropic_output_tokens.py` | 新增 | anthropic_converter MIN_OUTPUT_TOKENS 修复脚本 |
| `scripts/fix_max_tokens_floor.py` | 新增 | max_tokens 下限保护修复脚本 |
| `scripts/fix_max_tokens_default.py` | 新增 | **关键** - max_tokens=None 默认值修复脚本 |

---

## 6. 设计考量

### 6.1 为什么降低安全边际到 0.50-0.55？

Claude 思考模式 (Extended Thinking) 的输出特点：
- 思考过程可能产生 20K-40K tokens
- 最终回复可能需要 10K-20K tokens
- 工具调用 JSON 可能需要 5K-10K tokens

总计需要预留 **50-70K tokens** 给输出，因此：
- 200K × 0.50 = 100K 输入 + 100K 输出空间
- 这是一个合理的平衡点

### 6.2 为什么 Opus 用 0.50，Sonnet 用 0.55？

- Opus 4.5 的思考模式更深入，输出更长
- Sonnet 相对轻量，可以稍微激进一些
- Haiku 输出最短，使用 0.65

### 6.3 为什么提高 MIN_OUTPUT_TOKENS 到 16384？

- 原来 4096 tokens 只能写约 3000 字的文档
- MD 文档经常需要 10K-30K tokens（7000-20000 字）
- 16384 tokens 可以支持约 12000 字的输出
- 配合 DEFAULT_MAX_OUTPUT_TOKENS = 32768，非 thinking 模式可支持更长输出

---

## 7. 后续建议

### 7.1 立即执行

1. **重启服务**：使修改生效
   ```bash
   cd F:/antigravity2api/gcli2api
   # 停止当前服务后重新启动
   ```

### 7.2 短期观察

1. **观察启动日志**：确认 tiktoken 加载成功
2. **测试长对话**：验证截断逻辑正常触发
3. **测试 MD 文档写入**：确认输出不再被截断

### 7.3 中期优化

1. **监控截断频率**：如果截断过于频繁，可适当调高阈值
2. **优化截断策略**：保留更多关键上下文

### 7.4 长期改进

1. **自适应阈值**：根据历史数据动态调整安全边际
2. **输出预测**：基于请求类型预测输出大小
3. **智能分块**：超长输出自动分块处理

---

## 8. 总结

本次修复解决了 2026-01-10 优化后的五个问题：

| 问题 | 根因 | 修复方案 | 状态 |
|------|------|----------|------|
| tiktoken 导入失败 | 虚拟环境隔离 | 安装到 .venv | ✅ 已修复 |
| 输入截断未触发 | 安全边际过高 | 降低到 0.50-0.55 | ✅ 已修复 |
| 输出空间不足 (thinking=True) | MIN_OUTPUT_TOKENS 过小 | 提升到 16384 | ✅ 已修复 |
| 输出空间不足 (thinking=False) | 无下限保护 | 添加 MIN_OUTPUT_TOKENS_FLOOR | ✅ 已修复 |
| 输出空间不足 (max_tokens=None) | 缺少默认值 | 添加 else 分支设置默认值 | ✅ 已修复 |

修复后效果：
- 127,858 tokens 的长对话将触发截断逻辑
- 80,097 tokens 输入 + 16,384 tokens 输出空间 = 足够写 MD 文档
- 即使 `thinking=False`，输出空间也保证 >= 16,384 tokens
- 即使客户端不传 `max_tokens`，输出空间也保证 >= 16,384 tokens

---

*浮浮酱出品，必属精品喵～* ≡ω≡

